{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7ddad3c-6539-499b-a986-2e68b836b935",
   "metadata": {},
   "source": [
    "This should be the final pipeline for the cleaning and preparing the dataset. End result is a data frame, ready to be tossed into a ML model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7cbe85b4-2efa-4669-a8e5-9e06a3f16108",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b553d731-5df2-4d74-8a66-96b6b4a4456c",
   "metadata": {},
   "source": [
    "Loading the data, and convert all the strings to lower case for uniformity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2586bc1f-d182-417a-b63c-c5cd74091fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_0 = pd.read_csv(\"../data/Training_Set_Values.csv\")\n",
    "df_1 = pd.read_csv(\"../data/Training_Set_labels.csv\")\n",
    "df = df_0.merge(df_1, on='id')\n",
    "for col in df.select_dtypes(include='object').columns:\n",
    "    df[col] = df[col].map(lambda x: x.lower() if isinstance(x, str) else x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4397b76b-d454-4346-bbfb-4b38c1a6ba25",
   "metadata": {},
   "source": [
    "Print names of the columns that contain NaNs. Need to fix them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "138598d5-9154-49e5-b1af-299f7bd2e3bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['funder', 'installer', 'subvillage', 'public_meeting', 'scheme_management', 'scheme_name', 'permit']\n"
     ]
    }
   ],
   "source": [
    "cols_with_nan = df.columns[df.isna().any()].tolist()\n",
    "print(cols_with_nan)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "755c960c-6517-48b3-b96f-8a5337ef679c",
   "metadata": {},
   "source": [
    "Some columns have no NaNs, but have error in recording, such as latitude and longitude recorded as 0.0000, or population, which contains several 0s, 1s and values smaller than 10."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c9453e4-e407-43ca-99a6-4e52972d40ce",
   "metadata": {},
   "source": [
    "First, fixing subvillage. This is important, because we use the column 'subvillage' to impute missing values in other columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f2ae512b-5c10-48a2-9c89-344e2f675b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Replace blank strings with NaN\n",
    "df['subvillage'] = df['subvillage'].replace('', pd.NA)\n",
    "\n",
    "# 2. Function to fill NaN with fallback hierarchy\n",
    "def fill_subvillage(row):\n",
    "    if pd.isna(row['subvillage']):\n",
    "        ward_mode = df[df['ward'] == row['ward']]['subvillage'].mode()\n",
    "        if not ward_mode.empty:\n",
    "            return ward_mode[0]\n",
    "\n",
    "        lga_mode = df[df['lga'] == row['lga']]['subvillage'].mode()\n",
    "        if not lga_mode.empty:\n",
    "            return lga_mode[0]\n",
    "\n",
    "        region_mode = df[df['district_code'] == row['district_code']]['subvillage'].mode()\n",
    "        if not region_mode.empty:\n",
    "            return region_mode[0]\n",
    "\n",
    "        return 'unknown'\n",
    "\n",
    "    return row['subvillage']\n",
    "\n",
    "# 3. Apply the function\n",
    "df['subvillage'] = df.apply(fill_subvillage, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db077ed1-486b-48e3-869c-f27d31829364",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['subvillage'].isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c36a31-3618-4e78-8aba-84bbed4b24e5",
   "metadata": {},
   "source": [
    "Now, fixing the missing Lat and Long coordinate values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "78471430-fa43-4cec-8668-bf0d0a270d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['latitude'] = df['latitude'].apply(lambda x: pd.NA if abs(x) < 1e-6 else x)\n",
    "df['longitude'] = df['longitude'].apply(lambda x: pd.NA if abs(x) < 1e-6 else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "77dfb2f8-6603-4bd6-8a2e-0efb8c94f7f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_fallback_order = ['subvillage', 'ward', 'lga', 'district_code', 'region_code']\n",
    "\n",
    "def fill_missing_geo(df, group_cols):\n",
    "    for col in group_cols:\n",
    "        # Compute group-wise means (excluding missing lat/lon)\n",
    "        group_means = df.dropna(subset=['latitude', 'longitude']).groupby(col)[['latitude', 'longitude']].mean()\n",
    "\n",
    "        def fill(row):\n",
    "            if pd.isna(row['latitude']) or pd.isna(row['longitude']):\n",
    "                key = row[col]\n",
    "                if key in group_means.index:\n",
    "                    if pd.isna(row['latitude']):\n",
    "                        row['latitude'] = group_means.loc[key, 'latitude']\n",
    "                    if pd.isna(row['longitude']):\n",
    "                        row['longitude'] = group_means.loc[key, 'longitude']\n",
    "            return row\n",
    "\n",
    "        df = df.apply(fill, axis=1)\n",
    "\n",
    "        # Stop early if no more NaNs\n",
    "        if df['latitude'].isna().sum() == 0 and df['longitude'].isna().sum() == 0:\n",
    "            break\n",
    "\n",
    "    return df\n",
    "\n",
    "df = fill_missing_geo(df, geo_fallback_order)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ca666d-47ae-423e-8234-3d175c59977f",
   "metadata": {},
   "source": [
    "After fixing the latitude and longitude, lets fix population values. Since these are heavily skewed, use median values to fill instead of mean.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "81ad93b7-2428-4fb1-a33b-dbc4639f18ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remaining missing population values: 0\n"
     ]
    }
   ],
   "source": [
    "# 1. Convert suspicious population values to NaN\n",
    "df['population'] = df['population'].apply(lambda x: pd.NA if x in [0, 1] else x)\n",
    "\n",
    "# 2. Define fallback order from most local to most general\n",
    "fallback_order = ['subvillage','ward', 'lga', 'district_code', 'region_code']\n",
    "\n",
    "# 3. Imputation function using median per group\n",
    "def fill_missing_population(df, levels):\n",
    "    for level in levels:\n",
    "        # Calculate median population per group (excluding missing)\n",
    "        group_medians = df.dropna(subset=['population']).groupby(level)['population'].median()\n",
    "\n",
    "        def fill(row):\n",
    "            if pd.isna(row['population']):\n",
    "                key = row[level]\n",
    "                if key in group_medians:\n",
    "                    row['population'] = group_medians.loc[key]\n",
    "            return row\n",
    "\n",
    "        df = df.apply(fill, axis=1)\n",
    "\n",
    "        # Exit early if all values are filled\n",
    "        if df['population'].isna().sum() == 0:\n",
    "            break\n",
    "\n",
    "    return df\n",
    "\n",
    "# 4. Apply the imputation\n",
    "df = fill_missing_population(df, fallback_order)\n",
    "\n",
    "# Optional: Check remaining NaNs\n",
    "print(\"Remaining missing population values:\", df['population'].isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2adf2439-5b7d-4e01-a438-2239ebde3cac",
   "metadata": {},
   "source": [
    "construction_year and gps_height also contain many 0s. Gotta fix those too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "947ff292-bc69-42af-8a34-f8c6e4a6be66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def geo_groupwise_fill(df, target_col, group_cols):\n",
    "    for col in group_cols:\n",
    "        group_medians = df.dropna(subset=[target_col]).groupby(col)[target_col].median()\n",
    "\n",
    "        def fill(row):\n",
    "            if pd.isna(row[target_col]):\n",
    "                key = row[col]\n",
    "                if key in group_medians.index:\n",
    "                    row[target_col] = group_medians.loc[key]\n",
    "            return row\n",
    "\n",
    "        df = df.apply(fill, axis=1)\n",
    "\n",
    "        if df[target_col].isna().sum() == 0:\n",
    "            break\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff5d20a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def geo_groupwise_fill_mode(df, target_col, group_cols):\n",
    "    for col in group_cols:\n",
    "        group_medians = df.dropna(subset=[target_col]).groupby(col)[target_col].agg(lambda x: x.mode().iloc[0])\n",
    "\n",
    "        def fill(row):\n",
    "            if pd.isna(row[target_col]):\n",
    "                key = row[col]\n",
    "                if key in group_medians.index:\n",
    "                    row[target_col] = group_medians.loc[key]\n",
    "            return row\n",
    "\n",
    "        df = df.apply(fill, axis=1)\n",
    "\n",
    "        if df[target_col].isna().sum() == 0:\n",
    "            break\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cbb0f1bd-15c1-4d22-8a62-357846ae9935",
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_cols = ['subvillage', 'ward', 'lga', 'district_code', 'region_code']\n",
    "df['gps_height'] = df['gps_height'].apply(lambda x: pd.NA if x in [0] else x)\n",
    "df['construction_year'] = df['construction_year'].apply(lambda x: pd.NA if x in [0] else x)\n",
    "\n",
    "df = geo_groupwise_fill(df, 'gps_height', geo_cols)\n",
    "df = geo_groupwise_fill_mode(df, 'construction_year', geo_cols)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa64b08-6428-4b44-ada8-6bd2cd2a4dc6",
   "metadata": {},
   "source": [
    "from the exploratory analysis, we know that there are only two NaN values in the column wtp_name. Since there already is label called 'none', we label these two missing values also as none."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c3964a43-aa5e-4a58-a450-6e6f5fcc1b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['wpt_name'] = df['wpt_name'].fillna('none')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4336480d-c4e6-4d8b-9f04-ed9201d4663f",
   "metadata": {},
   "source": [
    "same thing, different column ('management_scheme'), different label ('other')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bbc6f5b5-a667-483a-8a7f-491866ca5dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['scheme_management'] = df['scheme_management'].fillna('other')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0244d54e-2c87-4a78-9cf4-2eeb762c7b8a",
   "metadata": {},
   "source": [
    "Columns with High cardinality, lets handle them by keeping the values that together make up the top 50% of the dataset, and label the rest as \"other\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8d35a5c4-1bdd-4408-8ec9-b0c3d750943f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relabel_by_coverage(df, column, threshold=0.5, new_label='other'):\n",
    "    \"\"\"\n",
    "    Keep only the categories in `column` that cumulatively make up `threshold` (e.g., 0.5 for 50%) of rows.\n",
    "    Others are relabeled as `new_label`.\n",
    "    \"\"\"\n",
    "    # Get value counts and cumulative percentage\n",
    "    value_counts = df[column].value_counts(normalize=True)\n",
    "    cumulative = value_counts.cumsum()\n",
    "\n",
    "    # Categories to keep: top ones covering up to the threshold\n",
    "    keep_labels = cumulative[cumulative <= threshold].index.tolist()\n",
    "\n",
    "    # Apply relabeling\n",
    "    df[column] = df[column].apply(lambda x: x if x in keep_labels else new_label)\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c7e24eb4-2f68-4121-8baa-81ccfd18bac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = relabel_by_coverage(df, 'funder', threshold=0.5)\n",
    "df = relabel_by_coverage(df, 'installer', threshold=0.5)\n",
    "df = relabel_by_coverage(df, 'scheme_name', threshold=0.5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e557f5d-207f-422e-b380-22f43716d3a5",
   "metadata": {},
   "source": [
    "filled in quite some columns. Check again what's missing?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "78032ee9-7576-4e2f-a653-7ec246eb875c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['public_meeting', 'permit']\n"
     ]
    }
   ],
   "source": [
    "cols_with_nan = df.columns[df.isna().any()].tolist()\n",
    "print(cols_with_nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bf626ac7-065f-4524-b22e-d47df2ecdba3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on known data: 0.9139\n",
      "✅ Missing permit values filled using model.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Step 1: Select features\n",
    "features = ['management', 'scheme_management', 'scheme_name','payment', 'population','basin','funder','installer']\n",
    "df_model = df[features + ['permit']].copy()\n",
    "\n",
    "# Step 2: One-hot encode categorical features\n",
    "df_model = pd.get_dummies(df_model, columns=features, drop_first=True)\n",
    "\n",
    "# Step 3: Separate known and unknown permit rows\n",
    "df_known = df_model[df_model['permit'].notna()]\n",
    "df_unknown = df_model[df_model['permit'].isna()]\n",
    "\n",
    "# Step 4: Train/test split on known data\n",
    "X = df_known.drop(columns='permit')\n",
    "y = df_known['permit'].astype(bool)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y\n",
    ")\n",
    "\n",
    "# Step 5: Train model\n",
    "clf = RandomForestClassifier(random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Step 6: Evaluate model\n",
    "y_pred = clf.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy on known data: {accuracy:.4f}\")\n",
    "\n",
    "# Step 7: If accuracy is acceptable, predict missing permit values\n",
    "if accuracy > 0.8:  # or your own threshold\n",
    "    X_missing = df_unknown.drop(columns='permit')\n",
    "    predicted_permit = clf.predict(X_missing)\n",
    "    df.loc[df['permit'].isna(), 'permit'] = predicted_permit\n",
    "    print(\"✅ Missing permit values filled using model.\")\n",
    "else:\n",
    "    print(\"⚠️ Accuracy too low. Consider another method or more features.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0f92230b-3f5e-479e-9819-f9d16800db19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on known data: 0.9530\n",
      "✅ Missing permit values filled using model.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Step 1: Select features\n",
    "features = ['management', 'scheme_management', 'scheme_name','payment', 'population','basin','funder','installer','permit']\n",
    "df_model = df[features + ['public_meeting']].copy()\n",
    "\n",
    "# Step 2: One-hot encode categorical features\n",
    "df_model = pd.get_dummies(df_model, columns=features, drop_first=True)\n",
    "\n",
    "# Step 3: Separate known and unknown permit rows\n",
    "df_known = df_model[df_model['public_meeting'].notna()]\n",
    "df_unknown = df_model[df_model['public_meeting'].isna()]\n",
    "\n",
    "# Step 4: Train/test split on known data\n",
    "X = df_known.drop(columns='public_meeting')\n",
    "y = df_known['public_meeting'].astype(bool)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y\n",
    ")\n",
    "\n",
    "# Step 5: Train model\n",
    "clf = RandomForestClassifier(random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Step 6: Evaluate model\n",
    "y_pred = clf.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy on known data: {accuracy:.4f}\")\n",
    "\n",
    "# Step 7: If accuracy is acceptable, predict missing permit values\n",
    "if accuracy > 0.8:  # or your own threshold\n",
    "    X_missing = df_unknown.drop(columns='public_meeting')\n",
    "    predicted_permit = clf.predict(X_missing)\n",
    "    df.loc[df['public_meeting'].isna(), 'public_meeting'] = predicted_permit\n",
    "    print(\"✅ Missing permit values filled using model.\")\n",
    "else:\n",
    "    print(\"⚠️ Accuracy too low. Consider another method or more features.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c1ddfe-18b6-4ff1-9aaa-7a3855139321",
   "metadata": {},
   "source": [
    "OK, this fixes the columns we are interested in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "88912700-69a2-40e2-b96e-7e27ea372bad",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'geopandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_19652\\1188210120.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mgeopandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mgpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# Read the shapefile (Tanzania boundaries)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mgdf_tz\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"../data/gadm41_TZA_shp/gadm41_TZA_1.shp\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'geopandas'"
     ]
    }
   ],
   "source": [
    "import geopandas as gpd\n",
    "\n",
    "# Read the shapefile (Tanzania boundaries)\n",
    "gdf_tz = gpd.read_file(\"../data/gadm41_TZA_shp/gadm41_TZA_1.shp\")\n",
    "\n",
    "# Check projection\n",
    "print(gdf_tz.crs)\n",
    "gdf_tz.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf77f07-cb89-45dd-b933-8f319aa293ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from shapely.geometry import Point\n",
    "\n",
    "# Drop rows with missing coordinates\n",
    "df_geo = df.dropna(subset=['latitude', 'longitude']).copy()\n",
    "\n",
    "# Create Point geometry from long/lat\n",
    "df_geo['geometry'] = df_geo.apply(lambda row: Point(row['longitude'], row['latitude']), axis=1)\n",
    "\n",
    "# Convert to GeoDataFrame with WGS84 CRS\n",
    "gdf_pumps = gpd.GeoDataFrame(df_geo, geometry='geometry', crs=\"EPSG:4326\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58367445-5261-4d2e-8408-391413393516",
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium\n",
    "\n",
    "# Center the map on Tanzania\n",
    "center_lat, center_lon = -6.3690, 34.8888\n",
    "m = folium.Map(location=[center_lat, center_lon], zoom_start=6)\n",
    "\n",
    "# Add Tanzania region boundaries\n",
    "folium.GeoJson(gdf_tz, name=\"Tanzania Regions\").add_to(m)\n",
    "\n",
    "# Define color mapping for status groups\n",
    "color_map = {\n",
    "    'functional': 'green',\n",
    "    'non functional': 'red',\n",
    "    'functional needs repair': 'orange'\n",
    "}\n",
    "\n",
    "# Add water pump markers colored by status\n",
    "for _, row in gdf_pumps.iterrows():\n",
    "    folium.CircleMarker(\n",
    "        location=[row['latitude'], row['longitude']],\n",
    "        radius=2,\n",
    "        color=color_map.get(row['status_group'], 'gray'),\n",
    "        fill=True,\n",
    "        fill_opacity=0.6,\n",
    "        popup=folium.Popup(\n",
    "    f\"Pump ID: {row['id']}<br>Region: {row['region']}<br>Status: {row['status_group']}\",\n",
    "    max_width=250\n",
    ")\n",
    "\n",
    "    ).add_to(m)\n",
    "\n",
    "# Optional: add layer control\n",
    "folium.LayerControl().add_to(m)\n",
    "\n",
    "# Show map (in Jupyter) or save to file\n",
    "#m.save(\"tanzania_water_pumps_map.html\")\n",
    "m\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "24e80023-58f7-4743-8e32-3c97d0f1780e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('your_filename.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4f5dc2b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting geopandas\n",
      "  Downloading geopandas-1.0.1-py3-none-any.whl (323 kB)\n",
      "     -------------------------------------- 323.6/323.6 kB 5.0 MB/s eta 0:00:00\n",
      "Collecting folium\n",
      "  Downloading folium-0.19.5-py2.py3-none-any.whl (110 kB)\n",
      "     -------------------------------------- 110.9/110.9 kB 6.7 MB/s eta 0:00:00\n",
      "Collecting shapely\n",
      "  Downloading shapely-2.0.7-cp39-cp39-win_amd64.whl (1.4 MB)\n",
      "     ---------------------------------------- 1.4/1.4 MB 23.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: pandas>=1.4.0 in c:\\users\\14ame\\anaconda3\\lib\\site-packages (from geopandas) (1.4.4)\n",
      "Collecting numpy>=1.22\n",
      "  Downloading numpy-2.0.2-cp39-cp39-win_amd64.whl (15.9 MB)\n",
      "     --------------------------------------- 15.9/15.9 MB 34.4 MB/s eta 0:00:00\n",
      "Collecting pyogrio>=0.7.2\n",
      "  Downloading pyogrio-0.10.0-cp39-cp39-win_amd64.whl (16.2 MB)\n",
      "     --------------------------------------- 16.2/16.2 MB 32.7 MB/s eta 0:00:00\n",
      "Requirement already satisfied: packaging in c:\\users\\14ame\\anaconda3\\lib\\site-packages (from geopandas) (21.3)\n",
      "Collecting pyproj>=3.3.0\n",
      "  Downloading pyproj-3.6.1-cp39-cp39-win_amd64.whl (6.1 MB)\n",
      "     ---------------------------------------- 6.1/6.1 MB 18.5 MB/s eta 0:00:00\n",
      "Collecting xyzservices\n",
      "  Downloading xyzservices-2025.1.0-py3-none-any.whl (88 kB)\n",
      "     ---------------------------------------- 88.4/88.4 kB ? eta 0:00:00\n",
      "Requirement already satisfied: jinja2>=2.9 in c:\\users\\14ame\\anaconda3\\lib\\site-packages (from folium) (2.11.3)\n",
      "Collecting branca>=0.6.0\n",
      "  Using cached branca-0.8.1-py3-none-any.whl (26 kB)\n",
      "Requirement already satisfied: requests in c:\\users\\14ame\\anaconda3\\lib\\site-packages (from folium) (2.28.1)\n",
      "Collecting jinja2>=2.9\n",
      "  Using cached jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\14ame\\anaconda3\\lib\\site-packages (from jinja2>=2.9->folium) (2.0.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\14ame\\anaconda3\\lib\\site-packages (from pandas>=1.4.0->geopandas) (2022.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\14ame\\anaconda3\\lib\\site-packages (from pandas>=1.4.0->geopandas) (2.8.2)\n",
      "Requirement already satisfied: certifi in c:\\users\\14ame\\anaconda3\\lib\\site-packages (from pyogrio>=0.7.2->geopandas) (2022.9.14)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\14ame\\anaconda3\\lib\\site-packages (from packaging->geopandas) (3.0.9)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\14ame\\anaconda3\\lib\\site-packages (from requests->folium) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\14ame\\anaconda3\\lib\\site-packages (from requests->folium) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\14ame\\anaconda3\\lib\\site-packages (from requests->folium) (1.26.11)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\14ame\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.1->pandas>=1.4.0->geopandas) (1.16.0)\n",
      "Installing collected packages: xyzservices, pyproj, numpy, jinja2, shapely, pyogrio, branca, geopandas, folium\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.21.5\n",
      "    Uninstalling numpy-1.21.5:\n",
      "      Successfully uninstalled numpy-1.21.5\n",
      "  Attempting uninstall: jinja2\n",
      "    Found existing installation: Jinja2 2.11.3\n",
      "    Uninstalling Jinja2-2.11.3:\n",
      "      Successfully uninstalled Jinja2-2.11.3\n",
      "Successfully installed branca-0.8.1 folium-0.19.5 geopandas-1.0.1 jinja2-3.1.6 numpy-2.0.2 pyogrio-0.10.0 pyproj-3.6.1 shapely-2.0.7 xyzservices-2025.1.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "daal4py 2021.6.0 requires daal==2021.4.0, which is not installed.\n",
      "tensorflow-intel 2.11.0 requires keras<2.12,>=2.11.0, but you have keras 2.10.0 which is incompatible.\n",
      "tensorflow-intel 2.11.0 requires tensorboard<2.12,>=2.11, but you have tensorboard 2.10.1 which is incompatible.\n",
      "tensorflow-intel 2.11.0 requires tensorflow-estimator<2.12,>=2.11.0, but you have tensorflow-estimator 2.10.0 which is incompatible.\n",
      "scipy 1.9.1 requires numpy<1.25.0,>=1.18.5, but you have numpy 2.0.2 which is incompatible.\n",
      "numba 0.55.1 requires numpy<1.22,>=1.18, but you have numpy 2.0.2 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "pip install geopandas folium shapely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c6097bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
