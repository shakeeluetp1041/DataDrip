{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7ddad3c-6539-499b-a986-2e68b836b935",
   "metadata": {},
   "source": [
    "This should be the final pipeline for the cleaning and preparing the dataset. End result is a data frame, ready to be tossed into a ML model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7cbe85b4-2efa-4669-a8e5-9e06a3f16108",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b553d731-5df2-4d74-8a66-96b6b4a4456c",
   "metadata": {},
   "source": [
    "Loading the data, and convert all the strings to lower case for uniformity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2586bc1f-d182-417a-b63c-c5cd74091fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_0 = pd.read_csv(\"../data/Training_Set_Values.csv\")\n",
    "df_1 = pd.read_csv(\"../data/Training_Set_labels.csv\")\n",
    "df = df_0.merge(df_1, on='id')\n",
    "for col in df.select_dtypes(include='object').columns:\n",
    "    df[col] = df[col].map(lambda x: x.lower() if isinstance(x, str) else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6c89f0fd-2e3c-46f9-9c95-c8a7c6f70faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Replace with your actual target column name\n",
    "target_col = 'status_group'  # Example: use your actual label column\n",
    "\n",
    "X = df.drop(columns=[target_col])  # Drop target and ID from features\n",
    "y = df[target_col]\n",
    "\n",
    "# Stratified train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.25, random_state=42, stratify=y\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f582f541-b7e3-427e-9577-73c6021b26e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14850"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_test_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0baa08ed-31e5-4a0c-b0ce-e517f2f696da",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_ids = X_test['id'].copy()  # Store before dropping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "39cdfd97-37d0-4436-9e88-02f4b2c5521b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns to be dropped for the baseline models\n",
    "columns_to_drop=['id','amount_tsh','num_private','recorded_by','scheme_name',\n",
    "              'extraction_type_group','extraction_type_class',\n",
    "              'management','payment_type','quality_group','quantity_group','source','waterpoint_type_group',\n",
    "              'funder','installer','wpt_name','scheme_management']\n",
    "\n",
    "X_test = X_test.drop(columns=columns_to_drop)\n",
    "X_train = X_train.drop(columns=columns_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "614e6edd-ca49-4f5f-800a-50d274ce29ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "class GeoContextImputer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.geo_group_cols = ['subvillage', 'ward', 'lga', 'district_code', 'region_code']\n",
    "        self.pop_group_cols = ['subvillage', 'ward', 'lga', 'district_code', 'region_code']\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "    \n",
    "        # Handle subvillage\n",
    "        if 'subvillage' in X.columns:\n",
    "            X['subvillage'] = X['subvillage'].replace('', pd.NA)\n",
    "            X['subvillage'] = X.apply(lambda row: self.fill_subvillage(row, X), axis=1)\n",
    "    \n",
    "        # Handle geo columns\n",
    "        for col in ['latitude', 'longitude']:\n",
    "            if col in X.columns:\n",
    "                X[col] = X[col].apply(lambda x: pd.NA if abs(x) < 1e-6 else x)\n",
    "    \n",
    "        if 'latitude' in X.columns and 'longitude' in X.columns:\n",
    "            X = self.fill_missing_geo(X, self.geo_group_cols)\n",
    "    \n",
    "        # Handle population\n",
    "        if 'population' in X.columns:\n",
    "            X['population'] = X['population'].apply(lambda x: pd.NA if x in [0, 1] else x)\n",
    "            X = self.fill_missing_population(X, self.pop_group_cols)\n",
    "    \n",
    "        # Handle gps_height and construction_year\n",
    "        if 'gps_height' in X.columns:\n",
    "            X['gps_height'] = X['gps_height'].apply(lambda x: pd.NA if x == 0 else x)\n",
    "            X = self.geo_groupwise_fill(X, 'gps_height', self.geo_group_cols)\n",
    "    \n",
    "        if 'construction_year' in X.columns:\n",
    "            X['construction_year'] = X['construction_year'].apply(lambda x: pd.NA if x == 0 else x)\n",
    "            X = self.geo_groupwise_fill_mode(X, 'construction_year', self.geo_group_cols)\n",
    "    \n",
    "        # Simple fill\n",
    "        if 'wpt_name' in X.columns:\n",
    "            X['wpt_name'] = X['wpt_name'].fillna('none')\n",
    "        if 'scheme_management' in X.columns:\n",
    "            X['scheme_management'] = X['scheme_management'].fillna('other')\n",
    "    \n",
    "        # Relabel by coverage (robust)\n",
    "        for col in ['funder', 'installer', 'scheme_name']:\n",
    "            if col in X.columns:\n",
    "                X = self.relabel_by_coverage(X, col, threshold=0.5)\n",
    "    \n",
    "        return X.replace({pd.NA: np.nan})\n",
    "\n",
    "    def fill_subvillage(self, row, df):\n",
    "        if pd.isna(row['subvillage']):\n",
    "            ward_mode = df[df['ward'] == row['ward']]['subvillage'].mode()\n",
    "            if not ward_mode.empty:\n",
    "                return ward_mode[0]\n",
    "\n",
    "            lga_mode = df[df['lga'] == row['lga']]['subvillage'].mode()\n",
    "            if not lga_mode.empty:\n",
    "                return lga_mode[0]\n",
    "\n",
    "            district_mode = df[df['district_code'] == row['district_code']]['subvillage'].mode()\n",
    "            if not district_mode.empty:\n",
    "                return district_mode[0]\n",
    "\n",
    "            return 'unknown'\n",
    "        return row['subvillage']\n",
    "\n",
    "    def fill_missing_geo(self, df, group_cols):\n",
    "        for col in group_cols:\n",
    "            group_means = df.dropna(subset=['latitude', 'longitude']).groupby(col)[['latitude', 'longitude']].mean()\n",
    "\n",
    "            def fill(row):\n",
    "                if pd.isna(row['latitude']) or pd.isna(row['longitude']):\n",
    "                    key = row[col]\n",
    "                    if key in group_means.index:\n",
    "                        if pd.isna(row['latitude']):\n",
    "                            row['latitude'] = group_means.loc[key, 'latitude']\n",
    "                        if pd.isna(row['longitude']):\n",
    "                            row['longitude'] = group_means.loc[key, 'longitude']\n",
    "                return row\n",
    "\n",
    "            df = df.apply(fill, axis=1)\n",
    "\n",
    "            if df['latitude'].isna().sum() == 0 and df['longitude'].isna().sum() == 0:\n",
    "                break\n",
    "\n",
    "        return df\n",
    "\n",
    "    def fill_missing_population(self, df, levels):\n",
    "        for level in levels:\n",
    "            group_medians = df.dropna(subset=['population']).groupby(level)['population'].median()\n",
    "\n",
    "            def fill(row):\n",
    "                if pd.isna(row['population']):\n",
    "                    key = row[level]\n",
    "                    if key in group_medians:\n",
    "                        row['population'] = group_medians.loc[key]\n",
    "                return row\n",
    "\n",
    "            df = df.apply(fill, axis=1)\n",
    "            if df['population'].isna().sum() == 0:\n",
    "                break\n",
    "\n",
    "        return df\n",
    "\n",
    "    def geo_groupwise_fill(self, df, target_col, group_cols):\n",
    "        for col in group_cols:\n",
    "            group_medians = df.dropna(subset=[target_col]).groupby(col)[target_col].median()\n",
    "\n",
    "            def fill(row):\n",
    "                if pd.isna(row[target_col]):\n",
    "                    key = row[col]\n",
    "                    if key in group_medians.index:\n",
    "                        row[target_col] = group_medians.loc[key]\n",
    "                return row\n",
    "\n",
    "            df = df.apply(fill, axis=1)\n",
    "            if df[target_col].isna().sum() == 0:\n",
    "                break\n",
    "\n",
    "        return df\n",
    "\n",
    "    def geo_groupwise_fill_mode(self, df, target_col, group_cols):\n",
    "        for col in group_cols:\n",
    "            group_modes = df.dropna(subset=[target_col]).groupby(col)[target_col].agg(lambda x: x.mode().iloc[0])\n",
    "\n",
    "            def fill(row):\n",
    "                if pd.isna(row[target_col]):\n",
    "                    key = row[col]\n",
    "                    if key in group_modes.index:\n",
    "                        row[target_col] = group_modes.loc[key]\n",
    "                return row\n",
    "\n",
    "            df = df.apply(fill, axis=1)\n",
    "            if df[target_col].isna().sum() == 0:\n",
    "                break\n",
    "\n",
    "        return df\n",
    "\n",
    "    def relabel_by_coverage(self, df, column, threshold=0.5, new_label='other'):\n",
    "        value_counts = df[column].value_counts(normalize=True)\n",
    "        cumulative = value_counts.cumsum()\n",
    "        keep_labels = cumulative[cumulative <= threshold].index.tolist()\n",
    "        df[column] = df[column].apply(lambda x: x if x in keep_labels else new_label)\n",
    "        return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c7cee93f-958e-4650-a9fc-980c6a2f2750",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the imputer manually to determine available columns\n",
    "imputer = GeoContextImputer()\n",
    "X_train_imputed = imputer.fit_transform(X_train.copy())\n",
    "\n",
    "# Recompute valid column types after imputation\n",
    "numeric_features = X_train_imputed.select_dtypes(include='number').columns.tolist()\n",
    "categorical_features = X_train_imputed.select_dtypes(include='object').columns.tolist()\n",
    "\n",
    "# Now define the transformers as usual\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "numeric_transformer = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "categorical_transformer = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('encoder', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('num', numeric_transformer, numeric_features),\n",
    "    ('cat', categorical_transformer, categorical_features)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bbee5767-bd56-4e92-a51a-829e2dd152d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "full_pipeline = Pipeline([\n",
    "    ('context_imputer', GeoContextImputer()),  # ← Your custom logic\n",
    "    ('preprocessor', preprocessor),            # ← ColumnTransformer\n",
    "    ('classifier', RandomForestClassifier(random_state=42, class_weight='balanced'))  # or any model\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "584d274f-d0d5-4ebc-8647-af3336e968f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_pipeline.fit(X_train, y_train)\n",
    "preds = full_pipeline.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "673d3b64-7dd5-47c2-98ad-b13543b8dd56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8072727272727273\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print(accuracy_score(y_test, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3897759e-3bde-4b2a-857f-9a256ee7b0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_3 = pd.read_csv(\"../data/Test_Set_Values.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6c4b3687-4e75-4c74-a03f-c0aa4f54bb27",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_test = full_pipeline.predict(df_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ecf33772-7ce5-4d9a-b845-850b5d40d0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Then build submission\n",
    "submission = pd.DataFrame({\n",
    "    \"id\": df_3[\"id\"].values,\n",
    "    \"status_group\": preds_test\n",
    "})\n",
    "submission.to_csv(\"submission.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad17760-8261-418b-82f1-3d12624dcf94",
   "metadata": {},
   "source": [
    "Submitting resut on drivendata with testset, public score = 0.76"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "48f43bf6-c1a4-4150-a3b9-e7baeaefb1ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "params = {\n",
    "    \"classifier__n_estimators\": [100, 300],\n",
    "    \"classifier__max_depth\": [10, 20, None]\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(\n",
    "    full_pipeline, param_grid=params, cv=3, scoring='accuracy', n_jobs=-1\n",
    ")\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "best_model = grid.best_estimator_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6bddab84-a3f1-406c-a1f9-9b8cac921953",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=[('context_imputer', GeoContextImputer()),\n",
      "                ('preprocessor',\n",
      "                 ColumnTransformer(transformers=[('num',\n",
      "                                                  Pipeline(steps=[('imputer',\n",
      "                                                                   SimpleImputer()),\n",
      "                                                                  ('scaler',\n",
      "                                                                   StandardScaler())]),\n",
      "                                                  ['gps_height', 'longitude',\n",
      "                                                   'latitude', 'region_code',\n",
      "                                                   'district_code',\n",
      "                                                   'population',\n",
      "                                                   'construction_year']),\n",
      "                                                 ('cat',\n",
      "                                                  Pipeline(steps=[('imputer',\n",
      "                                                                   SimpleImputer(strategy='most_...\n",
      "                                                                   OneHotEncoder(handle_unknown='ignore'))]),\n",
      "                                                  ['date_recorded', 'basin',\n",
      "                                                   'subvillage', 'region',\n",
      "                                                   'lga', 'ward',\n",
      "                                                   'public_meeting', 'permit',\n",
      "                                                   'extraction_type',\n",
      "                                                   'management_group',\n",
      "                                                   'payment', 'water_quality',\n",
      "                                                   'quantity', 'source_type',\n",
      "                                                   'source_class',\n",
      "                                                   'waterpoint_type'])])),\n",
      "                ('classifier',\n",
      "                 RandomForestClassifier(class_weight='balanced',\n",
      "                                        n_estimators=300, random_state=42))])\n"
     ]
    }
   ],
   "source": [
    "print(best_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "60201028-ca72-4f54-b334-8d0fecc812ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Automatically select column types\n",
    "numeric_features = X.select_dtypes(include='number').columns.tolist()\n",
    "categorical_features = X.select_dtypes(include='object').columns.tolist()\n",
    "\n",
    "# Preprocessing for numeric columns\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# Preprocessing for categorical columns\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Full column transformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3a9d67a1-fefb-4d13-ad4c-34790c260e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "\n",
    "# Define classifiers\n",
    "models = {\n",
    "    \"Decision Tree\": DecisionTreeClassifier(random_state=42),\n",
    "    \"Random Forest\": RandomForestClassifier(random_state=42),\n",
    "    \"Extra Trees\": ExtraTreesClassifier(random_state=42),\n",
    "    \"Gradient Boosting\": GradientBoostingClassifier(random_state=42),\n",
    "    \"SVC\": SVC(probability=True, random_state=42),\n",
    "    \"XGBoost\": xgb.XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', random_state=42),\n",
    "    \"LightGBM\": lgb.LGBMClassifier(random_state=42)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d1bbd11e-996f-4000-943f-722667640020",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Build pipelines\n",
    "pipelines = {\n",
    "    name: Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('classifier', model)\n",
    "    ]) for name, model in models.items()\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "af5bbf47-6f9c-4e03-8120-65a5f4478994",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Decision Tree...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, pipeline \u001b[38;5;129;01min\u001b[39;00m pipelines\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 8\u001b[0m     \u001b[43mpipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m pipeline\u001b[38;5;241m.\u001b[39mpredict(X_test)\n\u001b[1;32m     10\u001b[0m     report \u001b[38;5;241m=\u001b[39m classification_report(y_test, y_pred, output_dict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1387\u001b[0m     )\n\u001b[1;32m   1388\u001b[0m ):\n\u001b[0;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/pipeline.py:662\u001b[0m, in \u001b[0;36mPipeline.fit\u001b[0;34m(self, X, y, **params)\u001b[0m\n\u001b[1;32m    656\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_final_estimator \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassthrough\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    657\u001b[0m         last_step_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_metadata_for_step(\n\u001b[1;32m    658\u001b[0m             step_idx\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m    659\u001b[0m             step_params\u001b[38;5;241m=\u001b[39mrouted_params[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m0\u001b[39m]],\n\u001b[1;32m    660\u001b[0m             all_params\u001b[38;5;241m=\u001b[39mparams,\n\u001b[1;32m    661\u001b[0m         )\n\u001b[0;32m--> 662\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_final_estimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mlast_step_params\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfit\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    664\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1387\u001b[0m     )\n\u001b[1;32m   1388\u001b[0m ):\n\u001b[0;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/tree/_classes.py:1024\u001b[0m, in \u001b[0;36mDecisionTreeClassifier.fit\u001b[0;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[1;32m    993\u001b[0m \u001b[38;5;129m@_fit_context\u001b[39m(prefer_skip_nested_validation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    994\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, check_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m    995\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Build a decision tree classifier from the training set (X, y).\u001b[39;00m\n\u001b[1;32m    996\u001b[0m \n\u001b[1;32m    997\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1021\u001b[0m \u001b[38;5;124;03m        Fitted estimator.\u001b[39;00m\n\u001b[1;32m   1022\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1024\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1025\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1026\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1027\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1028\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheck_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1029\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1030\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/tree/_classes.py:472\u001b[0m, in \u001b[0;36mBaseDecisionTree._fit\u001b[0;34m(self, X, y, sample_weight, check_input, missing_values_in_feature_mask)\u001b[0m\n\u001b[1;32m    461\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    462\u001b[0m     builder \u001b[38;5;241m=\u001b[39m BestFirstTreeBuilder(\n\u001b[1;32m    463\u001b[0m         splitter,\n\u001b[1;32m    464\u001b[0m         min_samples_split,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    469\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_impurity_decrease,\n\u001b[1;32m    470\u001b[0m     )\n\u001b[0;32m--> 472\u001b[0m \u001b[43mbuilder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtree_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    474\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_outputs_ \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m is_classifier(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    475\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "import pandas as pd\n",
    "\n",
    "results = {}\n",
    "\n",
    "for name, pipeline in pipelines.items():\n",
    "    print(f\"Training {name}...\")\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "    report = classification_report(y_test, y_pred, output_dict=True)\n",
    "    results[name] = report[\"weighted avg\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc7d7db-eac2-4d7c-8927-5027671d8754",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "results_df = pd.DataFrame(results).T\n",
    "import ace_tools as tools; tools.display_dataframe_to_user(name=\"Model Evaluation Results\", dataframe=results_df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
