{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import sklearn\n",
    "from sklearn.preprocessing import  MinMaxScaler,OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "import lightgbm as lgb\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "scaler_minmax= MinMaxScaler()                                                             # Create a MinMaxScaler object\n",
    "ohe = OneHotEncoder(sparse_output=False, handle_unknown='ignore',drop='first')            # Create a OneHotEncoder object\n",
    "\n",
    "\n",
    "# Read CSV files\n",
    "\n",
    "df=pd.read_csv(\"../data/Training_Set_Values.csv\")       # Read the Training data CSV file\n",
    "name_featrures=df.columns                       # Get the features name\n",
    "len_features=len(name_featrures)                # Get the length of features\n",
    "labels=pd.read_csv(\"../data/Training_Set_labels.csv\")   # Read the labels (target) CSV file\n",
    "labels.head()\n",
    "df['target'] = labels['status_group']           # Add the target column to the dataframe\n",
    "#print(df.shape)                                 # Print the shape of the dataframe\n",
    "#df.head()\n",
    "#df.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape after dropping columns: (59400, 22)\n"
     ]
    }
   ],
   "source": [
    "# Columns to be dropped for the baseline models\n",
    "columns_drop=['id','amount_tsh','num_private','subvillage','recorded_by','scheme_name',\n",
    "              'extraction_type_group','extraction_type_class',\n",
    "              'management','payment_type','quality_group','quantity_group','source','waterpoint_type_group',\n",
    "              'funder','installer','wpt_name','ward','scheme_management']\n",
    "\n",
    "df = df.drop(columns=columns_drop)\n",
    "print('Shape after dropping columns:', df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the target column to labels \n",
    "#print(df['target'].unique())\n",
    "target_map_dict={'functional': 2, 'functional needs repair': 1, 'non functional': 0} # Defined the mapping of labels to numbers (integers)\n",
    "#print(df['target'].head())\n",
    "df['target'] =df['target'].map(target_map_dict) # transform the target column (labels) to  numbers (integers)\n",
    "#df['target'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and target and perform train test split\n",
    "X = df.drop(columns=['target'])  # Features only\n",
    "y = df['target']                 # Target column\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.04, random_state=42, stratify=y)  # 2376 records for the test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "y_enc = le.fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the custom transformers form helper_function.py\n",
    "# The helper_function.py file contains the definitions for StringConverter, YearExtractor, IQRCapper, and ConstructionYearTransformer\n",
    "from helper_function import (\n",
    "    StringConverter,\n",
    "    YearExtractor,\n",
    "    IQRCapper,\n",
    "    ConstructionYearTransformer,\n",
    "    ObjectToNumericConverter\n",
    ")\n",
    "    \n",
    "#pipeline transformers\n",
    "date_recorded_transformer_pipeline=Pipeline([\n",
    "    \n",
    "    ('year_extractor',YearExtractor()),\n",
    "    ('onehot', OneHotEncoder(sparse_output=False, handle_unknown='ignore',drop='first'))\n",
    "])\n",
    "\n",
    "\n",
    "oulier_minmax_pipeline_clip = Pipeline(steps=[\n",
    "    ('iqr_cap', IQRCapper(strategy='clip')),\n",
    "    ('scaler', MinMaxScaler())\n",
    "])\n",
    "\n",
    "oulier_minmax_pipeline_mean = Pipeline(steps=[\n",
    "    ('iqr_cap', IQRCapper(strategy='mean')),\n",
    "    ('scaler', MinMaxScaler())\n",
    "])\n",
    "\n",
    "oulier_minmax_pipeline_median = Pipeline(steps=[\n",
    "    ('iqr_cap', IQRCapper(strategy='median')),\n",
    "    ('scaler', MinMaxScaler())\n",
    "])\n",
    "\n",
    "\n",
    "cat_pipeline = Pipeline([\n",
    "     ('string_converter', StringConverter()),\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='Unknown')),\n",
    "    ('ohe', OneHotEncoder(sparse_output=False, handle_unknown='ignore',drop='first'))\n",
    "])\n",
    "\n",
    "constructionyear_pipeline = Pipeline(steps=[\n",
    "    ('replace_zeros_with_median', ConstructionYearTransformer()),\n",
    "    ('minmax_scaling', MinMaxScaler())\n",
    "])\n",
    "# ColumnTransformer and full pipeline setup for feature preprocessing\n",
    "# The ColumnTransformer allows us to apply different preprocessing steps to different columns of the DataFrame\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('date', date_recorded_transformer_pipeline, ['date_recorded']),\n",
    "        #('gps_height', scaler_minmax, ['gps_height']),\n",
    "        ('outlier_minmax_gps_height', oulier_minmax_pipeline_mean, ['gps_height']),\n",
    "        ('outlier_minmax_longitude', oulier_minmax_pipeline_mean, ['longitude']),\n",
    "        ('outlier_minmax_latitude', oulier_minmax_pipeline_mean, ['latitude']),\n",
    "         ('cat_ohe', cat_pipeline, ['basin','region','region_code','lga','public_meeting','permit','extraction_type','management_group','payment','water_quality','quantity','source_type','source_class','waterpoint_type']),\n",
    "        ('outlier_minmax_population', oulier_minmax_pipeline_clip, ['population']),\n",
    "        ('constructionyear', constructionyear_pipeline, ['construction_year'])\n",
    "\n",
    "\n",
    "    ],\n",
    "    remainder='passthrough',\n",
    "    verbose_feature_names_out=False\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "preprocess_pipeline = Pipeline([\n",
    "    ('preprocessing', preprocessor),\n",
    "    ('object_to_numeric', ObjectToNumericConverter())  # your custom step\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# models (Decision Tree, Random Forest, XGBoost) to be used\n",
    "models = {    \n",
    "    \"Decision Tree\": DecisionTreeClassifier(\n",
    "        max_depth=10,  # You can tune this\n",
    "        random_state=42\n",
    "    ),\n",
    "    \"Random Forest\": RandomForestClassifier(\n",
    "        n_estimators=100,\n",
    "        max_depth=15,  # You can tune this too\n",
    "        random_state=42\n",
    "    ),\"XGBoost\": XGBClassifier(\n",
    "        n_estimators=100,\n",
    "        max_depth=11,\n",
    "        learning_rate=0.1,\n",
    "        subsample=0.9,\n",
    "        colsample_bytree=0.8,\n",
    "        #use_label_encoder=False,\n",
    "        eval_metric='mlogloss',    # good for multi-class\n",
    "        objective='multi:softmax', # directly outputs class labels\n",
    "        num_class=3,               # number of target classes\n",
    "        random_state=42\n",
    "    ),\n",
    "    \"SVC\": SVC(\n",
    "    kernel='rbf',\n",
    "    C=1.0,\n",
    "    class_weight='balanced',\n",
    "    probability=True  # if you need .predict_proba\n",
    "    ),\n",
    "    \"Gradient Boosting\": GradientBoostingClassifier(\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.1,\n",
    "    random_state=42\n",
    "    ),\n",
    "    \"Extra Trees\": ExtraTreesClassifier(\n",
    "    n_estimators=100,\n",
    "    random_state=42\n",
    "    ),\n",
    "    \"LightGBM\": lgb.LGBMClassifier(\n",
    "    objective='multiclass',\n",
    "    num_class=len(le.classes_),\n",
    "    random_state=42\n",
    "    ),\n",
    "    \"Polynomial\": Pipeline([\n",
    "    ('poly', PolynomialFeatures(degree=2)),\n",
    "    ('scaler', StandardScaler()),        # often useful after poly\n",
    "    ('clf', LogisticRegression(\n",
    "        multi_class='multinomial',\n",
    "        solver='saga',\n",
    "        max_iter=500\n",
    "    ))\n",
    "    ]),\n",
    "    \"Ridge\": LogisticRegression(\n",
    "    penalty='l2',\n",
    "    C=1.0,\n",
    "    multi_class='multinomial',\n",
    "    solver='saga'\n",
    "    ),\n",
    "    \"Lasso\": LogisticRegression(\n",
    "    penalty='l1',\n",
    "    C=1.0,\n",
    "    multi_class='multinomial',\n",
    "    solver='saga'\n",
    "    ),\n",
    "    \"ElasticNet\": LogisticRegression(\n",
    "    penalty='elasticnet',\n",
    "    l1_ratio=0.5,\n",
    "    C=1.0,\n",
    "    multi_class='multinomial',\n",
    "    solver='saga'\n",
    "    )\n",
    "}\n",
    "\n",
    "# Results and feature importances storage\n",
    "results = {}\n",
    "feature_importances = {}\n",
    "\n",
    "# Loop through each model\n",
    "for name, model in models.items():\n",
    "    full_pipeline = Pipeline([\n",
    "        ('preprocessing', preprocessor),\n",
    "        ('model', model)\n",
    "                            ])\n",
    "\n",
    "\n",
    "    full_pipeline.fit(X_train, y_train) \n",
    "    \"\"\"\n",
    "    # Step 1: Each transformer in the preprocessing pipeline Computes and stores necessary statistics \n",
    "    # (e.g., quartiles, medians, scalers) from X_train only.\n",
    "    #Step 2: The transformers are applied (transformed) to X_train to produce the final preprocessed training features.\n",
    "    # Step 3: The model is trained using these transformed features and y_train.\n",
    "    \"\"\" \n",
    "\n",
    "\n",
    "    # Predictions\n",
    "    y_train_pred = full_pipeline.predict(X_train)\n",
    "    y_test_pred = full_pipeline.predict(X_test)\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    The stored training statistics are used to transform X_train again and X_test (no re-fitting!).\n",
    "\n",
    "    Feeds this transformed data to the already-trained model.\n",
    "\n",
    "    Outputs predictions (y_train_pred, y_test_pred).\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Accuracy scores\n",
    "    train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "    test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "    results[name] = {\n",
    "        \"Train Accuracy\": train_accuracy,\n",
    "        \"Test Accuracy\": test_accuracy\n",
    "    }\n",
    "    # Extract feature importances\n",
    "    fitted_model = full_pipeline.named_steps['model']\n",
    "    \n",
    "    if hasattr(fitted_model, 'feature_importances_'):\n",
    "        # Get transformed feature names from preprocessor\n",
    "        feature_names = full_pipeline.named_steps['preprocessing'].get_feature_names_out()\n",
    "        importances = fitted_model.feature_importances_\n",
    "        feature_importances[name] = sorted(\n",
    "            zip(feature_names, importances),\n",
    "            key=lambda x: x[1],\n",
    "            reverse=True\n",
    "        )\n",
    "\n",
    "# Print results\n",
    "print(\"\\nModel Comparison:\")\n",
    "print(\"{:<15} {:<15} {:<15}\".format(\"Model\", \"Train Acc\", \"Test Acc\"))\n",
    "print(\"-\" * 45)\n",
    "for model_name, scores in results.items():\n",
    "    print(\"{:<15} {:<15.4f} {:<15.4f}\".format(model_name, scores[\"Train Accuracy\"], scores[\"Test Accuracy\"]))\n",
    "\n",
    "# Print top features\n",
    "print(\"\\nTop 10 Important Features:\")\n",
    "for model_name, importance_list in feature_importances.items():\n",
    "    print(f\"\\n{model_name}:\")\n",
    "    for feature, importance in importance_list[:10]:\n",
    "        print(f\"{feature:<30} {importance:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree ▶️ Train Acc: 0.7667999438832772 Test Acc: 0.7483164983164983\n",
      "Top 10 features:\n",
      "  waterpoint_type_other          0.2024\n",
      "  quantity_seasonal              0.1304\n",
      "  quantity_enough                0.1123\n",
      "  quantity_insufficient          0.0860\n",
      "  longitude                      0.0614\n",
      "  construction_year              0.0598\n",
      "  latitude                       0.0412\n",
      "  waterpoint_type_communal standpipe multiple 0.0378\n",
      "  population                     0.0157\n",
      "  gps_height                     0.0151\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Decision Tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "dt_pipeline = Pipeline([\n",
    "    ('preprocessing', preprocessor),\n",
    "    ('model', DecisionTreeClassifier(max_depth=10, random_state=42))\n",
    "])\n",
    "\n",
    "dt_pipeline.fit(X_train, y_train)\n",
    "y_train_pred = dt_pipeline.predict(X_train)\n",
    "y_test_pred  = dt_pipeline.predict(X_test)\n",
    "\n",
    "print(\"Decision Tree ▶️\",\n",
    "      \"Train Acc:\", accuracy_score(y_train, y_train_pred),\n",
    "      \"Test Acc:\",  accuracy_score(y_test, y_test_pred))\n",
    "\n",
    "# Feature importances\n",
    "feat_names = dt_pipeline.named_steps['preprocessing'].get_feature_names_out()\n",
    "importances = dt_pipeline.named_steps['model'].feature_importances_\n",
    "top = sorted(zip(feat_names, importances), key=lambda x: x[1], reverse=True)[:10]\n",
    "print(\"Top 10 features:\")\n",
    "for f, imp in top:\n",
    "    print(f\"  {f:<30} {imp:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest ▶️ Train Acc: 0.8136574074074074 Test Acc: 0.7777777777777778\n",
      "Top 10 features:\n",
      "  quantity_enough                0.0774\n",
      "  waterpoint_type_other          0.0748\n",
      "  longitude                      0.0678\n",
      "  latitude                       0.0634\n",
      "  construction_year              0.0610\n",
      "  extraction_type_other          0.0586\n",
      "  gps_height                     0.0421\n",
      "  population                     0.0306\n",
      "  quantity_insufficient          0.0248\n",
      "  waterpoint_type_communal standpipe 0.0199\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Random Forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf_pipeline = Pipeline([\n",
    "    ('preprocessing', preprocessor),\n",
    "    ('model', RandomForestClassifier(\n",
    "        n_estimators=100, max_depth=15, random_state=42\n",
    "    ))\n",
    "])\n",
    "\n",
    "rf_pipeline.fit(X_train, y_train)\n",
    "y_train_pred = rf_pipeline.predict(X_train)\n",
    "y_test_pred  = rf_pipeline.predict(X_test)\n",
    "\n",
    "print(\"Random Forest ▶️\",\n",
    "      \"Train Acc:\", accuracy_score(y_train, y_train_pred),\n",
    "      \"Test Acc:\",  accuracy_score(y_test, y_test_pred))\n",
    "\n",
    "feat_names = rf_pipeline.named_steps['preprocessing'].get_feature_names_out()\n",
    "importances = rf_pipeline.named_steps['model'].feature_importances_\n",
    "top = sorted(zip(feat_names, importances), key=lambda x: x[1], reverse=True)[:10]\n",
    "print(\"Top 10 features:\")\n",
    "for f, imp in top:\n",
    "    print(f\"  {f:<30} {imp:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost ▶️ Train Acc: 0.8602518237934904 Test Acc: 0.8055555555555556\n",
      "Top 10 features:\n",
      "  waterpoint_type_other          0.0654\n",
      "  lga_Bariadi                    0.0275\n",
      "  quantity_seasonal              0.0269\n",
      "  region_Iringa                  0.0254\n",
      "  extraction_type_other          0.0164\n",
      "  region_code_11                 0.0155\n",
      "  region_code_15                 0.0151\n",
      "  lga_Ngara                      0.0147\n",
      "  lga_Rombo                      0.0145\n",
      "  lga_Chunya                     0.0122\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: XGBoost\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "xgb_pipeline = Pipeline([\n",
    "    ('preprocessing', preprocessor),\n",
    "    ('model', XGBClassifier(\n",
    "        n_estimators=100,\n",
    "        max_depth=11,\n",
    "        learning_rate=0.1,\n",
    "        subsample=0.9,\n",
    "        colsample_bytree=0.8,\n",
    "        objective='multi:softmax',\n",
    "        num_class=len(le.classes_),\n",
    "        eval_metric='mlogloss',\n",
    "        random_state=42\n",
    "    ))\n",
    "])\n",
    "\n",
    "xgb_pipeline.fit(X_train, y_train)\n",
    "y_train_pred = xgb_pipeline.predict(X_train)\n",
    "y_test_pred  = xgb_pipeline.predict(X_test)\n",
    "\n",
    "print(\"XGBoost ▶️\",\n",
    "      \"Train Acc:\", accuracy_score(y_train, y_train_pred),\n",
    "      \"Test Acc:\",  accuracy_score(y_test, y_test_pred))\n",
    "\n",
    "feat_names = xgb_pipeline.named_steps['preprocessing'].get_feature_names_out()\n",
    "importances = xgb_pipeline.named_steps['model'].feature_importances_\n",
    "top = sorted(zip(feat_names, importances), key=lambda x: x[1], reverse=True)[:10]\n",
    "print(\"Top 10 features:\")\n",
    "for f, imp in top:\n",
    "    print(f\"  {f:<30} {imp:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: SVC\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "svc_pipeline = Pipeline([\n",
    "    ('preprocessing', preprocessor),\n",
    "    ('model', SVC(\n",
    "        kernel='rbf', C=1.0,\n",
    "        class_weight='balanced',\n",
    "        probability=True\n",
    "    ))\n",
    "])\n",
    "\n",
    "svc_pipeline.fit(X_train, y_train)\n",
    "y_train_pred = svc_pipeline.predict(X_train)\n",
    "y_test_pred  = svc_pipeline.predict(X_test)\n",
    "\n",
    "print(\"SVC ▶️\",\n",
    "      \"Train Acc:\", accuracy_score(y_train, y_train_pred),\n",
    "      \"Test Acc:\",  accuracy_score(y_test, y_test_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Boosting ▶️ Train Acc: 0.7565761784511784 Test Acc: 0.7554713804713805\n",
      "Top 10 features:\n",
      "  waterpoint_type_other          0.1380\n",
      "  quantity_insufficient          0.1350\n",
      "  quantity_enough                0.1288\n",
      "  extraction_type_other          0.0769\n",
      "  construction_year              0.0765\n",
      "  quantity_seasonal              0.0506\n",
      "  waterpoint_type_communal standpipe multiple 0.0393\n",
      "  longitude                      0.0320\n",
      "  latitude                       0.0232\n",
      "  payment_pay per bucket         0.0187\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Gradient Boosting\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "gb_pipeline = Pipeline([\n",
    "    ('preprocessing', preprocessor),\n",
    "    ('model', GradientBoostingClassifier(\n",
    "        n_estimators=100, learning_rate=0.1, random_state=42\n",
    "    ))\n",
    "])\n",
    "\n",
    "gb_pipeline.fit(X_train, y_train)\n",
    "y_train_pred = gb_pipeline.predict(X_train)\n",
    "y_test_pred  = gb_pipeline.predict(X_test)\n",
    "\n",
    "print(\"Gradient Boosting ▶️\",\n",
    "      \"Train Acc:\", accuracy_score(y_train, y_train_pred),\n",
    "      \"Test Acc:\",  accuracy_score(y_test, y_test_pred))\n",
    "\n",
    "feat_names = gb_pipeline.named_steps['preprocessing'].get_feature_names_out()\n",
    "importances = gb_pipeline.named_steps['model'].feature_importances_\n",
    "top = sorted(zip(feat_names, importances), key=lambda x: x[1], reverse=True)[:10]\n",
    "print(\"Top 10 features:\")\n",
    "for f, imp in top:\n",
    "    print(f\"  {f:<30} {imp:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extra Trees ▶️ Train Acc: 0.995493125701459 Test Acc: 0.7882996632996633\n",
      "Top 10 features:\n",
      "  latitude                       0.1439\n",
      "  longitude                      0.1436\n",
      "  gps_height                     0.0687\n",
      "  construction_year              0.0527\n",
      "  quantity_enough                0.0498\n",
      "  population                     0.0460\n",
      "  waterpoint_type_other          0.0324\n",
      "  quantity_insufficient          0.0283\n",
      "  extraction_type_other          0.0280\n",
      "  payment_pay per bucket         0.0140\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: Extra Trees\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "et_pipeline = Pipeline([\n",
    "    ('preprocessing', preprocessor),\n",
    "    ('model', ExtraTreesClassifier(n_estimators=100, random_state=42))\n",
    "])\n",
    "\n",
    "et_pipeline.fit(X_train, y_train)\n",
    "y_train_pred = et_pipeline.predict(X_train)\n",
    "y_test_pred  = et_pipeline.predict(X_test)\n",
    "\n",
    "print(\"Extra Trees ▶️\",\n",
    "      \"Train Acc:\", accuracy_score(y_train, y_train_pred),\n",
    "      \"Test Acc:\",  accuracy_score(y_test, y_test_pred))\n",
    "\n",
    "feat_names = et_pipeline.named_steps['preprocessing'].get_feature_names_out()\n",
    "importances = et_pipeline.named_steps['model'].feature_importances_\n",
    "top = sorted(zip(feat_names, importances), key=lambda x: x[1], reverse=True)[:10]\n",
    "print(\"Top 10 features:\")\n",
    "for f, imp in top:\n",
    "    print(f\"  {f:<30} {imp:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008996 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1523\n",
      "[LightGBM] [Info] Number of data points in the train set: 57024, number of used features: 237\n",
      "[LightGBM] [Info] Start training from score -0.956483\n",
      "[LightGBM] [Info] Start training from score -2.621811\n",
      "[LightGBM] [Info] Start training from score -0.610486\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/amey/.local/lib/python3.10/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LightGBM ▶️ Train Acc: 0.7993476430976431 Test Acc: 0.7891414141414141\n",
      "Top 10 features:\n",
      "  latitude                       915.0000\n",
      "  longitude                      913.0000\n",
      "  construction_year              632.0000\n",
      "  gps_height                     591.0000\n",
      "  population                     356.0000\n",
      "  district_code                  203.0000\n",
      "  quantity_enough                202.0000\n",
      "  quantity_insufficient          143.0000\n",
      "  payment_pay per bucket         125.0000\n",
      "  quantity_seasonal              109.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/amey/.local/lib/python3.10/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Cell 8: LightGBM\n",
    "import lightgbm as lgb\n",
    "\n",
    "lgb_pipeline = Pipeline([\n",
    "    ('preprocessing', preprocessor),\n",
    "    ('model', lgb.LGBMClassifier(\n",
    "        objective='multiclass',\n",
    "        num_class=len(le.classes_),\n",
    "        random_state=42\n",
    "    ))\n",
    "])\n",
    "\n",
    "lgb_pipeline.fit(X_train, y_train)\n",
    "y_train_pred = lgb_pipeline.predict(X_train)\n",
    "y_test_pred  = lgb_pipeline.predict(X_test)\n",
    "\n",
    "print(\"LightGBM ▶️\",\n",
    "      \"Train Acc:\", accuracy_score(y_train, y_train_pred),\n",
    "      \"Test Acc:\",  accuracy_score(y_test, y_test_pred))\n",
    "\n",
    "feat_names = lgb_pipeline.named_steps['preprocessing'].get_feature_names_out()\n",
    "importances = lgb_pipeline.named_steps['model'].feature_importances_\n",
    "top = sorted(zip(feat_names, importances), key=lambda x: x[1], reverse=True)[:10]\n",
    "print(\"Top 10 features:\")\n",
    "for f, imp in top:\n",
    "    print(f\"  {f:<30} {imp:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/amey/.local/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Cell 9: Polynomial + LogisticRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "poly_pipeline = Pipeline([\n",
    "    ('preprocessing', preprocessor),\n",
    "    ('poly', PolynomialFeatures(degree=2)),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('model', LogisticRegression(\n",
    "        multi_class='multinomial', solver='saga', max_iter=500\n",
    "    ))\n",
    "])\n",
    "\n",
    "poly_pipeline.fit(X_train, y_train)\n",
    "y_train_pred = poly_pipeline.predict(X_train)\n",
    "y_test_pred  = poly_pipeline.predict(X_test)\n",
    "\n",
    "print(\"Polynomial LR ▶️\",\n",
    "      \"Train Acc:\", accuracy_score(y_train, y_train_pred),\n",
    "      \"Test Acc:\",  accuracy_score(y_test, y_test_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/amey/.local/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "/home/amey/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge LR ▶️ Train Acc: 0.7493160774410774 Test Acc: 0.7470538720538721\n"
     ]
    }
   ],
   "source": [
    "# Cell 10: Ridge (L2 penalty)\n",
    "ridge_pipeline = Pipeline([\n",
    "    ('preprocessing', preprocessor),\n",
    "    ('model', LogisticRegression(\n",
    "        penalty='l2', C=1.0,\n",
    "        multi_class='multinomial', solver='saga'\n",
    "    ))\n",
    "])\n",
    "\n",
    "ridge_pipeline.fit(X_train, y_train)\n",
    "y_train_pred = ridge_pipeline.predict(X_train)\n",
    "y_test_pred  = ridge_pipeline.predict(X_test)\n",
    "\n",
    "print(\"Ridge LR ▶️\",\n",
    "      \"Train Acc:\", accuracy_score(y_train, y_train_pred),\n",
    "      \"Test Acc:\",  accuracy_score(y_test, y_test_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/amey/.local/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "/home/amey/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso LR ▶️ Train Acc: 0.749175785634119 Test Acc: 0.7483164983164983\n"
     ]
    }
   ],
   "source": [
    "# Cell 11: Lasso (L1 penalty)\n",
    "lasso_pipeline = Pipeline([\n",
    "    ('preprocessing', preprocessor),\n",
    "    ('model', LogisticRegression(\n",
    "        penalty='l1', C=1.0,\n",
    "        multi_class='multinomial', solver='saga'\n",
    "    ))\n",
    "])\n",
    "\n",
    "lasso_pipeline.fit(X_train, y_train)\n",
    "y_train_pred = lasso_pipeline.predict(X_train)\n",
    "y_test_pred  = lasso_pipeline.predict(X_test)\n",
    "\n",
    "print(\"Lasso LR ▶️\",\n",
    "      \"Train Acc:\", accuracy_score(y_train, y_train_pred),\n",
    "      \"Test Acc:\",  accuracy_score(y_test, y_test_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/amey/.local/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "/home/amey/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elastic Net LR ▶️ Train Acc: 0.7493336139169473 Test Acc: 0.7470538720538721\n"
     ]
    }
   ],
   "source": [
    "# Cell 12: Elastic Net\n",
    "enet_pipeline = Pipeline([\n",
    "    ('preprocessing', preprocessor),\n",
    "    ('model', LogisticRegression(\n",
    "        penalty='elasticnet', l1_ratio=0.5, C=1.0,\n",
    "        multi_class='multinomial', solver='saga'\n",
    "    ))\n",
    "])\n",
    "\n",
    "enet_pipeline.fit(X_train, y_train)\n",
    "y_train_pred = enet_pipeline.predict(X_train)\n",
    "y_test_pred  = enet_pipeline.predict(X_test)\n",
    "\n",
    "print(\"Elastic Net LR ▶️\",\n",
    "      \"Train Acc:\", accuracy_score(y_train, y_train_pred),\n",
    "      \"Test Acc:\",  accuracy_score(y_test, y_test_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
