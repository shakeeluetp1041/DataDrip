{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import sklearn\n",
    "from sklearn.preprocessing import  MinMaxScaler,OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "import lightgbm as lgb\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "scaler_minmax= MinMaxScaler()                                                             # Create a MinMaxScaler object\n",
    "ohe = OneHotEncoder(sparse_output=False, handle_unknown='ignore',drop='first')            # Create a OneHotEncoder object\n",
    "\n",
    "\n",
    "# Read CSV files\n",
    "\n",
    "df=pd.read_csv(\"../data/Training_Set_Values.csv\")       # Read the Training data CSV file\n",
    "name_featrures=df.columns                       # Get the features name\n",
    "len_features=len(name_featrures)                # Get the length of features\n",
    "labels=pd.read_csv(\"../data/Training_Set_labels.csv\")   # Read the labels (target) CSV file\n",
    "labels.head()\n",
    "df['target'] = labels['status_group']           # Add the target column to the dataframe\n",
    "#print(df.shape)                                 # Print the shape of the dataframe\n",
    "#df.head()\n",
    "#df.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape after dropping columns: (59400, 22)\n"
     ]
    }
   ],
   "source": [
    "# Columns to be dropped for the baseline models\n",
    "columns_drop=['id','amount_tsh','num_private','subvillage','recorded_by','scheme_name',\n",
    "              'extraction_type_group','extraction_type_class',\n",
    "              'management','payment_type','quality_group','quantity_group','source','waterpoint_type_group',\n",
    "              'funder','installer','wpt_name','ward','scheme_management']\n",
    "\n",
    "df = df.drop(columns=columns_drop)\n",
    "print('Shape after dropping columns:', df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the target column to labels \n",
    "#print(df['target'].unique())\n",
    "target_map_dict={'functional': 2, 'functional needs repair': 1, 'non functional': 0} # Defined the mapping of labels to numbers (integers)\n",
    "#print(df['target'].head())\n",
    "df['target'] =df['target'].map(target_map_dict) # transform the target column (labels) to  numbers (integers)\n",
    "#df['target'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and target and perform train test split\n",
    "X = df.drop(columns=['target'])  # Features only\n",
    "y = df['target']                 # Target column\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=y)  # 2376 records for the test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "y_enc = le.fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the custom transformers form helper_function.py\n",
    "# The helper_function.py file contains the definitions for StringConverter, YearExtractor, IQRCapper, and ConstructionYearTransformer\n",
    "from helper_function import (\n",
    "    StringConverter,\n",
    "    YearExtractor,\n",
    "    IQRCapper,\n",
    "    ConstructionYearTransformer,\n",
    "    ObjectToNumericConverter\n",
    ")\n",
    "    \n",
    "#pipeline transformers\n",
    "date_recorded_transformer_pipeline=Pipeline([\n",
    "    \n",
    "    ('year_extractor',YearExtractor()),\n",
    "    ('onehot', OneHotEncoder(sparse_output=False, handle_unknown='ignore',drop='first'))\n",
    "])\n",
    "\n",
    "\n",
    "oulier_minmax_pipeline_clip = Pipeline(steps=[\n",
    "    ('iqr_cap', IQRCapper(strategy='clip')),\n",
    "    ('scaler', MinMaxScaler())\n",
    "])\n",
    "\n",
    "oulier_minmax_pipeline_mean = Pipeline(steps=[\n",
    "    ('iqr_cap', IQRCapper(strategy='mean')),\n",
    "    ('scaler', MinMaxScaler())\n",
    "])\n",
    "\n",
    "oulier_minmax_pipeline_median = Pipeline(steps=[\n",
    "    ('iqr_cap', IQRCapper(strategy='median')),\n",
    "    ('scaler', MinMaxScaler())\n",
    "])\n",
    "\n",
    "\n",
    "cat_pipeline = Pipeline([\n",
    "     ('string_converter', StringConverter()),\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='Unknown')),\n",
    "    ('ohe', OneHotEncoder(sparse_output=False, handle_unknown='ignore',drop='first'))\n",
    "])\n",
    "\n",
    "constructionyear_pipeline = Pipeline(steps=[\n",
    "    ('replace_zeros_with_median', ConstructionYearTransformer()),\n",
    "    ('minmax_scaling', MinMaxScaler())\n",
    "])\n",
    "# ColumnTransformer and full pipeline setup for feature preprocessing\n",
    "# The ColumnTransformer allows us to apply different preprocessing steps to different columns of the DataFrame\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('date', date_recorded_transformer_pipeline, ['date_recorded']),\n",
    "        #('gps_height', scaler_minmax, ['gps_height']),\n",
    "        ('outlier_minmax_gps_height', oulier_minmax_pipeline_mean, ['gps_height']),\n",
    "        ('outlier_minmax_longitude', oulier_minmax_pipeline_mean, ['longitude']),\n",
    "        ('outlier_minmax_latitude', oulier_minmax_pipeline_mean, ['latitude']),\n",
    "         ('cat_ohe', cat_pipeline, ['basin','region','region_code','lga','public_meeting','permit','extraction_type','management_group','payment','water_quality','quantity','source_type','source_class','waterpoint_type']),\n",
    "        ('outlier_minmax_population', oulier_minmax_pipeline_clip, ['population']),\n",
    "        ('constructionyear', constructionyear_pipeline, ['construction_year'])\n",
    "\n",
    "\n",
    "    ],\n",
    "    remainder='passthrough',\n",
    "    verbose_feature_names_out=False\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "preprocess_pipeline = Pipeline([\n",
    "    ('preprocessing', preprocessor),\n",
    "    ('object_to_numeric', ObjectToNumericConverter())  # your custom step\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'columns'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 30\u001b[0m\n\u001b[1;32m     22\u001b[0m imb_pipeline \u001b[38;5;241m=\u001b[39m ImbPipeline([\n\u001b[1;32m     23\u001b[0m     (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpreprocessor\u001b[39m\u001b[38;5;124m'\u001b[39m,         step_preprocessor),\n\u001b[1;32m     24\u001b[0m     (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mobject_to_numeric\u001b[39m\u001b[38;5;124m'\u001b[39m,    step_obj2num),\n\u001b[1;32m     25\u001b[0m     (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msmote\u001b[39m\u001b[38;5;124m'\u001b[39m,                step_smote),\n\u001b[1;32m     26\u001b[0m     (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclassifier\u001b[39m\u001b[38;5;124m'\u001b[39m,           step_clf),\n\u001b[1;32m     27\u001b[0m ])\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# Fit it just like any other pipeline\u001b[39;00m\n\u001b[0;32m---> 30\u001b[0m \u001b[43mimb_pipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m imb_pipeline\u001b[38;5;241m.\u001b[39mpredict(X_test)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1387\u001b[0m     )\n\u001b[1;32m   1388\u001b[0m ):\n\u001b[0;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/imblearn/pipeline.py:518\u001b[0m, in \u001b[0;36mPipeline.fit\u001b[0;34m(self, X, y, **params)\u001b[0m\n\u001b[1;32m    511\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    512\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe `transform_input` parameter is not supported in scikit-learn \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    513\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mversions prior to 1.4. Please upgrade to scikit-learn 1.4 or \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    514\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlater.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    515\u001b[0m     )\n\u001b[1;32m    517\u001b[0m routed_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_method_params(method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit\u001b[39m\u001b[38;5;124m\"\u001b[39m, props\u001b[38;5;241m=\u001b[39mparams)\n\u001b[0;32m--> 518\u001b[0m Xt, yt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrouted_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mraw_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    519\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _print_elapsed_time(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPipeline\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_log_message(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)):\n\u001b[1;32m    520\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_final_estimator \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassthrough\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/imblearn/pipeline.py:430\u001b[0m, in \u001b[0;36mPipeline._fit\u001b[0;34m(self, X, y, routed_params, raw_params)\u001b[0m\n\u001b[1;32m    422\u001b[0m step_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_metadata_for_step(\n\u001b[1;32m    423\u001b[0m     step_idx\u001b[38;5;241m=\u001b[39mstep_idx,\n\u001b[1;32m    424\u001b[0m     step_params\u001b[38;5;241m=\u001b[39mrouted_params[name],\n\u001b[1;32m    425\u001b[0m     all_params\u001b[38;5;241m=\u001b[39mraw_params,\n\u001b[1;32m    426\u001b[0m )\n\u001b[1;32m    427\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(cloned_transformer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransform\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\n\u001b[1;32m    428\u001b[0m     cloned_transformer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit_transform\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    429\u001b[0m ):\n\u001b[0;32m--> 430\u001b[0m     X, fitted_transformer \u001b[38;5;241m=\u001b[39m \u001b[43mfit_transform_one_cached\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    431\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcloned_transformer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    432\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    433\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    434\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    435\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessage_clsname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPipeline\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    436\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_log_message\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep_idx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    437\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstep_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    438\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    439\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(cloned_transformer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit_resample\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    440\u001b[0m     X, y, fitted_transformer \u001b[38;5;241m=\u001b[39m fit_resample_one_cached(\n\u001b[1;32m    441\u001b[0m         cloned_transformer,\n\u001b[1;32m    442\u001b[0m         X,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    446\u001b[0m         params\u001b[38;5;241m=\u001b[39mrouted_params[name],\n\u001b[1;32m    447\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/joblib/memory.py:312\u001b[0m, in \u001b[0;36mNotMemorizedFunc.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    311\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 312\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/imblearn/pipeline.py:1383\u001b[0m, in \u001b[0;36m_fit_transform_one\u001b[0;34m(transformer, X, y, weight, message_clsname, message, params)\u001b[0m\n\u001b[1;32m   1381\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _print_elapsed_time(message_clsname, message):\n\u001b[1;32m   1382\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(transformer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit_transform\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m-> 1383\u001b[0m         res \u001b[38;5;241m=\u001b[39m \u001b[43mtransformer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfit_transform\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1384\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1385\u001b[0m         res \u001b[38;5;241m=\u001b[39m transformer\u001b[38;5;241m.\u001b[39mfit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit\u001b[39m\u001b[38;5;124m\"\u001b[39m, {}))\u001b[38;5;241m.\u001b[39mtransform(\n\u001b[1;32m   1386\u001b[0m             X, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransform\u001b[39m\u001b[38;5;124m\"\u001b[39m, {})\n\u001b[1;32m   1387\u001b[0m         )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/utils/_set_output.py:319\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 319\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    320\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    321\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    322\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    323\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    324\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[1;32m    325\u001b[0m         )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/base.py:921\u001b[0m, in \u001b[0;36mTransformerMixin.fit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    918\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit(X, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n\u001b[1;32m    919\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    920\u001b[0m     \u001b[38;5;66;03m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[0;32m--> 921\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtransform(X)\n",
      "File \u001b[0;32m~/DataDrip/notebooks/helper_function.py:92\u001b[0m, in \u001b[0;36mObjectToNumericConverter.fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m---> 92\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_names \u001b[38;5;241m=\u001b[39m \u001b[43mX\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'columns'"
     ]
    }
   ],
   "source": [
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# 1) Use your ColumnTransformer directly\n",
    "#    (instead of preprocess_pipeline)\n",
    "step_preprocessor = preprocessor\n",
    "\n",
    "# 2) Then your ObjectToNumericConverter\n",
    "step_obj2num = ObjectToNumericConverter()\n",
    "\n",
    "# 3) Then SMOTE\n",
    "step_smote = SMOTE(sampling_strategy=0.5, random_state=42)\n",
    "\n",
    "# 4) Then your classifier\n",
    "step_clf = RandomForestClassifier(\n",
    "    n_estimators=200,\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "imb_pipeline = ImbPipeline([\n",
    "    ('preprocessor',         step_preprocessor),\n",
    "    ('object_to_numeric',    step_obj2num),\n",
    "    ('smote',                step_smote),\n",
    "    ('classifier',           step_clf),\n",
    "])\n",
    "\n",
    "# Fit it just like any other pipeline\n",
    "imb_pipeline.fit(X_train, y_train)\n",
    "y_pred = imb_pipeline.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# models (Decision Tree, Random Forest, XGBoost) to be used\n",
    "models = {    \n",
    "    \"Decision Tree\": DecisionTreeClassifier(\n",
    "        max_depth=10,  # You can tune this\n",
    "        random_state=42\n",
    "    ),\n",
    "    \"Random Forest\": RandomForestClassifier(\n",
    "        n_estimators=100,\n",
    "        max_depth=15,  # You can tune this too\n",
    "        random_state=42\n",
    "    ),\"XGBoost\": XGBClassifier(\n",
    "        n_estimators=100,\n",
    "        max_depth=11,\n",
    "        learning_rate=0.1,\n",
    "        subsample=0.9,\n",
    "        colsample_bytree=0.8,\n",
    "        #use_label_encoder=False,\n",
    "        eval_metric='mlogloss',    # good for multi-class\n",
    "        objective='multi:softmax', # directly outputs class labels\n",
    "        num_class=3,               # number of target classes\n",
    "        random_state=42\n",
    "    ),\n",
    "    \"SVC\": SVC(\n",
    "    kernel='rbf',\n",
    "    C=1.0,\n",
    "    class_weight='balanced',\n",
    "    probability=True  # if you need .predict_proba\n",
    "    ),\n",
    "    \"Gradient Boosting\": GradientBoostingClassifier(\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.1,\n",
    "    random_state=42\n",
    "    ),\n",
    "    \"Extra Trees\": ExtraTreesClassifier(\n",
    "    n_estimators=100,\n",
    "    random_state=42\n",
    "    ),\n",
    "    \"LightGBM\": lgb.LGBMClassifier(\n",
    "    objective='multiclass',\n",
    "    num_class=len(le.classes_),\n",
    "    random_state=42\n",
    "    ),\n",
    "    \"Polynomial\": Pipeline([\n",
    "    ('poly', PolynomialFeatures(degree=2)),\n",
    "    ('scaler', StandardScaler()),        # often useful after poly\n",
    "    ('clf', LogisticRegression(\n",
    "        multi_class='multinomial',\n",
    "        solver='saga',\n",
    "        max_iter=500\n",
    "    ))\n",
    "    ]),\n",
    "    \"Ridge\": LogisticRegression(\n",
    "    penalty='l2',\n",
    "    C=1.0,\n",
    "    multi_class='multinomial',\n",
    "    solver='saga'\n",
    "    ),\n",
    "    \"Lasso\": LogisticRegression(\n",
    "    penalty='l1',\n",
    "    C=1.0,\n",
    "    multi_class='multinomial',\n",
    "    solver='saga'\n",
    "    ),\n",
    "    \"ElasticNet\": LogisticRegression(\n",
    "    penalty='elasticnet',\n",
    "    l1_ratio=0.5,\n",
    "    C=1.0,\n",
    "    multi_class='multinomial',\n",
    "    solver='saga'\n",
    "    )\n",
    "}\n",
    "\n",
    "# Results and feature importances storage\n",
    "results = {}\n",
    "feature_importances = {}\n",
    "\n",
    "# Loop through each model\n",
    "for name, model in models.items():\n",
    "    full_pipeline = Pipeline([\n",
    "        ('preprocessing', preprocessor),\n",
    "        ('model', model)\n",
    "                            ])\n",
    "\n",
    "\n",
    "    full_pipeline.fit(X_train, y_train) \n",
    "    \"\"\"\n",
    "    # Step 1: Each transformer in the preprocessing pipeline Computes and stores necessary statistics \n",
    "    # (e.g., quartiles, medians, scalers) from X_train only.\n",
    "    #Step 2: The transformers are applied (transformed) to X_train to produce the final preprocessed training features.\n",
    "    # Step 3: The model is trained using these transformed features and y_train.\n",
    "    \"\"\" \n",
    "\n",
    "\n",
    "    # Predictions\n",
    "    y_train_pred = full_pipeline.predict(X_train)\n",
    "    y_test_pred = full_pipeline.predict(X_test)\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    The stored training statistics are used to transform X_train again and X_test (no re-fitting!).\n",
    "\n",
    "    Feeds this transformed data to the already-trained model.\n",
    "\n",
    "    Outputs predictions (y_train_pred, y_test_pred).\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Accuracy scores\n",
    "    train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "    test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "    results[name] = {\n",
    "        \"Train Accuracy\": train_accuracy,\n",
    "        \"Test Accuracy\": test_accuracy\n",
    "    }\n",
    "    # Extract feature importances\n",
    "    fitted_model = full_pipeline.named_steps['model']\n",
    "    \n",
    "    if hasattr(fitted_model, 'feature_importances_'):\n",
    "        # Get transformed feature names from preprocessor\n",
    "        feature_names = full_pipeline.named_steps['preprocessing'].get_feature_names_out()\n",
    "        importances = fitted_model.feature_importances_\n",
    "        feature_importances[name] = sorted(\n",
    "            zip(feature_names, importances),\n",
    "            key=lambda x: x[1],\n",
    "            reverse=True\n",
    "        )\n",
    "\n",
    "# Print results\n",
    "print(\"\\nModel Comparison:\")\n",
    "print(\"{:<15} {:<15} {:<15}\".format(\"Model\", \"Train Acc\", \"Test Acc\"))\n",
    "print(\"-\" * 45)\n",
    "for model_name, scores in results.items():\n",
    "    print(\"{:<15} {:<15.4f} {:<15.4f}\".format(model_name, scores[\"Train Accuracy\"], scores[\"Test Accuracy\"]))\n",
    "\n",
    "# Print top features\n",
    "print(\"\\nTop 10 Important Features:\")\n",
    "for model_name, importance_list in feature_importances.items():\n",
    "    print(f\"\\n{model_name}:\")\n",
    "    for feature, importance in importance_list[:10]:\n",
    "        print(f\"{feature:<30} {importance:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree ▶️ Train Acc: 0.7664502164502165 Test Acc: 0.7408529741863075\n",
      "Top 10 features:\n",
      "  waterpoint_type_other          0.2031\n",
      "  quantity_seasonal              0.1311\n",
      "  quantity_enough                0.1134\n",
      "  quantity_insufficient          0.0873\n",
      "  longitude                      0.0634\n",
      "  construction_year              0.0522\n",
      "  waterpoint_type_communal standpipe multiple 0.0391\n",
      "  latitude                       0.0383\n",
      "  population                     0.0196\n",
      "  region_Iringa                  0.0161\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Decision Tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "dt_pipeline = Pipeline([\n",
    "    ('preprocessing', preprocessor),\n",
    "    ('model', DecisionTreeClassifier(max_depth=10, random_state=42))\n",
    "])\n",
    "\n",
    "dt_pipeline.fit(X_train, y_train)\n",
    "y_train_pred = dt_pipeline.predict(X_train)\n",
    "y_test_pred  = dt_pipeline.predict(X_test)\n",
    "\n",
    "print(\"Decision Tree ▶️\",\n",
    "      \"Train Acc:\", accuracy_score(y_train, y_train_pred),\n",
    "      \"Test Acc:\",  accuracy_score(y_test, y_test_pred))\n",
    "\n",
    "# Feature importances\n",
    "feat_names = dt_pipeline.named_steps['preprocessing'].get_feature_names_out()\n",
    "importances = dt_pipeline.named_steps['model'].feature_importances_\n",
    "top = sorted(zip(feat_names, importances), key=lambda x: x[1], reverse=True)[:10]\n",
    "print(\"Top 10 features:\")\n",
    "for f, imp in top:\n",
    "    print(f\"  {f:<30} {imp:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest ▶️ Train Acc: 0.8136574074074074 Test Acc: 0.7777777777777778\n",
      "Top 10 features:\n",
      "  quantity_enough                0.0774\n",
      "  waterpoint_type_other          0.0748\n",
      "  longitude                      0.0678\n",
      "  latitude                       0.0634\n",
      "  construction_year              0.0610\n",
      "  extraction_type_other          0.0586\n",
      "  gps_height                     0.0421\n",
      "  population                     0.0306\n",
      "  quantity_insufficient          0.0248\n",
      "  waterpoint_type_communal standpipe 0.0199\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Random Forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf_pipeline = Pipeline([\n",
    "    ('preprocessing', preprocessor),\n",
    "    ('model', RandomForestClassifier(\n",
    "        n_estimators=100, max_depth=15, random_state=42\n",
    "    ))\n",
    "])\n",
    "\n",
    "rf_pipeline.fit(X_train, y_train)\n",
    "y_train_pred = rf_pipeline.predict(X_train)\n",
    "y_test_pred  = rf_pipeline.predict(X_test)\n",
    "\n",
    "print(\"Random Forest ▶️\",\n",
    "      \"Train Acc:\", accuracy_score(y_train, y_train_pred),\n",
    "      \"Test Acc:\",  accuracy_score(y_test, y_test_pred))\n",
    "\n",
    "feat_names = rf_pipeline.named_steps['preprocessing'].get_feature_names_out()\n",
    "importances = rf_pipeline.named_steps['model'].feature_importances_\n",
    "top = sorted(zip(feat_names, importances), key=lambda x: x[1], reverse=True)[:10]\n",
    "print(\"Top 10 features:\")\n",
    "for f, imp in top:\n",
    "    print(f\"  {f:<30} {imp:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost ▶️ Train Acc: 0.8602518237934904 Test Acc: 0.8055555555555556\n",
      "Top 10 features:\n",
      "  waterpoint_type_other          0.0654\n",
      "  lga_Bariadi                    0.0275\n",
      "  quantity_seasonal              0.0269\n",
      "  region_Iringa                  0.0254\n",
      "  extraction_type_other          0.0164\n",
      "  region_code_11                 0.0155\n",
      "  region_code_15                 0.0151\n",
      "  lga_Ngara                      0.0147\n",
      "  lga_Rombo                      0.0145\n",
      "  lga_Chunya                     0.0122\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: XGBoost\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "xgb_pipeline = Pipeline([\n",
    "    ('preprocessing', preprocessor),\n",
    "    ('model', XGBClassifier(\n",
    "        n_estimators=100,\n",
    "        max_depth=11,\n",
    "        learning_rate=0.1,\n",
    "        subsample=0.9,\n",
    "        colsample_bytree=0.8,\n",
    "        objective='multi:softmax',\n",
    "        num_class=len(le.classes_),\n",
    "        eval_metric='mlogloss',\n",
    "        random_state=42\n",
    "    ))\n",
    "])\n",
    "\n",
    "xgb_pipeline.fit(X_train, y_train)\n",
    "y_train_pred = xgb_pipeline.predict(X_train)\n",
    "y_test_pred  = xgb_pipeline.predict(X_test)\n",
    "\n",
    "print(\"XGBoost ▶️\",\n",
    "      \"Train Acc:\", accuracy_score(y_train, y_train_pred),\n",
    "      \"Test Acc:\",  accuracy_score(y_test, y_test_pred))\n",
    "\n",
    "feat_names = xgb_pipeline.named_steps['preprocessing'].get_feature_names_out()\n",
    "importances = xgb_pipeline.named_steps['model'].feature_importances_\n",
    "top = sorted(zip(feat_names, importances), key=lambda x: x[1], reverse=True)[:10]\n",
    "print(\"Top 10 features:\")\n",
    "for f, imp in top:\n",
    "    print(f\"  {f:<30} {imp:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: SVC\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "svc_pipeline = Pipeline([\n",
    "    ('preprocessing', preprocessor),\n",
    "    ('model', SVC(\n",
    "        kernel='rbf', C=1.0,\n",
    "        class_weight='balanced',\n",
    "        probability=True\n",
    "    ))\n",
    "])\n",
    "\n",
    "svc_pipeline.fit(X_train, y_train)\n",
    "y_train_pred = svc_pipeline.predict(X_train)\n",
    "y_test_pred  = svc_pipeline.predict(X_test)\n",
    "\n",
    "print(\"SVC ▶️\",\n",
    "      \"Train Acc:\", accuracy_score(y_train, y_train_pred),\n",
    "      \"Test Acc:\",  accuracy_score(y_test, y_test_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Boosting ▶️ Train Acc: 0.7565761784511784 Test Acc: 0.7554713804713805\n",
      "Top 10 features:\n",
      "  waterpoint_type_other          0.1380\n",
      "  quantity_insufficient          0.1350\n",
      "  quantity_enough                0.1288\n",
      "  extraction_type_other          0.0769\n",
      "  construction_year              0.0765\n",
      "  quantity_seasonal              0.0506\n",
      "  waterpoint_type_communal standpipe multiple 0.0393\n",
      "  longitude                      0.0320\n",
      "  latitude                       0.0232\n",
      "  payment_pay per bucket         0.0187\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Gradient Boosting\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "gb_pipeline = Pipeline([\n",
    "    ('preprocessing', preprocessor),\n",
    "    ('model', GradientBoostingClassifier(\n",
    "        n_estimators=100, learning_rate=0.1, random_state=42\n",
    "    ))\n",
    "])\n",
    "\n",
    "gb_pipeline.fit(X_train, y_train)\n",
    "y_train_pred = gb_pipeline.predict(X_train)\n",
    "y_test_pred  = gb_pipeline.predict(X_test)\n",
    "\n",
    "print(\"Gradient Boosting ▶️\",\n",
    "      \"Train Acc:\", accuracy_score(y_train, y_train_pred),\n",
    "      \"Test Acc:\",  accuracy_score(y_test, y_test_pred))\n",
    "\n",
    "feat_names = gb_pipeline.named_steps['preprocessing'].get_feature_names_out()\n",
    "importances = gb_pipeline.named_steps['model'].feature_importances_\n",
    "top = sorted(zip(feat_names, importances), key=lambda x: x[1], reverse=True)[:10]\n",
    "print(\"Top 10 features:\")\n",
    "for f, imp in top:\n",
    "    print(f\"  {f:<30} {imp:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extra Trees ▶️ Train Acc: 0.995493125701459 Test Acc: 0.7882996632996633\n",
      "Top 10 features:\n",
      "  latitude                       0.1439\n",
      "  longitude                      0.1436\n",
      "  gps_height                     0.0687\n",
      "  construction_year              0.0527\n",
      "  quantity_enough                0.0498\n",
      "  population                     0.0460\n",
      "  waterpoint_type_other          0.0324\n",
      "  quantity_insufficient          0.0283\n",
      "  extraction_type_other          0.0280\n",
      "  payment_pay per bucket         0.0140\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: Extra Trees\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "et_pipeline = Pipeline([\n",
    "    ('preprocessing', preprocessor),\n",
    "    ('model', ExtraTreesClassifier(n_estimators=100, random_state=42))\n",
    "])\n",
    "\n",
    "et_pipeline.fit(X_train, y_train)\n",
    "y_train_pred = et_pipeline.predict(X_train)\n",
    "y_test_pred  = et_pipeline.predict(X_test)\n",
    "\n",
    "print(\"Extra Trees ▶️\",\n",
    "      \"Train Acc:\", accuracy_score(y_train, y_train_pred),\n",
    "      \"Test Acc:\",  accuracy_score(y_test, y_test_pred))\n",
    "\n",
    "feat_names = et_pipeline.named_steps['preprocessing'].get_feature_names_out()\n",
    "importances = et_pipeline.named_steps['model'].feature_importances_\n",
    "top = sorted(zip(feat_names, importances), key=lambda x: x[1], reverse=True)[:10]\n",
    "print(\"Top 10 features:\")\n",
    "for f, imp in top:\n",
    "    print(f\"  {f:<30} {imp:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008996 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1523\n",
      "[LightGBM] [Info] Number of data points in the train set: 57024, number of used features: 237\n",
      "[LightGBM] [Info] Start training from score -0.956483\n",
      "[LightGBM] [Info] Start training from score -2.621811\n",
      "[LightGBM] [Info] Start training from score -0.610486\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/amey/.local/lib/python3.10/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LightGBM ▶️ Train Acc: 0.7993476430976431 Test Acc: 0.7891414141414141\n",
      "Top 10 features:\n",
      "  latitude                       915.0000\n",
      "  longitude                      913.0000\n",
      "  construction_year              632.0000\n",
      "  gps_height                     591.0000\n",
      "  population                     356.0000\n",
      "  district_code                  203.0000\n",
      "  quantity_enough                202.0000\n",
      "  quantity_insufficient          143.0000\n",
      "  payment_pay per bucket         125.0000\n",
      "  quantity_seasonal              109.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/amey/.local/lib/python3.10/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Cell 8: LightGBM\n",
    "import lightgbm as lgb\n",
    "\n",
    "lgb_pipeline = Pipeline([\n",
    "    ('preprocessing', preprocessor),\n",
    "    ('model', lgb.LGBMClassifier(\n",
    "        objective='multiclass',\n",
    "        num_class=len(le.classes_),\n",
    "        random_state=42\n",
    "    ))\n",
    "])\n",
    "\n",
    "lgb_pipeline.fit(X_train, y_train)\n",
    "y_train_pred = lgb_pipeline.predict(X_train)\n",
    "y_test_pred  = lgb_pipeline.predict(X_test)\n",
    "\n",
    "print(\"LightGBM ▶️\",\n",
    "      \"Train Acc:\", accuracy_score(y_train, y_train_pred),\n",
    "      \"Test Acc:\",  accuracy_score(y_test, y_test_pred))\n",
    "\n",
    "feat_names = lgb_pipeline.named_steps['preprocessing'].get_feature_names_out()\n",
    "importances = lgb_pipeline.named_steps['model'].feature_importances_\n",
    "top = sorted(zip(feat_names, importances), key=lambda x: x[1], reverse=True)[:10]\n",
    "print(\"Top 10 features:\")\n",
    "for f, imp in top:\n",
    "    print(f\"  {f:<30} {imp:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/amey/.local/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Cell 9: Polynomial + LogisticRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "poly_pipeline = Pipeline([\n",
    "    ('preprocessing', preprocessor),\n",
    "    ('poly', PolynomialFeatures(degree=2)),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('model', LogisticRegression(\n",
    "        multi_class='multinomial', solver='saga', max_iter=500\n",
    "    ))\n",
    "])\n",
    "\n",
    "poly_pipeline.fit(X_train, y_train)\n",
    "y_train_pred = poly_pipeline.predict(X_train)\n",
    "y_test_pred  = poly_pipeline.predict(X_test)\n",
    "\n",
    "print(\"Polynomial LR ▶️\",\n",
    "      \"Train Acc:\", accuracy_score(y_train, y_train_pred),\n",
    "      \"Test Acc:\",  accuracy_score(y_test, y_test_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/amey/.local/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "/home/amey/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge LR ▶️ Train Acc: 0.7493160774410774 Test Acc: 0.7470538720538721\n"
     ]
    }
   ],
   "source": [
    "# Cell 10: Ridge (L2 penalty)\n",
    "ridge_pipeline = Pipeline([\n",
    "    ('preprocessing', preprocessor),\n",
    "    ('model', LogisticRegression(\n",
    "        penalty='l2', C=1.0,\n",
    "        multi_class='multinomial', solver='saga'\n",
    "    ))\n",
    "])\n",
    "\n",
    "ridge_pipeline.fit(X_train, y_train)\n",
    "y_train_pred = ridge_pipeline.predict(X_train)\n",
    "y_test_pred  = ridge_pipeline.predict(X_test)\n",
    "\n",
    "print(\"Ridge LR ▶️\",\n",
    "      \"Train Acc:\", accuracy_score(y_train, y_train_pred),\n",
    "      \"Test Acc:\",  accuracy_score(y_test, y_test_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/amey/.local/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "/home/amey/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso LR ▶️ Train Acc: 0.749175785634119 Test Acc: 0.7483164983164983\n"
     ]
    }
   ],
   "source": [
    "# Cell 11: Lasso (L1 penalty)\n",
    "lasso_pipeline = Pipeline([\n",
    "    ('preprocessing', preprocessor),\n",
    "    ('model', LogisticRegression(\n",
    "        penalty='l1', C=1.0,\n",
    "        multi_class='multinomial', solver='saga'\n",
    "    ))\n",
    "])\n",
    "\n",
    "lasso_pipeline.fit(X_train, y_train)\n",
    "y_train_pred = lasso_pipeline.predict(X_train)\n",
    "y_test_pred  = lasso_pipeline.predict(X_test)\n",
    "\n",
    "print(\"Lasso LR ▶️\",\n",
    "      \"Train Acc:\", accuracy_score(y_train, y_train_pred),\n",
    "      \"Test Acc:\",  accuracy_score(y_test, y_test_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/amey/.local/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "/home/amey/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elastic Net LR ▶️ Train Acc: 0.7493336139169473 Test Acc: 0.7470538720538721\n"
     ]
    }
   ],
   "source": [
    "# Cell 12: Elastic Net\n",
    "enet_pipeline = Pipeline([\n",
    "    ('preprocessing', preprocessor),\n",
    "    ('model', LogisticRegression(\n",
    "        penalty='elasticnet', l1_ratio=0.5, C=1.0,\n",
    "        multi_class='multinomial', solver='saga'\n",
    "    ))\n",
    "])\n",
    "\n",
    "enet_pipeline.fit(X_train, y_train)\n",
    "y_train_pred = enet_pipeline.predict(X_train)\n",
    "y_test_pred  = enet_pipeline.predict(X_test)\n",
    "\n",
    "print(\"Elastic Net LR ▶️\",\n",
    "      \"Train Acc:\", accuracy_score(y_train, y_train_pred),\n",
    "      \"Test Acc:\",  accuracy_score(y_test, y_test_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
