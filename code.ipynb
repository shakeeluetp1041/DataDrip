{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler,OrdinalEncoder,OneHotEncoder\n",
    "\n",
    "\n",
    "scaler_minmax= MinMaxScaler()                                           # Create a MinMaxScaler object\n",
    "scaler_standered=StandardScaler()                                       # Create a StandardScaler object\n",
    "oe=OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1) # Create a OrdenalEncoder object\n",
    "ohe = OneHotEncoder(sparse_output=False, handle_unknown='ignore')       # Create a OneHotEncoder object\n",
    "\n",
    "# Read CSV files\n",
    "\n",
    "df=pd.read_csv(\"Training_Set_Values.csv\")    # Read the CSV file\n",
    "name_featrures=df.columns                    # Get the features name\n",
    "len_features=len(name_featrures)                # Get the length of features\n",
    "labels=pd.read_csv(\"Training_Set_Labels.csv\") # Read the labels CSV file\n",
    "labels.head()\n",
    "df['target'] = labels['status_group']        # Add the target column to the dataframe\n",
    "print(df.shape)                              # Print the shape of the dataframe\n",
    "#df.head()\n",
    "#df.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns with Nan values\n",
    "nan_columns = df.columns[df.isnull().any()].tolist()           # Get the columns with NaN values\n",
    "print(\"Columns with NaN values: \", nan_columns)                # Print the columns names with NaN values\n",
    "print(\"Number of columns with NaN values: \", len(nan_columns)) # Print the number of columns with NaN values\n",
    "#df[nan_columns].head()                                         # Print the first 5 rows of the columns with NaN values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploration of NaN Columns\n",
    "for col in nan_columns:                                             # Loop through the columns with NaN values\n",
    "    print(\"--------------------------------------------------\")     # Print a separator line\n",
    "    print(\"Name:\",col)                                              # Print the column name\n",
    "    print(\"Number of NaN:\",df[col].isnull().sum())                  # Print the number of NaN values in the column\n",
    "    print(\"Percentage of NaN:\", df[col].isnull().sum()/len(df)*100) # Print the percentage of NaN values in the column\n",
    "    print(df[col].value_counts())                                   # Print the value counts of the column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column 01 amount_tsh (Static Head)  ## 50 percent values are zero....I think Drop this column\n",
    "print('Nan count in amount_tsh',df[\"amount_tsh\"].isnull().sum()) # Number of NaN values in the column\n",
    "\n",
    "print(df[\"amount_tsh\"].describe())\n",
    "print(\"median: \", df[\"amount_tsh\"].median()) # Median is zero...means half of the points are zero (since points are positive)\n",
    "plt.figure(figsize=(8,5))\n",
    "sns.histplot(df[\"amount_tsh\"], bins=200, kde=True)\n",
    "plt.xlabel(\"Amount TSH (Static Head)\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Histogram of Static Head\")\n",
    "plt.show()\n",
    "plt.figure(figsize=(8,5))\n",
    "sns.boxplot(df[\"amount_tsh\"])\n",
    "sns.stripplot(df[\"amount_tsh\"],color=\"red\",alpha=0.5)\n",
    "plt.ylim(0, 100) \n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Column 02 date_recorded (Date when the recoed entered)\n",
    "print(\"date_recorded\")\n",
    "print(\"NaN value count:\",df[\"date_recorded\"].isnull().sum()) # Check if there are any null values in the column\n",
    "df['date_recorded'] = pd.to_datetime(df['date_recorded']).dt.year# Convert to datetime format and extract year\n",
    "print(df['date_recorded'].value_counts()) # Print the value counts of the column\n",
    "df = df[~df['date_recorded'].isin([2002, 2004])] # Remove the record for years 2002 and 2004 from the dataframe.\n",
    "                                             #31 records will be droped\n",
    "\n",
    "df['date_recorded']=oe.fit_transform(df['date_recorded'].values.reshape(-1, 1)) # Fit and transform the column using OrdinalEncoder\n",
    "print('After Transformatiom',df['date_recorded'].value_counts()) # Print the value counts of the column\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column 03 funder # Not Clear how to handle this column\n",
    "print(\"NaN value count in funder:\",df[\"funder\"].isnull().sum()) # Check if there are any null values in the column\n",
    "print('Unique values:',df['funder'].nunique()) # Print the number of unique values in the column\n",
    "print(df['funder'].describe()) # Print the value counts of the column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column 04 gps_height (GPS height) # The column needs to be considered as data is ok, minmax normalization is used\n",
    "print(\"--------------------------------\")\n",
    "print(\"gps_height\")\n",
    "print(\"NaN value count:\",df[\"gps_height\"].isnull().sum())                # Check if there are any null values in the column\n",
    "print(\"Percentage of NaN:\", df[\"gps_height\"].isnull().sum()/len(df)*100) # Print the percentage of NaN values in the column\n",
    "print(df[\"gps_height\"].describe())                                       # Print the description of the column\n",
    "print(\"Median:\", df[\"gps_height\"].median())                              # Print the median of the column\n",
    "plt.figure(figsize=(8,5))\n",
    "sns.histplot(df[\"gps_height\"], bins=200, kde=True)                       # Plot the histogram of the column\n",
    "plt.xlabel(\"GPS Height\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Histogram of GPS Height\")\n",
    "plt.show()\n",
    "plt.figure(figsize=(8,5))\n",
    "sns.boxplot(y=df[\"gps_height\"])\n",
    "sns.stripplot(y=df[\"gps_height\"], color=\"red\", alpha=0.5)                # Adds all points\n",
    "plt.title(\"Boxplot of GPS Height Before Normalization\")\n",
    "plt.show() # Show the plot\n",
    "\n",
    "\n",
    "\n",
    "df[\"gps_height\"]=scaler_minmax.fit_transform(df[\"gps_height\"].values.reshape(-1,1)) # Fit and transform the data using MinMaxScaler\n",
    "df[\"gps_height\"] = np.clip(df[\"gps_height\"], 0, 1) # To ensure that the values are between 0 and 1, in case the testdata has values outside the range of training data\n",
    "plt.figure(figsize=(8,5))\n",
    "sns.boxplot(df[\"gps_height\"])\n",
    "sns.stripplot(df[\"gps_height\"], color=\"red\", alpha=0.5)  # Adds all points\n",
    "plt.ylabel(\"GPS Height (MinMax Normalized)\")\n",
    "plt.title(\"Boxplot of GPS Height After Normalization\")\n",
    "plt.show() # Show the plot\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column 05 installer # Not Clear how to handle this column\n",
    "print(\"NaN value count in installer:\",df[\"installer\"].isnull().sum()) # Check if there are any null values in the column\n",
    "print('Unique values:',df['installer'].nunique()) # Print the number of unique values in the column\n",
    "print(df['installer'].describe()) # Print the value counts of the column\n",
    "#print(df['installer'].value_counts()) # Print the value counts of the column\n",
    "pd.crosstab(df['installer'],df['target']).head() # Cross tabulation of installer and status_group\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Columns 06 longitude (GPS Coordinates) # The column needs to be considered as data is ok,outlier replaced with medain and minmax normalization\n",
    "print(\"--------------------------------\")\n",
    "print(\"longitude\")\n",
    "print(\"NaN value count:\",df[\"longitude\"].isnull().sum())                # Check if there are any null values in the column\n",
    "print(df[\"longitude\"].describe())                                                  # Print the description of the column\n",
    "print(\"Median:\", df[\"longitude\"].median())                              # Print the median of the column\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "sns.histplot(df[\"longitude\"], bins=200, kde=True)               # Plot the histogram of the column\n",
    "plt.show()\n",
    "plt.figure(figsize=(8,5))\n",
    "sns.boxplot(df[\"longitude\"])\n",
    "sns.stripplot(df[\"longitude\"], color=\"red\", alpha=0.5)                  # Adds all points\n",
    "plt.title(\"Boxplot of Longitude Before Outlier Removal\")\n",
    "plt.show() # Show the plot\n",
    "\n",
    "# Remove the outliers from the longitude column\n",
    "Q1=df[\"longitude\"].quantile(0.25) # 25th percentile\n",
    "Q3=df[\"longitude\"].quantile(0.75) # 75th percentile\n",
    "IQR=Q3-Q1 # Interquartile range\n",
    "Lower_bound_longitude=Q1-1.5*IQR # Lower bound\n",
    "Upper_bound_longitude=Q3+1.5*IQR # Upper bound\n",
    "median_longitude=df[\"longitude\"].median() # Median value\n",
    "\n",
    "df[\"longitude_outlier_replaced_median\"]=df[\"longitude\"].apply(lambda x:x if ((x>=Lower_bound_longitude) &(x<=Upper_bound_longitude)) else median_longitude) # Replace outliers with median value\n",
    "\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "sns.boxplot(df[\"longitude\"])\n",
    "sns.stripplot(df[\"longitude_outlier_replaced_median\"], color=\"red\", alpha=0.5)                  # Adds all points\n",
    "plt.title(\"Boxplot of Longitude After Outlier Removal With Median\")\n",
    "plt.show() # Show the plot\n",
    "\n",
    "\n",
    "df[\"longitude_outlier_replaced_median_minmax_normalized\"]=scaler_minmax.fit_transform(df[\"longitude_outlier_replaced_median\"].values.reshape(-1,1)) # Fit and transform the data using MinMaxScaler\n",
    "df[\"longitude_outlier_replaced_median_minmax_normalized\"] = np.clip(df[\"longitude_outlier_replaced_median_minmax_normalized\"], 0, 1) # To ensure that the values are between 0 and 1, in case the testdata has values outside the range of training data\n",
    "\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "sns.boxplot(df[\"longitude_outlier_replaced_median_minmax_normalized\"])\n",
    "sns.stripplot(df[\"longitude_outlier_replaced_median_minmax_normalized\"], color=\"red\", alpha=0.5)  # Adds all points\n",
    "plt.ylabel(\"longitude (Outlier Replaced by Median and MinMax Normalized)\")\n",
    "plt.title(\"Boxplot of GPS longitude After Outlier Removal and Normalization\")\n",
    "plt.show() # Show the plot\n",
    "\n",
    "\n",
    "df[\"longitude\"] = df[\"longitude_outlier_replaced_median_minmax_normalized\"] # Replace original longitude with processed values\n",
    "\n",
    "df.drop(\"longitude_outlier_replaced_median\", axis=1, inplace=True) # Drop the intermediate column\n",
    "df.drop(\"longitude_outlier_replaced_median_minmax_normalized\", axis=1, inplace=True) # Drop the intermediate column\n",
    "\n",
    "\n",
    "df.columns\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Columns 07 latitude (GPS Coordinates) # The column needs to be considered as data is ok\n",
    "print(\"--------------------------------\")\n",
    "print(\"latitude\")\n",
    "print(\"NaN value count:\",df[\"latitude\"].isnull().sum())                # Check if there are any null values in the column\n",
    "print(df[\"latitude\"].describe())                                                  # Print the description of the column\n",
    "print(\"Median:\", df[\"latitude\"].median())                              # Print the median of the column\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "sns.histplot(df[\"latitude\"], bins=200, kde=True)               # Plot the histogram of the column\n",
    "plt.show()\n",
    "plt.figure(figsize=(8,5))\n",
    "sns.boxplot(df[\"latitude\"])\n",
    "sns.stripplot(df[\"latitude\"], color=\"red\", alpha=0.5)                  # Adds all points\n",
    "plt.title(\"Boxplot of latitude\")\n",
    "plt.show() # Show the plot\n",
    "\n",
    "\n",
    "df[\"latitude_minmax_normalized\"]=scaler_minmax.fit_transform(df[\"latitude\"].values.reshape(-1,1)) # Fit and transform the data using MinMaxScaler\n",
    "df[\"latitude_minmax_normalized\"] = np.clip(df[\"latitude_minmax_normalized\"], 0, 1) # To ensure that the values are between 0 and 1, in case the testdata has values outside the range of training data\n",
    "\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "sns.boxplot(df[\"latitude_minmax_normalized\"])\n",
    "sns.stripplot(df[\"latitude_minmax_normalized\"], color=\"red\", alpha=0.5)  # Adds all points\n",
    "plt.ylabel(\"Latitude MinMax Normalized)\")\n",
    "plt.title(\"Boxplot of Latitude After Normalization\")\n",
    "plt.show() # Show the plot\n",
    "\n",
    "df[\"latitude\"] = df[\"latitude_minmax_normalized\"] # Replace original longitude with processed values\n",
    "df.drop(\"latitude_minmax_normalized\", axis=1, inplace=True) # Drop the intermediate column\n",
    "df.columns\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column 08 wpt_name (Waterpoint Name)  # Not Clear how to handle this column\n",
    "print(\"NaN value count in wpt_name:\",df[\"wpt_name\"].isnull().sum()) # Check if there are any null values in the column\n",
    "df['wpt_name'].value_counts() # Print the value counts of the column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column 09 num_private # Since 75% values are zero better to drop this column\n",
    "print(\"--------------------------------\")\n",
    "print(df[\"num_private\"].describe()) # Print the description of the column\n",
    "df[\"num_private\"].isnull().sum() # Check if there are any null values in the column\n",
    "plt.figure(figsize=(8,5))\n",
    "sns.histplot(df[\"num_private\"], bins=200,kde=True) # Plot the histogram of the column\n",
    "plt.title(\"Histogram of num_private\")\n",
    "plt.xlabel(\"num_private\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.scatter(df.index, df['num_private'], color='blue', label='Values')\n",
    "plt.xlabel('Index')\n",
    "plt.ylabel('Values')\n",
    "plt.title(\"num_private vs Index\")\n",
    "plt.show()\n",
    "plt.figure(figsize=(8,5))\n",
    "sns.boxplot(df[\"num_private\"])\n",
    "#sns.stripplot(df[\"num_private\"], color=\"red\", alpha=0.5)                  # Adds all points\n",
    "plt.title(\"Boxplot of num_private\")\n",
    "# Labels and title\n",
    "plt.xlabel('Index')\n",
    "plt.ylabel('Values')\n",
    "plt.title('Values vs Index')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column 10 basin # The column needs to be considered as data is ok..I used One Hot Encoding for this\n",
    "print(\"NaN value count:\",df[\"basin\"].isnull().sum())                # Check if there are any null values in the column\n",
    "df[\"basin\"].describe() # Print the value counts of the column\n",
    "print(df.basin.value_counts()) # Print the value counts of the column\n",
    "print(df[\"basin\"].head())\n",
    "encoded_basin = ohe.fit_transform(df[['basin']])\n",
    "encoded_basin_df = pd.DataFrame(encoded_basin, columns=ohe.get_feature_names_out(['basin']))\n",
    "print(encoded_basin_df.head(5)) # Print the first 5 rows of the encoded dataframe\n",
    "df = pd.concat([df, encoded_basin_df], axis=1) # Concatenate the original dataframe with the encoded dataframe\n",
    "df.drop(columns=['basin'], inplace=True) # Drop the original column\n",
    "df.head()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1041,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN count: 402\n",
      "count        58998\n",
      "unique       19281\n",
      "top       Madukani\n",
      "freq           508\n",
      "Name: subvillage, dtype: object\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0      Mnyusi B\n",
       "1       Nyamara\n",
       "2       Majengo\n",
       "3    Mahakamani\n",
       "4    Kyanyamisa\n",
       "Name: subvillage, dtype: object"
      ]
     },
     "execution_count": 1041,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Column 11 subvillage # Since region code and district code gives same info \n",
    "                    # so ignore this because it has NaN and string to number \n",
    "                    # conversion is needed \n",
    "print(\"NaN count:\",df[\"subvillage\"].isnull().sum()) # Check if there are any null values in the column\n",
    "print(df[\"subvillage\"].describe()) # Print the value counts of the column\n",
    "df[\"subvillage\"].value_counts() # Print the value counts of the column\n",
    "df[\"subvillage\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1062,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subvillage</th>\n",
       "      <th>region</th>\n",
       "      <th>region_code</th>\n",
       "      <th>district_code</th>\n",
       "      <th>lga</th>\n",
       "      <th>ward</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>762</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1189</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2601</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3446</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8729</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8736</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10441</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13366</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14697</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15103</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16468</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18730</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22754</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23373</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26086</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27501</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28490</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30319</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32619</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32775</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33942</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39559</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42343</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47000</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48150</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48555</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50564</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52271</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55669</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57291</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58663</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      subvillage  region  region_code  district_code  lga ward\n",
       "762          NaN     NaN          NaN            NaN  NaN  NaN\n",
       "1189         NaN     NaN          NaN            NaN  NaN  NaN\n",
       "2601         NaN     NaN          NaN            NaN  NaN  NaN\n",
       "3446         NaN     NaN          NaN            NaN  NaN  NaN\n",
       "8729         NaN     NaN          NaN            NaN  NaN  NaN\n",
       "8736         NaN     NaN          NaN            NaN  NaN  NaN\n",
       "10441        NaN     NaN          NaN            NaN  NaN  NaN\n",
       "13366        NaN     NaN          NaN            NaN  NaN  NaN\n",
       "14697        NaN     NaN          NaN            NaN  NaN  NaN\n",
       "15103        NaN     NaN          NaN            NaN  NaN  NaN\n",
       "16468        NaN     NaN          NaN            NaN  NaN  NaN\n",
       "18730        NaN     NaN          NaN            NaN  NaN  NaN\n",
       "22754        NaN     NaN          NaN            NaN  NaN  NaN\n",
       "23373        NaN     NaN          NaN            NaN  NaN  NaN\n",
       "26086        NaN     NaN          NaN            NaN  NaN  NaN\n",
       "27501        NaN     NaN          NaN            NaN  NaN  NaN\n",
       "28490        NaN     NaN          NaN            NaN  NaN  NaN\n",
       "30319        NaN     NaN          NaN            NaN  NaN  NaN\n",
       "32619        NaN     NaN          NaN            NaN  NaN  NaN\n",
       "32775        NaN     NaN          NaN            NaN  NaN  NaN\n",
       "33942        NaN     NaN          NaN            NaN  NaN  NaN\n",
       "39559        NaN     NaN          NaN            NaN  NaN  NaN\n",
       "42343        NaN     NaN          NaN            NaN  NaN  NaN\n",
       "47000        NaN     NaN          NaN            NaN  NaN  NaN\n",
       "48150        NaN     NaN          NaN            NaN  NaN  NaN\n",
       "48555        NaN     NaN          NaN            NaN  NaN  NaN\n",
       "50564        NaN     NaN          NaN            NaN  NaN  NaN\n",
       "52271        NaN     NaN          NaN            NaN  NaN  NaN\n",
       "55669        NaN     NaN          NaN            NaN  NaN  NaN\n",
       "57291        NaN     NaN          NaN            NaN  NaN  NaN\n",
       "58663        NaN     NaN          NaN            NaN  NaN  NaN"
      ]
     },
     "execution_count": 1062,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[pd.isna(df[\"region\"])][['subvillage','region','region_code','district_code','lga','ward']]# Print the first 10 rows of the dataframe where region is Kigoma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN count: 31\n",
      "count      59369\n",
      "unique        21\n",
      "top       Iringa\n",
      "freq        5293\n",
      "Name: region, dtype: object\n",
      "0    0.089154\n",
      "1    0.033132\n",
      "2    0.026664\n",
      "3    0.029072\n",
      "4    0.055787\n",
      "Name: region, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Column 12 region # Since region code and district code gives same info \n",
    "                    # so ignore this because string to number \n",
    "                    # conversion is needed\n",
    "print(\"NaN count:\", df[\"region\"].isnull().sum())           # Check if there are any null values in the column\n",
    "print(df[\"region\"].describe())                             # Print the description of the column\n",
    "df[\"region\"].value_counts()                                # Print the value counts of the column\n",
    "freq_encoding = df['region'].value_counts(normalize=True)  # Frequency Encoding for region column\n",
    "df['region'] = df['region'].map(freq_encoding)             # Map the frequencies to the original column\n",
    "\n",
    "print(df['region'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1056,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN count 31\n",
      "Unique values: 27\n",
      "count    59369.000000\n",
      "mean        15.290977\n",
      "std         17.578962\n",
      "min          1.000000\n",
      "25%          5.000000\n",
      "50%         12.000000\n",
      "75%         17.000000\n",
      "max         99.000000\n",
      "Name: region_code, dtype: float64\n",
      "code vs freq region_code\n",
      "11.0    5299\n",
      "17.0    5007\n",
      "12.0    4637\n",
      "3.0     4379\n",
      "5.0     4039\n",
      "18.0    3320\n",
      "19.0    3042\n",
      "2.0     3024\n",
      "16.0    2816\n",
      "10.0    2640\n",
      "4.0     2509\n",
      "1.0     2201\n",
      "13.0    2093\n",
      "14.0    1979\n",
      "20.0    1967\n",
      "15.0    1807\n",
      "6.0     1608\n",
      "21.0    1583\n",
      "80.0    1238\n",
      "60.0    1023\n",
      "90.0     913\n",
      "7.0      805\n",
      "99.0     423\n",
      "9.0      390\n",
      "24.0     326\n",
      "8.0      300\n",
      "40.0       1\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    0.089255\n",
       "1    0.033132\n",
       "2    0.026664\n",
       "3    0.015378\n",
       "4    0.055921\n",
       "Name: region_code, dtype: float64"
      ]
     },
     "execution_count": 1056,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Column region_code # Since it has 27 unique codes i chose to use normalized frequncy encoding.\n",
    "                    # Since frequncy of all codes are not balanced therefore frequency encoding\n",
    "                    # may result in a biased model..I think solution is to use the distric_code column as well\n",
    "print(\"NaN count\",df[\"region_code\"].isnull().sum()) # Check if there are any null values in the column\n",
    "print(\"Unique values:\",df[\"region_code\"].nunique()) # Print the distinct values of the column\n",
    "print(df[\"region_code\"].describe()) # Print the value counts of the column\n",
    "print(\"code vs freq\",df[\"region_code\"].value_counts()) # Print the value counts of the column\n",
    "df[\"region_code\"].value_counts() # Print the first 5 rows of the column\n",
    "freq_encoding=df[\"region_code\"].value_counts(normalize=True) # Frequency Encoding for region_code column\n",
    "df[\"region_code\"]=df[\"region_code\"].map(freq_encoding) # Map the frequencies to the original column\n",
    "df['region_code'].head() # Print the first 5 rows of the column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1057,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN count 31\n",
      "Unique values: 20\n",
      "count    59369.000000\n",
      "mean         5.627196\n",
      "std          9.631542\n",
      "min          0.000000\n",
      "25%          2.000000\n",
      "50%          3.000000\n",
      "75%          5.000000\n",
      "max         80.000000\n",
      "Name: district_code, dtype: float64\n",
      "code vs freq district_code\n",
      "1.0     12199\n",
      "2.0     11169\n",
      "3.0      9995\n",
      "4.0      8994\n",
      "5.0      4355\n",
      "6.0      4074\n",
      "7.0      3339\n",
      "8.0      1040\n",
      "30.0      994\n",
      "33.0      868\n",
      "53.0      745\n",
      "43.0      505\n",
      "13.0      391\n",
      "23.0      293\n",
      "63.0      195\n",
      "62.0      109\n",
      "60.0       63\n",
      "0.0        23\n",
      "80.0       12\n",
      "67.0        6\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0     5.0\n",
       "1     2.0\n",
       "2     4.0\n",
       "3    63.0\n",
       "4     1.0\n",
       "Name: district_code, dtype: float64"
      ]
     },
     "execution_count": 1057,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Column district_code # Since has 20 unique codes i chose to use normalized frequncy encoding.\n",
    "                    # Since frequncy of all codes are not balanced therefore frequency encoding\n",
    "                    # may result in a biased model..I think solution is to use the region_code column as well\n",
    "print(\"NaN count\",df[\"district_code\"].isnull().sum()) # Check if there are any null values in the column\n",
    "print(\"Unique values:\",df[\"district_code\"].nunique()) # Print the distinct values of the column\n",
    "\n",
    "print(df[\"district_code\"].describe()) # Print the value counts of the column\n",
    "print(\"code vs freq\",df[\"district_code\"].value_counts()) # Print the value counts of the column\n",
    "df[\"district_code\"].head() # Print the first 5 rows of the column\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1058,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN count: 31\n",
      "Unique values: 125\n",
      "count      59369\n",
      "unique       125\n",
      "top       Njombe\n",
      "freq        2502\n",
      "Name: lga, dtype: object\n",
      "0       Ludewa\n",
      "1    Serengeti\n",
      "2    Simanjiro\n",
      "3     Nanyumbu\n",
      "4      Karagwe\n",
      "Name: lga, dtype: object\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    0.009500\n",
       "1    0.012026\n",
       "2    0.005188\n",
       "3    0.002661\n",
       "4    0.012987\n",
       "Name: lga, dtype: float64"
      ]
     },
     "execution_count": 1058,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Column lga # No NaN, 125 unique values, used normalized frequency encoding\n",
    "print(\"NaN count:\",df[\"lga\"].isnull().sum())# Print the value counts of the column\n",
    "print(\"Unique values:\",df[\"lga\"].nunique())# Print the value counts of the column\n",
    "print(df[\"lga\"].describe()) # Print the description of the column\n",
    "print(df[\"lga\"].head()) # Print the description of the column\n",
    "freq_encoding=df[\"lga\"].value_counts(normalize=True) # Frequency Encoding for lga column\n",
    "df[\"lga\"]=df[\"lga\"].map(freq_encoding) # Map the frequencies to the original column\n",
    "df[\"lga\"].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1059,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN count: 31\n",
      "Unique values: 2092\n",
      "count     59369\n",
      "unique     2092\n",
      "top       Igosi\n",
      "freq        307\n",
      "Name: ward, dtype: object\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0      Mundindi\n",
       "1         Natta\n",
       "2       Ngorika\n",
       "3      Nanyumbu\n",
       "4    Nyakasimbi\n",
       "Name: ward, dtype: object"
      ]
     },
     "execution_count": 1059,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Column ward # No NaN, 2092 unique values, used normalized frequency encoding\n",
    "print(\"NaN count:\",df[\"ward\"].isnull().sum())# Print the value counts of the column\n",
    "print(\"Unique values:\",df[\"ward\"].nunique())# Print the value counts of the column\n",
    "print(df[\"ward\"].describe()) # Print the description of the column\n",
    "#print(df[\"ward\"].head()) # Print the description of the column\n",
    "freq_encoding=df[\"ward\"].value_counts(normalize=True) # Frequency Encoding for ward column\n",
    "df[\"ward_freq\"]=df[\"ward\"].map(freq_encoding) # Map the frequencies to the original column\n",
    "df[\"ward\"].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column population\n",
    "print('NaN count:',df['population'].isnull().sum()) # Check if there are any null values in the column\n",
    "#print('Unique values',df['population'].value_counts()) # Print the value counts of the column\n",
    "print('Median',df['population'].median()) # Print the median of the column\n",
    "print(df['population'].describe())\n",
    "plt.figure(figsize=(8,5))\n",
    "sns.histplot(df[\"population\"], bins=200, kde=True) # Plot the histogram of the column\n",
    "plt.figure(figsize=(8,5))\n",
    "sns.boxplot(df[\"population\"])\n",
    "sns.stripplot(df[\"population\"], color=\"red\", alpha=0.5)                  # Adds all points\n",
    "plt.ylim(-100,3000)\n",
    "plt.title(\"Boxplot of Population Before Outlier Removal\")\n",
    "plt.show() # Show the plot\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.scatter(df.index, df['population'], color='blue', label='Values')\n",
    "plt.show() # Show the plot\n",
    "\n",
    "# Remove the outliers from the population column\n",
    "Q1=df[\"population\"].quantile(0.25) # 25th percentile\n",
    "Q3=df[\"population\"].quantile(0.75) # 75th percentile\n",
    "IQR=Q3-Q1 # Interquartile range\n",
    "Lower_bound_population=Q1-1.5*IQR # Lower bound\n",
    "Upper_bound_population=Q3+1.5*IQR # Upper bound\n",
    "\n",
    "df[\"population_outlier_replaced_NaN\"]=df[\"population\"].apply(lambda x:x if ((x>=Lower_bound_population) & (x<=Upper_bound_population)) else np.nan) # Replace outliers with median value\n",
    "\n",
    "print(\"LB:\",Lower_bound_population)\n",
    "print(\"UB:\",Upper_bound_population)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "sns.boxplot(y=df[\"population\"])\n",
    "sns.stripplot(y=df[\"population_outlier_replaced_NaN\"], color=\"red\", alpha=0.5)\n",
    "plt.ylim(-100,3000)\n",
    "plt.title(\"Boxplot of Population After Outlier Removal With NaN\")\n",
    "plt.show() # Show the plot\n",
    "\n",
    "print('Nan count in population_outlier_replaced_NaN:',df['population_outlier_replaced_NaN'].isnull().sum()) # Check if there are any null values in the column\n",
    "df['population_outlier_replaced_mean'] = df.groupby('ward')['population_outlier_replaced_NaN'].transform(\n",
    "    lambda x: x.fillna(x.mean())\n",
    ")\n",
    "print(\"NaN count after imputaion: \",df['population_outlier_replaced_mean'].isnull().sum()) # Check if there are any null values in the column\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Column public_meeting # 3334 NaN values replaced with string Unknown, One Hot Encoding used\n",
    "print(df['public_meeting'].value_counts())                   # Print the value counts of the column\n",
    "df['public_meeting'].isnull().sum()                          # Check if there are any null values in the column)\n",
    "df['public_meeting']=df['public_meeting'].fillna('Unknown')  # Fill NaN values with 'Unknown'\n",
    "print(df['public_meeting'].value_counts())                   # Print the value counts of the column\n",
    "df = pd.get_dummies(df, columns=[\"public_meeting\"], prefix=\"public_meeting\", dtype=int)# Convert categorical variable into dummy/indicator variables\n",
    "df.drop(columns=[\"public_meeting_Unknown\"], inplace=True)    # Drop the first column to avoid dummy variable trap\n",
    "#print(df[['public_meeting_True','public_meeting_False']].head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column recorded_by # All values are \"GeoData Consultants Ltd\" ignore this column\n",
    "print('Nan count:',df['recorded_by'].isnull().sum()) # Check if there are any null values in the column\n",
    "df['recorded_by'].describe() # Print the value counts of the column\n",
    "print('Unique values:',df['recorded_by'].nunique()) # Print the value counts of the column\n",
    "print(df['recorded_by'].describe())\n",
    "df.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column scheme_management # could not decide about this column..drop/keep\n",
    "print('Nan count',df['scheme_management'].isnull().sum())\n",
    "#print(df['scheme_name'].nunique())\n",
    "print(df['scheme_management'].value_counts())\n",
    "#pd.crosstab(df['scheme_name'],df['scheme_management'])\n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "print(df[df['scheme_management'].isna()][['scheme_management','scheme_name']]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Column scheme_name # Nan count is 28810..almost 50 percemt so ignore the column\n",
    "print('Nan count:',df['scheme_name'].isnull().sum()) # Check if there are any null values in the column\n",
    "df['scheme_name'].describe() # Print the value counts of the column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column permit # NaN replaced with unknown and 1 hot encdoing is used\n",
    "print('Nan count:',df['permit'].isnull().sum()) # Check if there are any null values in the column\n",
    "print(df['permit'].describe())\n",
    "print('value count:',df['permit'].value_counts())\n",
    "df['permit']=df['permit'].fillna('Unknown')\n",
    "print('Nan count:',df['permit'].isnull().sum()) # Check if there are any null values in the column\n",
    "print('value count:',df['permit'].value_counts())\n",
    "df=pd.get_dummies(df,columns=['permit'],prefix='permit',dtype=int)\n",
    "df.drop(columns=['permit_Unknown'],inplace=True) # Drop the first column to avoid dummy variable trap\n",
    "df[['permit_False', 'permit_True']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Column construction_year # 1/3 values are zero..dont know yet how to deal with it\n",
    "print('Nan count:',df['construction_year'].isnull().sum()) # Check if there are any null values in the column\n",
    "print(df['construction_year'].value_counts()) # Print the value counts of the column\n",
    "print('unique values:',df['construction_year'].nunique()) # Print the value counts of the column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns [extraction_type, extraction_type_group, extraction_type_class]\n",
    "# mismatch b/w extraction type and group 2469\n",
    "# outof 2469, the count for mismatch rows between group and class is: 486\n",
    "# Based on anlysis I decided to drop two columns [extraction_type, extraction_type_class]\n",
    "# use column ['extraction_type_group']\n",
    "# Since 'extraction_type_group' has 0 NaN values and 13 unique values, use one hot encoding\n",
    "\n",
    "print('mismatch b/w extraction type and group',(df['extraction_type'] != df['extraction_type_group']).sum()) # Check if there are any mismatch values in the type and group columns\n",
    "print('mismatch b/w extraction type and class',(df['extraction_type'] != df['extraction_type_class']).sum()) # Check if there are any mismatch values in the type and class column\n",
    "print('mismatch b/w extraction group and class',(df['extraction_type_group'] != df['extraction_type_class']).sum()) # Check if there are any mismatch values in the group and class column\n",
    "print('--------------------------------------------------------')\n",
    "a=df[df['extraction_type'] != df['extraction_type_group']][['extraction_type_group','extraction_type_class']]\n",
    "print(\"Out of 2469, the count for mismatch rows between group and class is:\", (a['extraction_type_group'] != a['extraction_type_class']).sum())\n",
    "print('--------------------------------------------------------')\n",
    "print('Nan count',df['extraction_type_group'].isnull().sum())                                               # Check if there are any null values in the column\n",
    "print(df['extraction_type_group'].value_counts())                                                           # Print the value counts of the column\n",
    "print('Unique values:',df['extraction_type_group'].nunique())                                               # Print the value counts of the column\n",
    "print('--------------------------------------------------------')\n",
    "df=pd.get_dummies(df,columns=['extraction_type_group'],prefix='extraction_type_group',dtype=int)            # Convert categorical variable into dummy/indicator variables\n",
    "df.drop(columns=['extraction_type_group_afridev'],inplace=True)                                             # Drop the first column to avoid dummy variable trap\n",
    "\"\"\"\n",
    "df[['extraction_type_group_gravity', 'extraction_type_group_india mark ii',\n",
    "       'extraction_type_group_india mark iii', 'extraction_type_group_mono',\n",
    "       'extraction_type_group_nira/tanira', 'extraction_type_group_other',\n",
    "       'extraction_type_group_other handpump',\n",
    "       'extraction_type_group_other motorpump',\n",
    "       'extraction_type_group_rope pump', 'extraction_type_group_submersible',\n",
    "       'extraction_type_group_swn 80', 'extraction_type_group_wind-powered']].head()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column ['management', 'management_group'] # No NaN Values.\n",
    "# 12 unique values in management\n",
    "# 5 unique values in management_group\n",
    "# Used one hot encoding for both columns\n",
    "df[['management', 'management_group']].head()\n",
    "df['management'].nunique()                                                                      # Print the number of unique values in the column\n",
    "pd.crosstab(df['management'],df['management_group'])                                            # Print the cross tabulation of management and management_group columns\n",
    "pd.crosstab(df['management'],df['target'])                                                      # Print the cross tabulation of management and target columns\n",
    "pd.crosstab(df['management_group'],df['target'])                                                # Print the cross tabulation of management_group and target columns\n",
    "print('--------------------------------------------------------')\n",
    "print('Nan count in management column',df['management'].isnull().sum())                         # Check if there are any null values in the column\n",
    "print('value_counts in management column',df['management'].value_counts())                      # Print the value counts of the column\n",
    "print('Unique values in management column',df['management'].nunique())                          # Print the number of unique values in the column\n",
    "print('--------------------------------------------------------')\n",
    "print('Nan count in management_group column',df['management_group'].isnull().sum())             # Check if there are any null values in the column\n",
    "print('value_counts in management_group column',df['management_group'].value_counts())          # Print the value counts of the column\n",
    "print('Unique values in management_group column',df['management_group'].nunique())              # Print the number of unique values in the column\n",
    "print('--------------------------------------------------------')\n",
    "df=pd.get_dummies(df,columns=['management'],prefix='management',dtype=int)                      # Convert categorical variable into dummy/indicator variables\n",
    "df=pd.get_dummies(df,columns=['management_group'],prefix='management_group',dtype=int)          # Convert categorical variable into dummy/indicator variables\n",
    "\n",
    "df.drop(columns=['management_unknown'],inplace=True)                                           # Drop the first column to avoid dummy variable trap\n",
    "df.drop(columns=['management_group_unknown'],inplace=True)                                     # Drop the first column to avoid dummy variable trap\n",
    "\n",
    "\"\"\"\n",
    "df[[   'management_company', \n",
    "       'management_other', \n",
    "       'management_other - school',\n",
    "       'management_parastatal', 'management_private operator',\n",
    "       'management_trust', 'management_vwc',\n",
    "       'management_water authority', 'management_water board',\n",
    "       'management_wua', 'management_wug']].head()\n",
    "df[['management_group_commercial',\n",
    "    'management_group_other', \n",
    "    'management_group_parastatal', \n",
    "    'management_group_user-group']].head()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Columns ['payment', 'payment_type']\n",
    "# No Nan values and 07 Unique categories in both columns\n",
    "# Semantically same values in both columns\n",
    "# After mapping(staderdization) choose one column i.e payment,\n",
    "# One hot encoding is used for payment column\n",
    "print(df['payment'].value_counts()) #\n",
    "print('--------------------------------------------------------')\n",
    "print(df['payment_type'].value_counts()) # Check if there are any null values in the column\n",
    "\n",
    "# The values in the two columns semantically are almost same just differet wording\n",
    "# Use same wording in both columns to avoid confusion\n",
    "\n",
    "standard_map = {\n",
    "    'annually': 'annually',\n",
    "    'pay annually': 'annually',\n",
    "    'monthly': 'monthly',\n",
    "    'pay monthly': 'monthly',\n",
    "    'per bucket': 'per bucket',\n",
    "    'pay per bucket': 'per bucket',     # Define mapping to use same wording in both columns ['payment, 'payment_type'] to avoid confusion\n",
    "    'on failure': 'on failure',\n",
    "    'pay when scheme fails': 'on failure',\n",
    "    'never pay': 'never pay',\n",
    "    'unknown': 'unknown',\n",
    "    'other': 'other'\n",
    "}\n",
    "df['payment'] = df['payment'].map(standard_map)\n",
    "df['payment_type'] = df['payment_type'].map(standard_map)\n",
    "print(df['payment'].value_counts()) #\n",
    "print('--------------------------------------------------------')\n",
    "print(df['payment_type'].value_counts()) # Check if there are any null values in the column\n",
    "print('After mapping: mismatch count:should be zero',(df['payment'] != df['payment_type']).sum()) # Check if there are any null values in the column\n",
    "print('Nan count:',df['payment'].isnull().sum()) # Check if there are any null values in the column\n",
    "print('Unique values', df['payment'].nunique())# Check if there are any null values in the column\n",
    "df=pd.get_dummies(df,columns=['payment'],prefix='payment',dtype=int) # Convert categorical variable into dummy/indicator variables\n",
    "#print(df.columns)\n",
    "df.drop(columns=['payment_unknown'],inplace=True) # Drop the first column to avoid dummy variable trap\n",
    "df[['payment_annually', 'payment_monthly', 'payment_never pay',\n",
    "       'payment_on failure', 'payment_other', 'payment_per bucket']].head(5) # Print the first 5 rows "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns ['water_quality', 'quality_group']\n",
    "# Based on the value_counts water_quality is the detailed version of quality_group\n",
    "# No NaN values in both columns\n",
    "# 8 categories in water_quality and 6 categories in quality_group\n",
    "# I chose one hot encoding for water quality and decided to drop quality_group column\n",
    "#print(df['water_quality'].head())\n",
    "print('--------------------------------------------------------')\n",
    "print(df['water_quality'].value_counts()) # Check if there are any null values in the column\n",
    "print('Nan count in water quality:',df['water_quality'].isnull().sum()) # Check if there are any null values in the column\n",
    "print('--------------------------------------------------------')\n",
    "print(df['quality_group'].value_counts()) # Check if there are any null values in the column\n",
    "print('Nan count in quality group:',df['water_quality'].isnull().sum()) # Check if there are any null values in the column\n",
    "df=pd.get_dummies(df,columns=['water_quality'],prefix='wate_quality',dtype=int) # Convert categorical variable into dummy/indicator variables\n",
    "df.drop(columns=['wate_quality_unknown'],inplace=True) # Drop the first column to avoid dummy variable trap\n",
    "df.columns\n",
    "#df[['water_quality','quality_group']].head()\n",
    "\n",
    "df[['wate_quality_coloured', 'wate_quality_fluoride',\n",
    "       'wate_quality_fluoride abandoned', 'wate_quality_milky',\n",
    "       'wate_quality_salty', 'wate_quality_salty abandoned',\n",
    "       'wate_quality_soft' ]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns ['quantity', 'quantity_group'] are same\n",
    "# No Nan Values in both columns\n",
    "# 5 categories in both columns\n",
    "# I chose to keep quantity column and drop quantity_group column\n",
    "# 1 hot encoding is used for quantity column\n",
    "\n",
    "print(df['quantity'].head()) \n",
    "print(df['quantity'].value_counts()) # Check if there are any null values in the column\n",
    "print('Nan count in quantity',df['quantity'].isnull().sum()) # Check if there are any null values in the column\n",
    "\n",
    "print('---------------------------------------------------------')\n",
    "print(df['quantity_group'].value_counts()) # Check if there are any null values in the column\n",
    "print('Nan count in quantity group',df['quantity_group'].isnull().sum()) # Check if there are any null values in the column\n",
    "print('---------------------------------------------------------')\n",
    "df=pd.get_dummies(df,columns=['quantity'],prefix='quantity',dtype=int) # Convert categorical variable into dummy/indicator variables\n",
    "df.drop(columns=['quantity_unknown'],inplace=True) # Drop the first column to avoid dummy variable trap\n",
    "print(df[['quantity_dry', 'quantity_enough',\n",
    "       'quantity_insufficient', 'quantity_seasonal']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns ['source', 'source_type']\n",
    "# No NaN values in both columns\n",
    "# 'source' is the detailed version of 'source_type'\n",
    "# 10 categories in source and 7 categories in source_type\n",
    "# I chose to keep source column and drop source_type column\n",
    "# 1 hot encoding is used for source column\n",
    "print(df['source'].head()) # Print the first 5 rows of the column\n",
    "print('---------------------------------------------------------')\n",
    "print(df['source'].value_counts()) # Check if there are any null values in the column\n",
    "print('Nan count in source',df['source'].isnull().sum()) # Check if there are any null values in the column\n",
    "\n",
    "print('---------------------------------------------------------')\n",
    "print(df['source_type'].value_counts()) # Check if there are any null values in the column\n",
    "print('Nan count in source type',df['source_type'].isnull().sum()) # Check if there are any null values in the column\n",
    "\n",
    "print('---------------------------------------------------------')\n",
    "df=pd.get_dummies(df,columns=['source'],prefix='source',dtype=int) # Convert categorical variable into dummy/indicator variables\n",
    "df.drop(columns=['source_unknown'],inplace=True) # Drop the first column to avoid dummy variable trap\n",
    "df[['source_dam', 'source_hand dtw', 'source_lake',\n",
    "       'source_machine dbh', 'source_other', 'source_rainwater harvesting',\n",
    "       'source_river', 'source_shallow well', 'source_spring']].head(5) # Print the first 5 rows of the column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column ['source_class']\n",
    "# No Nan values in the column\n",
    "# 3 categories in the column\n",
    "# 1 hot encoding is used for source_class column\n",
    "print(df['source_class'].head()) # Print the first 5 rows of the column\n",
    "print('---------------------------------------------------------')\n",
    "#print(pd.crosstab(df['source_class'],df['source_type'])) # Print the cross tabulation of source and source_type columns\n",
    "print(df['source_class'].value_counts()) # Check if there are any null values in the column\n",
    "print('Nan count in source class',df['source_class'].isnull().sum()) # Check if there are any null values in the column\n",
    "print('---------------------------------------------------------')\n",
    "df=pd.get_dummies(df,columns=['source_class'],prefix='source_class',dtype=int) # Convert categorical variable into dummy/indicator variables\n",
    "df.drop(columns=['source_class_unknown'],inplace=True) # Drop the first column to avoid dummy variable trap\n",
    "df[['source_class_groundwater', 'source_class_surface']].head() # Print the first 5 rows of the column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns ['waterpoint_type', 'waterpoint_type_group'] have same info\n",
    "# 'waterpoint_type' is the detailed version of 'waterpoint_type_group'\n",
    "# No NaN values in both columns\n",
    "# 7 categories in waterpoint_type and 6 categories in waterpoint_type_group\n",
    "# I chose to keep waterpoint_type column and drop waterpoint_type_group column\n",
    "# 1 hot encoding is used for waterpoint_type column\n",
    "print(df['waterpoint_type'].head()) \n",
    "print('---------------------------------------------------------')\n",
    "print(df['waterpoint_type'].value_counts()) # Check if there are any null values in the column\n",
    "print('Nan count in waterpoint type',df['waterpoint_type'].isnull().sum()) # Check if there are any null values in the column\n",
    "\n",
    "print('---------------------------------------------------------')\n",
    "print(df['waterpoint_type_group'].value_counts()) # Check if there are any null values in the column\n",
    "print('Nan count in waterpoint type group',df['waterpoint_type_group'].isnull().sum()) # Check if there are any null values in the column\n",
    "print('---------------------------------------------------------')\n",
    "df=pd.get_dummies(df, columns=['waterpoint_type'],prefix='waterpoint_type',dtype=int) # Convert categorical variable into dummy/indicator variables\n",
    "df.drop(columns=['waterpoint_type_other'],inplace=True) # Drop the first column to avoid dummy variable trap\n",
    "df[['waterpoint_type_cattle trough',\n",
    "       'waterpoint_type_communal standpipe',\n",
    "       'waterpoint_type_communal standpipe multiple', 'waterpoint_type_dam',\n",
    "       'waterpoint_type_hand pump', 'waterpoint_type_improved spring']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "28522+6103"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
