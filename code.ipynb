{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import sklearn\n",
    "from sklearn.preprocessing import  MinMaxScaler,OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import joblib # Save and load preprocessing pieline and models\n",
    "\n",
    "\n",
    "scaler_minmax= MinMaxScaler()                                                             # Create a MinMaxScaler object\n",
    "ohe = OneHotEncoder(sparse_output=False, handle_unknown='ignore',drop='first')            # Create a OneHotEncoder object\n",
    "\n",
    "\n",
    "# Read CSV files\n",
    "\n",
    "df=pd.read_csv(\"Training_Set_Values.csv\")       # Read the Training data CSV file\n",
    "name_featrures=df.columns                       # Get the features name\n",
    "len_features=len(name_featrures)                # Get the length of features\n",
    "labels=pd.read_csv(\"Training_Set_Labels.csv\")   # Read the labels (target) CSV file\n",
    "labels.head()\n",
    "df['target'] = labels['status_group']           # Add the target column to the dataframe\n",
    "#print(df.shape)                                 # Print the shape of the dataframe\n",
    "#df.head()\n",
    "#df.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape after dropping columns: (59400, 22)\n"
     ]
    }
   ],
   "source": [
    "# Columns to be dropped for the baseline models\n",
    "columns_drop=['id','amount_tsh','num_private','subvillage','recorded_by','scheme_name',\n",
    "              'extraction_type_group','extraction_type_class',\n",
    "              'management','payment_type','quality_group','quantity_group','source','waterpoint_type_group',\n",
    "              'funder','installer','wpt_name','ward','scheme_management']\n",
    "\n",
    "df = df.drop(columns=columns_drop)\n",
    "print('Shape after dropping columns:', df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the target column to labels \n",
    "#print(df['target'].unique())\n",
    "target_map_dict={'functional': 2, 'functional needs repair': 1, 'non functional': 0} # Defined the mapping of labels to numbers (integers)\n",
    "#print(df['target'].head())\n",
    "df['target'] =df['target'].map(target_map_dict) # transform the target column (labels) to  numbers (integers)\n",
    "#df['target'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and target and perform train test split\n",
    "X = df.drop(columns=['target'])  # Features only\n",
    "y = df['target']                 # Target column\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.04, random_state=42, stratify=y)  # 2376 records for the test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['preprocessor.joblib']"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the custom transformers form helper_function.py\n",
    "# The helper_function.py file contains the definitions for StringConverter, YearExtractor, IQRCapper, and ConstructionYearTransformer\n",
    "from helper_function import (\n",
    "    StringConverter,\n",
    "    YearExtractor,\n",
    "    IQRCapper,\n",
    "    ConstructionYearTransformer,\n",
    "    ObjectToNumericConverter\n",
    ")\n",
    "    \n",
    "#pipeline transformers\n",
    "date_recorded_transformer_pipeline=Pipeline([\n",
    "    \n",
    "    ('year_extractor',YearExtractor()),\n",
    "    ('onehot', OneHotEncoder(sparse_output=False, handle_unknown='ignore',drop='first'))\n",
    "])\n",
    "\n",
    "\n",
    "oulier_minmax_pipeline_clip = Pipeline(steps=[\n",
    "    ('iqr_cap', IQRCapper(strategy='clip')),\n",
    "    ('scaler', MinMaxScaler())\n",
    "])\n",
    "\n",
    "oulier_minmax_pipeline_mean = Pipeline(steps=[\n",
    "    ('iqr_cap', IQRCapper(strategy='mean')),\n",
    "    ('scaler', MinMaxScaler())\n",
    "])\n",
    "\n",
    "oulier_minmax_pipeline_median = Pipeline(steps=[\n",
    "    ('iqr_cap', IQRCapper(strategy='median')),\n",
    "    ('scaler', MinMaxScaler())\n",
    "])\n",
    "\n",
    "\n",
    "cat_pipeline = Pipeline([\n",
    "     ('string_converter', StringConverter()),\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='Unknown')),\n",
    "    ('ohe', OneHotEncoder(sparse_output=False, handle_unknown='ignore',drop='first'))\n",
    "])\n",
    "\n",
    "constructionyear_pipeline = Pipeline(steps=[\n",
    "    ('replace_zeros_with_median', ConstructionYearTransformer()),\n",
    "    ('minmax_scaling', MinMaxScaler())\n",
    "])\n",
    "# ColumnTransformer and full pipeline setup for feature preprocessing\n",
    "# The ColumnTransformer allows us to apply different preprocessing steps to different columns of the DataFrame\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('date', date_recorded_transformer_pipeline, ['date_recorded']),\n",
    "        #('gps_height', scaler_minmax, ['gps_height']),\n",
    "        ('outlier_minmax_gps_height', oulier_minmax_pipeline_mean, ['gps_height']),\n",
    "        ('outlier_minmax_longitude', oulier_minmax_pipeline_mean, ['longitude']),\n",
    "        ('outlier_minmax_latitude', oulier_minmax_pipeline_mean, ['latitude']),\n",
    "         ('cat_ohe', cat_pipeline, ['basin','region','region_code','district_code','lga','public_meeting','permit','extraction_type',\n",
    "                                    'management_group','payment','water_quality','quantity','source_type','source_class',\n",
    "                                    'waterpoint_type']),\n",
    "        ('outlier_minmax_population', oulier_minmax_pipeline_clip, ['population']),\n",
    "        ('constructionyear', constructionyear_pipeline, ['construction_year'])\n",
    "\n",
    "\n",
    "    ],\n",
    "    remainder='passthrough',\n",
    "    verbose_feature_names_out=False\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "preprocess_pipeline = Pipeline([\n",
    "    ('preprocessing', preprocessor),\n",
    "    ('object_to_numeric', ObjectToNumericConverter())  # your custom step\n",
    "])\n",
    "\n",
    "\n",
    "X_train_transformed = preprocessor.fit_transform(X_train)\n",
    "X_test_transformed = preprocessor.transform(X_test)\n",
    "# Save the preprocessor (this is done only once)\n",
    "joblib.dump(preprocessor, \"preprocessor.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Comparison:\n",
      "Model           Train Acc       Test Acc       \n",
      "---------------------------------------------\n",
      "DecisionTree    0.7672          0.7496         \n",
      "RandomForest    0.8141          0.7765         \n",
      "XGBoost         0.8607          0.8068         \n",
      "\n",
      "Top 10 Important Features:\n",
      "\n",
      "DecisionTree:\n",
      "waterpoint_type_other          0.2024\n",
      "quantity_seasonal              0.1305\n",
      "quantity_enough                0.1124\n",
      "quantity_insufficient          0.0861\n",
      "longitude                      0.0624\n",
      "construction_year              0.0595\n",
      "latitude                       0.0408\n",
      "waterpoint_type_communal standpipe multiple 0.0378\n",
      "population                     0.0175\n",
      "gps_height                     0.0145\n",
      "\n",
      "RandomForest:\n",
      "quantity_enough                0.0745\n",
      "longitude                      0.0676\n",
      "waterpoint_type_other          0.0663\n",
      "latitude                       0.0649\n",
      "construction_year              0.0643\n",
      "extraction_type_other          0.0635\n",
      "gps_height                     0.0393\n",
      "population                     0.0301\n",
      "quantity_insufficient          0.0257\n",
      "waterpoint_type_communal standpipe 0.0256\n",
      "\n",
      "XGBoost:\n",
      "waterpoint_type_other          0.0560\n",
      "quantity_seasonal              0.0248\n",
      "lga_Bariadi                    0.0229\n",
      "region_code_11                 0.0223\n",
      "extraction_type_other          0.0194\n",
      "region_Iringa                  0.0188\n",
      "lga_Kigoma Rural               0.0123\n",
      "region_code_18                 0.0119\n",
      "lga_Rombo                      0.0113\n",
      "region_code_17                 0.0109\n"
     ]
    }
   ],
   "source": [
    "# models (Decision Tree, Random Forest, XGBoost) to be used\n",
    "models = {    \n",
    "    \"DecisionTree\": DecisionTreeClassifier(\n",
    "        max_depth=10,  # You can tune this\n",
    "        random_state=42\n",
    "    ),\n",
    "    \"RandomForest\": RandomForestClassifier(\n",
    "        n_estimators=100,\n",
    "        max_depth=15,  # You can tune this too\n",
    "        random_state=42\n",
    "    ),\"XGBoost\": XGBClassifier(\n",
    "        n_estimators=100,\n",
    "        max_depth=11,\n",
    "        learning_rate=0.1,\n",
    "        subsample=0.9,\n",
    "        colsample_bytree=0.8,\n",
    "        #use_label_encoder=False,\n",
    "        eval_metric='mlogloss',    # good for multi-class\n",
    "        objective='multi:softmax', # directly outputs class labels\n",
    "        num_class=3,               # number of target classes\n",
    "        random_state=42\n",
    "    )\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "# Results and feature importances storage\n",
    "results = {}\n",
    "feature_importances = {}\n",
    "\n",
    "# Loop through each model\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train_transformed, y_train)  # Fit the model on the transformed training data\n",
    "    # Save the trained model for later use\n",
    "    joblib.dump(model, f\"{name}_model.joblib\")\n",
    "    # Predictions\n",
    "    y_train_pred = model.predict(X_train_transformed)\n",
    "    y_test_pred =  model.predict(X_test_transformed)\n",
    "\n",
    "    # Accuracy scores\n",
    "    train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "    test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "    results[name] = {\n",
    "        \"Train Accuracy\": train_accuracy,\n",
    "        \"Test Accuracy\": test_accuracy\n",
    "    }\n",
    "    \n",
    "    # Extract feature importances  \n",
    "    if hasattr(model, 'feature_importances_'):\n",
    "        # Get transformed feature names from preprocessor\n",
    "        feature_names = preprocessor.get_feature_names_out()\n",
    "        importances = model.feature_importances_\n",
    "        feature_importances[name] = sorted(\n",
    "            zip(feature_names, importances),\n",
    "            key=lambda x: x[1],\n",
    "            reverse=True\n",
    "        )\n",
    "\n",
    "# Print results\n",
    "print(\"\\nModel Comparison:\")\n",
    "print(\"{:<15} {:<15} {:<15}\".format(\"Model\", \"Train Acc\", \"Test Acc\"))\n",
    "print(\"-\" * 45)\n",
    "for model_name, scores in results.items():\n",
    "    print(\"{:<15} {:<15.4f} {:<15.4f}\".format(model_name, scores[\"Train Accuracy\"], scores[\"Test Accuracy\"]))\n",
    "\n",
    "# Print top features\n",
    "print(\"\\nTop 10 Important Features:\")\n",
    "for model_name, importance_list in feature_importances.items():\n",
    "    print(f\"\\n{model_name}:\")\n",
    "    for feature, importance in importance_list[:10]:\n",
    "        print(f\"{feature:<30} {importance:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nModel Comparison:\\nModel           Train Acc       Test Acc       \\n---------------------------------------------\\nDecision Tree   0.7672          0.7496         \\nRandom Forest   0.8141          0.7765         \\nXGBoost         0.8607          0.8068  \\n'"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Model Comparison:\n",
    "Model           Train Acc       Test Acc       \n",
    "---------------------------------------------\n",
    "Decision Tree   0.7672          0.7496         \n",
    "Random Forest   0.8141          0.7765         \n",
    "XGBoost         0.8607          0.8068  \n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_4076\\1964544937.py:15: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  Result={'Index': random_index,'True label': int(y_test_sample), 'Predicted label': int(y_test_sample_pred)}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Index</th>\n",
       "      <th>True label</th>\n",
       "      <th>Predicted label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1947</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Index  True label  Predicted label\n",
       "0   1947           0                2"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing the model on one sample form the test data\n",
    "import random\n",
    "\n",
    "# Load preprocessor and a trained model (e.g., RandomForest)\n",
    "preprocessor = joblib.load(\"preprocessor.joblib\")\n",
    "model = joblib.load(\"RandomForest_model.joblib\")  # Change to desired model: let say I chose RandomForest form list: [DecisionTree, RandomForest and XGBoost]\n",
    "# Pick a random index from the test set\n",
    "random_index = random.randint(0, len(X_test) - 1)\n",
    "X_test_sample=X_test.iloc[[random_index]]  # extra [] is sued to get the dataframe not a series as our pipeline expects dataframe as input\n",
    "y_test_sample=y_test.iloc[random_index]\n",
    "# Preprocess the test sample\n",
    "X_test_sample_transformed = preprocessor.transform(X_test_sample)\n",
    "y_test_sample_pred = model.predict(X_test_sample_transformed)\n",
    "\n",
    "Result={'Index': random_index,'True label': int(y_test_sample), 'Predicted label': int(y_test_sample_pred)}\n",
    "Result_df = pd.DataFrame([Result])\n",
    "Result_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
