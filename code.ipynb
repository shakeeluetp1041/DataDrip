{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import sklearn\n",
    "from sklearn.preprocessing import  MinMaxScaler,OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "scaler_minmax= MinMaxScaler()                                                             # Create a MinMaxScaler object\n",
    "ohe = OneHotEncoder(sparse_output=False, handle_unknown='ignore',drop='first')            # Create a OneHotEncoder object\n",
    "\n",
    "\n",
    "# Read CSV files\n",
    "\n",
    "df=pd.read_csv(\"Training_Set_Values.csv\")       # Read the Training data CSV file\n",
    "name_featrures=df.columns                       # Get the features name\n",
    "len_features=len(name_featrures)                # Get the length of features\n",
    "labels=pd.read_csv(\"Training_Set_Labels.csv\")   # Read the labels (target) CSV file\n",
    "labels.head()\n",
    "df['target'] = labels['status_group']           # Add the target column to the dataframe\n",
    "#print(df.shape)                                 # Print the shape of the dataframe\n",
    "#df.head()\n",
    "#df.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape after dropping columns: (59400, 22)\n"
     ]
    }
   ],
   "source": [
    "# Columns to be dropped for baseline model\n",
    "columns_drop=['id','amount_tsh','num_private','subvillage','recorded_by','scheme_name',\n",
    "              'extraction_type_group','extraction_type_class',\n",
    "              'management','payment_type','quality_group','quantity_group','source','waterpoint_type_group',\n",
    "              'funder','installer','wpt_name','ward','scheme_management']\n",
    "\n",
    "df = df.drop(columns=columns_drop)\n",
    "print('Shape after dropping columns:', df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the target column to labels \n",
    "#print(df['target'].unique())\n",
    "target_map_dict={'functional': 2, 'functional needs repair': 1, 'non functional': 0} # Defined the mapping of labels to numbers (integers)\n",
    "#print(df['target'].head())\n",
    "df['target'] =df['target'].map(target_map_dict) # transform the target column (labels) to  numbers (integers)\n",
    "#df['target'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and target and perform train test split\n",
    "X = df.drop(columns=['target'])  # Features only\n",
    "y = df['target']                 # Target column\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.04, random_state=42, stratify=y)  # 2376 records for the test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the custom transformers form helper_function.py\n",
    "# The helper_function.py file contains the definitions for StringConverter, YearExtractor, IQRCapper, and ConstructionYearTransformer\n",
    "from helper_function import (\n",
    "    StringConverter,\n",
    "    YearExtractor,\n",
    "    IQRCapper,\n",
    "    ConstructionYearTransformer,\n",
    "    ObjectToNumericConverter\n",
    ")\n",
    "    \n",
    "#pipeline transformers\n",
    "date_recorded_transformer_pipeline=Pipeline([\n",
    "    \n",
    "    ('year_extractor',YearExtractor()),\n",
    "    ('onehot', OneHotEncoder(sparse_output=False, handle_unknown='ignore',drop='first'))\n",
    "])\n",
    "\n",
    "\n",
    "oulier_minmax_pipeline_clip = Pipeline(steps=[\n",
    "    ('iqr_cap', IQRCapper(strategy='clip')),\n",
    "    ('scaler', MinMaxScaler())\n",
    "])\n",
    "\n",
    "oulier_minmax_pipeline_mean = Pipeline(steps=[\n",
    "    ('iqr_cap', IQRCapper(strategy='mean')),\n",
    "    ('scaler', MinMaxScaler())\n",
    "])\n",
    "\n",
    "oulier_minmax_pipeline_median = Pipeline(steps=[\n",
    "    ('iqr_cap', IQRCapper(strategy='median')),\n",
    "    ('scaler', MinMaxScaler())\n",
    "])\n",
    "\n",
    "\n",
    "cat_pipeline = Pipeline([\n",
    "     ('string_converter', StringConverter()),\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='Unknown')),\n",
    "    ('ohe', OneHotEncoder(sparse_output=False, handle_unknown='ignore',drop='first'))\n",
    "])\n",
    "\n",
    "constructionyear_pipeline = Pipeline(steps=[\n",
    "    ('replace_zeros_with_median', ConstructionYearTransformer()),\n",
    "    ('minmax_scaling', MinMaxScaler())\n",
    "])\n",
    "# ColumnTransformer and full pipeline setup for feature preprocessing\n",
    "# The ColumnTransformer allows us to apply different preprocessing steps to different columns of the DataFrame\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('date', date_recorded_transformer_pipeline, ['date_recorded']),\n",
    "        #('gps_height', scaler_minmax, ['gps_height']),\n",
    "        ('outlier_minmax_gps_height', oulier_minmax_pipeline_mean, ['gps_height']),\n",
    "        ('outlier_minmax_longitude', oulier_minmax_pipeline_mean, ['longitude']),\n",
    "        ('outlier_minmax_latitude', oulier_minmax_pipeline_mean, ['latitude']),\n",
    "         ('cat_ohe', cat_pipeline, ['basin','region','region_code','lga','public_meeting','permit','extraction_type','management_group','payment','water_quality','quantity','source_type','source_class','waterpoint_type']),\n",
    "        ('outlier_minmax_population', oulier_minmax_pipeline_clip, ['population']),\n",
    "        ('constructionyear', constructionyear_pipeline, ['construction_year'])\n",
    "\n",
    "\n",
    "    ],\n",
    "    remainder='passthrough',\n",
    "    verbose_feature_names_out=False\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "preprocess_pipeline = Pipeline([\n",
    "    ('preprocessing', preprocessor),\n",
    "    ('object_to_numeric', ObjectToNumericConverter())  # your custom step\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Aug 2024\\ReDI School\\Spring Semester 2025\\DataDrip_Water_Pump_Predictive_Maintainence\\venv\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [14:12:34] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Comparison:\n",
      "Model           Train Acc       Test Acc       \n",
      "---------------------------------------------\n",
      "Decision Tree   0.7668          0.7483         \n",
      "Random Forest   0.8137          0.7778         \n",
      "XGBoost         0.8598          0.8085         \n"
     ]
    }
   ],
   "source": [
    "# Define your models\n",
    "models = {    \n",
    "    \"Decision Tree\": DecisionTreeClassifier(\n",
    "        max_depth=10,  # You can tune this\n",
    "        random_state=42\n",
    "    ),\n",
    "    \"Random Forest\": RandomForestClassifier(\n",
    "        n_estimators=100,\n",
    "        max_depth=15,  # You can tune this too\n",
    "        random_state=42\n",
    "    ),\"XGBoost\": XGBClassifier(\n",
    "        n_estimators=100,\n",
    "        max_depth=11,\n",
    "        learning_rate=0.1,\n",
    "        subsample=0.9,\n",
    "        colsample_bytree=0.8,\n",
    "        use_label_encoder=False,\n",
    "        eval_metric='mlogloss',    # good for multi-class\n",
    "        objective='multi:softmax', # directly outputs class labels\n",
    "        num_class=3,               # number of target classes\n",
    "        random_state=42\n",
    "    )\n",
    "}\n",
    "\n",
    "# Store results\n",
    "results = {}\n",
    "\n",
    "# Loop through each model\n",
    "for name, model in models.items():\n",
    "    full_pipeline = Pipeline([\n",
    "        ('preprocessing', preprocessor),\n",
    "        ('model', model)\n",
    "                            ])\n",
    "\n",
    "\n",
    "    full_pipeline.fit(X_train, y_train) \n",
    "    \"\"\"\n",
    "    # Step 1: Each transformer in the preprocessing pipeline Computes and stores necessary statistics \n",
    "    # (e.g., quartiles, medians, scalers) from X_train only.\n",
    "    #Step 2: The transformers are applied (transformed) to X_train to produce the final preprocessed training features.\n",
    "    # Step 3: The model is trained using these transformed features and y_train.\n",
    "    \"\"\" \n",
    "\n",
    "\n",
    "    # Predictions\n",
    "    y_train_pred = full_pipeline.predict(X_train)\n",
    "    y_test_pred = full_pipeline.predict(X_test)\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    The stored training statistics are used to transform X_train again and X_test (no re-fitting!).\n",
    "\n",
    "    Feeds this transformed data to the already-trained model.\n",
    "\n",
    "    Outputs predictions (y_train_pred, y_test_pred).\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Accuracy scores\n",
    "    train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "    test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "    results[name] = {\n",
    "        \"Train Accuracy\": train_accuracy,\n",
    "        \"Test Accuracy\": test_accuracy\n",
    "    }\n",
    "\n",
    "# Print results\n",
    "print(\"\\nModel Comparison:\")\n",
    "print(\"{:<15} {:<15} {:<15}\".format(\"Model\", \"Train Acc\", \"Test Acc\"))\n",
    "print(\"-\" * 45)\n",
    "for model_name, scores in results.items():\n",
    "    print(\"{:<15} {:<15.4f} {:<15.4f}\".format(model_name, scores[\"Train Accuracy\"], scores[\"Test Accuracy\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2862310629.py, line 1)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[69]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mTraining Accuracy: 0.8499\u001b[39m\n             ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "Training Accuracy: 0.8499\n",
    " Test Accuracy:     0.8095"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Training Accuracy: 0.8598\n",
    " Test Accuracy:     0.8085"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "All arrays must be of the same length",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[62]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m feature_names = X.columns\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Create a DataFrame\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m importance_df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mFeature\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeature_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mImportance\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mimportances\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m}\u001b[49m\u001b[43m)\u001b[49m.sort_values(by=\u001b[33m'\u001b[39m\u001b[33mImportance\u001b[39m\u001b[33m'\u001b[39m, ascending=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# Display  important features\u001b[39;00m\n\u001b[32m     12\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m Important Features:\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Aug 2024\\ReDI School\\Spring Semester 2025\\DataDrip_Water_Pump_Predictive_Maintainence\\venv\\Lib\\site-packages\\pandas\\core\\frame.py:778\u001b[39m, in \u001b[36mDataFrame.__init__\u001b[39m\u001b[34m(self, data, index, columns, dtype, copy)\u001b[39m\n\u001b[32m    772\u001b[39m     mgr = \u001b[38;5;28mself\u001b[39m._init_mgr(\n\u001b[32m    773\u001b[39m         data, axes={\u001b[33m\"\u001b[39m\u001b[33mindex\u001b[39m\u001b[33m\"\u001b[39m: index, \u001b[33m\"\u001b[39m\u001b[33mcolumns\u001b[39m\u001b[33m\"\u001b[39m: columns}, dtype=dtype, copy=copy\n\u001b[32m    774\u001b[39m     )\n\u001b[32m    776\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, \u001b[38;5;28mdict\u001b[39m):\n\u001b[32m    777\u001b[39m     \u001b[38;5;66;03m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m778\u001b[39m     mgr = \u001b[43mdict_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmanager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    779\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ma.MaskedArray):\n\u001b[32m    780\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mma\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m mrecords\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Aug 2024\\ReDI School\\Spring Semester 2025\\DataDrip_Water_Pump_Predictive_Maintainence\\venv\\Lib\\site-packages\\pandas\\core\\internals\\construction.py:503\u001b[39m, in \u001b[36mdict_to_mgr\u001b[39m\u001b[34m(data, index, columns, dtype, typ, copy)\u001b[39m\n\u001b[32m    499\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    500\u001b[39m         \u001b[38;5;66;03m# dtype check to exclude e.g. range objects, scalars\u001b[39;00m\n\u001b[32m    501\u001b[39m         arrays = [x.copy() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(x, \u001b[33m\"\u001b[39m\u001b[33mdtype\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m arrays]\n\u001b[32m--> \u001b[39m\u001b[32m503\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43marrays_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtyp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconsolidate\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Aug 2024\\ReDI School\\Spring Semester 2025\\DataDrip_Water_Pump_Predictive_Maintainence\\venv\\Lib\\site-packages\\pandas\\core\\internals\\construction.py:114\u001b[39m, in \u001b[36marrays_to_mgr\u001b[39m\u001b[34m(arrays, columns, index, dtype, verify_integrity, typ, consolidate)\u001b[39m\n\u001b[32m    111\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m verify_integrity:\n\u001b[32m    112\u001b[39m     \u001b[38;5;66;03m# figure out the index, if necessary\u001b[39;00m\n\u001b[32m    113\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m         index = \u001b[43m_extract_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    116\u001b[39m         index = ensure_index(index)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Aug 2024\\ReDI School\\Spring Semester 2025\\DataDrip_Water_Pump_Predictive_Maintainence\\venv\\Lib\\site-packages\\pandas\\core\\internals\\construction.py:677\u001b[39m, in \u001b[36m_extract_index\u001b[39m\u001b[34m(data)\u001b[39m\n\u001b[32m    675\u001b[39m lengths = \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mset\u001b[39m(raw_lengths))\n\u001b[32m    676\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(lengths) > \u001b[32m1\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m677\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mAll arrays must be of the same length\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    679\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m have_dicts:\n\u001b[32m    680\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    681\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mMixing dicts with non-Series may lead to ambiguous ordering.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    682\u001b[39m     )\n",
      "\u001b[31mValueError\u001b[39m: All arrays must be of the same length"
     ]
    }
   ],
   "source": [
    "# Get feature importances\n",
    "importances = model.feature_importances_\n",
    "feature_names = X.columns\n",
    "\n",
    "# Create a DataFrame\n",
    "importance_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Importance': importances\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Display  important features\n",
    "print(\" Important Features:\")\n",
    "print(importance_df.head())\n",
    "\n",
    "# Optional: Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(importance_df['Feature'][:10][::-1], importance_df['Importance'][:10][::-1])\n",
    "plt.xlabel(\"Feature Importance\")\n",
    "plt.title(\"Feature importance in Random Forest\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importance_df.sort_values(by='Importance').head(No_of_col_to_drop)\n",
    "\n",
    "importances = model.feature_importances_\n",
    "feature_names = X.columns\n",
    "len(importances),len(importance_df['Importance']),len(importance_df['Feature'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importance_df.sort_values(by='Importance', ascending=False)[['Feature', 'Importance']]\n",
    "importance_df['Feature'].head(20).tolist()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
