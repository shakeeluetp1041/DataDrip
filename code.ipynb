{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler,OrdinalEncoder,LabelEncoder,OneHotEncoder\n",
    "\n",
    "\n",
    "scaler_minmax= MinMaxScaler()                                           # Create a MinMaxScaler object\n",
    "scaler_standered=StandardScaler()                                       # Create a StandardScaler object\n",
    "oe=OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1) # Create a OrdenalEncoder object\n",
    "le=LabelEncoder()                                                       # Create a LabelEncoder object\n",
    "ohe = OneHotEncoder(sparse_output=False, handle_unknown='ignore')       # Create a OneHotEncoder object\n",
    "\n",
    "# Read CSV files\n",
    "\n",
    "df=pd.read_csv(\"Training_Set_Values.csv\")    # Read the CSV file\n",
    "name_featrures=df.columns                    # Get the features name\n",
    "len_features=len(name_featrures)                # Get the length of features\n",
    "labels=pd.read_csv(\"Training_Set_Labels.csv\") # Read the labels CSV file\n",
    "labels.head()\n",
    "df['target'] = labels['status_group']        # Add the target column to the dataframe\n",
    "print(df.shape)                              # Print the shape of the dataframe\n",
    "df.head()\n",
    "df.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label Encodings for target columns\n",
    "print(df['target'].head())\n",
    "print(df['target'].unique())\n",
    "df['target'] = le.fit_transform(df['target'])\n",
    "print(df['target'].head())\n",
    "dict(zip(le.classes_, le.transform(le.classes_))) # Check the mapping of labels to numbers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to see the relationship of longitude and latitude with the target column\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "categories = df['target'].unique()\n",
    "colors = {0: 'green', 1: 'orange', 2: 'red'}\n",
    "\n",
    "for category in categories:\n",
    "    subset = df[df['target'] == category]\n",
    "    plt.scatter(subset['longitude'], subset['latitude'], \n",
    "                c=colors[category], label=category, alpha=0.5, s=10)\n",
    "\n",
    "plt.xlabel('Longitude')\n",
    "plt.ylabel('Latitude')\n",
    "plt.title('Spatial Distribution by Status')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns with Nan values\n",
    "nan_columns = df.columns[df.isnull().any()].tolist()           # Get the columns with NaN values\n",
    "print(\"Columns with NaN values: \", nan_columns)                # Print the columns names with NaN values\n",
    "print(\"Number of columns with NaN values: \", len(nan_columns)) # Print the number of columns with NaN values\n",
    "#df[nan_columns].head()                                         # Print the first 5 rows of the columns with NaN values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploration of NaN Columns\n",
    "for col in nan_columns:                                             # Loop through the columns with NaN values\n",
    "    print(\"--------------------------------------------------\")     # Print a separator line\n",
    "    print(\"Name:\",col)                                              # Print the column name\n",
    "    print(\"Number of NaN:\",df[col].isnull().sum())                  # Print the number of NaN values in the column\n",
    "    print(\"Percentage of NaN:\", df[col].isnull().sum()/len(df)*100) # Print the percentage of NaN values in the column\n",
    "    print(df[col].value_counts())                                   # Print the value counts of the column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column 01 amount_tsh (Static Head)  ## 50 percent values are zero....I think Drop this column\n",
    "print('Nan count in amount_tsh',df[\"amount_tsh\"].isnull().sum()) # Number of NaN values in the column\n",
    "\n",
    "print(df[\"amount_tsh\"].describe())\n",
    "print(\"median: \", df[\"amount_tsh\"].median()) # Median is zero...means half of the points are zero (since points are positive)\n",
    "plt.figure(figsize=(8,5))\n",
    "sns.boxplot(df[\"amount_tsh\"])\n",
    "sns.stripplot(df[\"amount_tsh\"],color=\"red\",alpha=0.5)\n",
    "plt.ylim(0, 100) \n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Remove the outliers from the amount_tsh column\n",
    "Q1=df[\"amount_tsh\"].quantile(0.25) # 25th percentile\n",
    "Q3=df[\"amount_tsh\"].quantile(0.75) # 75th percentile\n",
    "IQR=Q3-Q1 # Interquartile range\n",
    "Lower_bound_amount_tsh=Q1-1.5*IQR # Lower bound\n",
    "Upper_bound_amount_tsh=Q3+1.5*IQR # Upper bound\n",
    "median_amount_tsh=df[\"amount_tsh\"].median() # Median value\n",
    "\n",
    "df[\"amount_tsh_outlier_replaced_median\"]=df[\"amount_tsh\"].apply(lambda x:x if ((x>=Lower_bound_amount_tsh) &(x<=Upper_bound_amount_tsh)) else median_amount_tsh) # Replace outliers with median value\n",
    "\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "sns.boxplot(df[\"amount_tsh\"])\n",
    "sns.stripplot(df[\"amount_tsh_outlier_replaced_median\"], color=\"red\", alpha=0.5)                  # Adds all points\n",
    "plt.title(\"Boxplot of amount_tsh After Outlier Removal With Median\")\n",
    "plt.ylim(0, 100) \n",
    "plt.show() # Show the plot\n",
    "print(df[\"amount_tsh_outlier_replaced_median\"].describe())\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Column 02 date_recorded (Date when the recoed entered)\n",
    "print(\"date_recorded\")\n",
    "print(\"NaN value count:\",df[\"date_recorded\"].isnull().sum()) # Check if there are any null values in the column\n",
    "df['date_recorded'] = pd.to_datetime(df['date_recorded']).dt.year# Convert to datetime format and extract year\n",
    "print(df['date_recorded'].value_counts()) # Print the value counts of the column\n",
    "\n",
    "print('Number of Unique values before outlier removal',df['date_recorded'].nunique())\n",
    "df = df[~df['date_recorded'].isin([2002, 2004])] # Remove the record for years 2002 and 2004 from the dataframe.\n",
    "print('Number of Unique values after outlier removal',df['date_recorded'].nunique())\n",
    "                                             #31 records will be droped\n",
    "\n",
    "\"\"\"\n",
    "df['date_recorded']=oe.fit_transform(df['date_recorded'].values.reshape(-1, 1)) # Fit and transform the column using OrdinalEncoder\n",
    "print('After Transformatiom',df['date_recorded'].value_counts()) # Print the value counts of the column\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column 03 funder # Not Clear how to handle this column\n",
    "print(\"NaN value count in funder:\",df[\"funder\"].isnull().sum()) # Check if there are any null values in the column\n",
    "print('Unique values:',df['funder'].nunique()) # Print the number of unique values in the column\n",
    "print(df['funder'].describe()) # Print the value counts of the column\n",
    "\n",
    "\n",
    "# Get top 10 most common funders\n",
    "top_funders = df['funder'].value_counts().head(10).index\n",
    "\n",
    "# Filter only top funders\n",
    "df_top = df[df['funder'].isin(top_funders)]\n",
    "\n",
    "# Plot count of target classes per funder\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.countplot(x='funder', hue='target', data=df_top)\n",
    "plt.xticks(rotation=45)\n",
    "plt.title('Water Pump Status by Top 10 Funders')\n",
    "plt.xlabel('Funder')\n",
    "plt.ylabel('Count')\n",
    "plt.legend(title='Status', labels=['Functional', 'Needs Repair', 'Nonfunctional'])\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column 04 gps_height (GPS height) # The column needs to be considered as data is ok, minmax normalization is used\n",
    "print(\"--------------------------------\")\n",
    "print(\"gps_height\")\n",
    "print(\"NaN value count:\",df[\"gps_height\"].isnull().sum())                # Check if there are any null values in the column\n",
    "print(\"Percentage of NaN:\", df[\"gps_height\"].isnull().sum()/len(df)*100) # Print the percentage of NaN values in the column\n",
    "print(df[\"gps_height\"].describe())                                       # Print the description of the column\n",
    "print(\"Median:\", df[\"gps_height\"].median())                              # Print the median of the column\n",
    "plt.figure(figsize=(8,5))\n",
    "sns.histplot(df[\"gps_height\"], bins=200, kde=True)                       # Plot the histogram of the column\n",
    "plt.xlabel(\"GPS Height\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Histogram of GPS Height\")\n",
    "plt.show()\n",
    "plt.figure(figsize=(8,5))\n",
    "sns.boxplot(y=df[\"gps_height\"])\n",
    "sns.stripplot(y=df[\"gps_height\"], color=\"red\", alpha=0.5)                # Adds all points\n",
    "plt.title(\"Boxplot of GPS Height Before Normalization\")\n",
    "plt.show() # Show the plot\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "df[\"gps_height\"]=scaler_minmax.fit_transform(df[\"gps_height\"].values.reshape(-1,1)) # Fit and transform the data using MinMaxScaler\n",
    "df[\"gps_height\"] = np.clip(df[\"gps_height\"], 0, 1) # To ensure that the values are between 0 and 1, in case the testdata has values outside the range of training data\n",
    "plt.figure(figsize=(8,5))\n",
    "sns.boxplot(df[\"gps_height\"])\n",
    "sns.stripplot(df[\"gps_height\"], color=\"red\", alpha=0.5)  # Adds all points\n",
    "plt.ylabel(\"GPS Height (MinMax Normalized)\")\n",
    "plt.title(\"Boxplot of GPS Height After Normalization\")\n",
    "plt.show() # Show the plot\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column 05 installer # Not Clear how to handle this column\n",
    "print(\"NaN value count in installer:\",df[\"installer\"].isnull().sum()) # Check if there are any null values in the column\n",
    "print('Unique values:',df['installer'].nunique()) # Print the number of unique values in the column\n",
    "print(df['installer'].describe()) # Print the value counts of the column\n",
    "#print(df['installer'].value_counts()) # Print the value counts of the column\n",
    "pd.crosstab(df['installer'],df['target']).head() # Cross tabulation of installer and status_group\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Get top 10 most common installer\n",
    "top_installer = df['installer'].value_counts().head(10).index\n",
    "\n",
    "# Filter only top installer\n",
    "df_top = df[df['installer'].isin(top_installer)]\n",
    "\n",
    "# Plot count of target classes per installer\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.countplot(x='installer', hue='target', data=df_top)\n",
    "plt.xticks(rotation=45)\n",
    "plt.title('Water Pump Status by Top 10 Installer')\n",
    "plt.xlabel('Installer')\n",
    "plt.ylabel('Count')\n",
    "plt.legend(title='Status', labels=['Functional', 'Needs Repair', 'Nonfunctional'])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "import plotly.express as px\n",
    "\n",
    "# Create a scatter mapbox plot with latitude and longitude\n",
    "fig = px.scatter_mapbox(df, lat=\"latitude\", lon=\"longitude\", \n",
    "                        color=\"target\",  # Optional: Use if you want to color by a target column\n",
    "                        title=\"Pump Locations before oulier removal\",\n",
    "                        hover_data=[\"latitude\", \"longitude\"])  # Optional: Display more info\n",
    "\n",
    "fig.update_layout(mapbox_style=\"open-street-map\")  # Use OpenStreetMap tiles\n",
    "\n",
    "# Use an appropriate renderer for your environment (e.g., 'notebook', 'browser', etc.)\n",
    "fig.show(renderer='browser')\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Columns 06 longitude (GPS Coordinates) # The column needs to be considered as data is ok,outlier replaced with medain and minmax normalization\n",
    "print(\"--------------------------------\")\n",
    "print(\"longitude\")\n",
    "print(\"NaN value count:\",df[\"longitude\"].isnull().sum())                # Check if there are any null values in the column\n",
    "print(df[\"longitude\"].describe())                                                  # Print the description of the column\n",
    "print(\"Median:\", df[\"longitude\"].median())                              # Print the median of the column\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "sns.histplot(df[\"longitude\"], bins=200, kde=True)               # Plot the histogram of the column\n",
    "plt.show()\n",
    "plt.figure(figsize=(8,5))\n",
    "sns.boxplot(df[\"longitude\"])\n",
    "sns.stripplot(df[\"longitude\"], color=\"red\", alpha=0.5)                  # Adds all points\n",
    "plt.title(\"Boxplot of Longitude Before Outlier Removal\")\n",
    "plt.show() # Show the plot\n",
    "\n",
    "# Remove the outliers from the longitude column\n",
    "Q1=df[\"longitude\"].quantile(0.25) # 25th percentile\n",
    "Q3=df[\"longitude\"].quantile(0.75) # 75th percentile\n",
    "IQR=Q3-Q1 # Interquartile range\n",
    "Lower_bound_longitude=Q1-1.5*IQR # Lower bound\n",
    "Upper_bound_longitude=Q3+1.5*IQR # Upper bound\n",
    "median_longitude=df[\"longitude\"].median() # Median value\n",
    "\n",
    "df[\"longitude_outlier_replaced_median\"]=df[\"longitude\"].apply(lambda x:x if ((x>=Lower_bound_longitude) &(x<=Upper_bound_longitude)) else median_longitude) # Replace outliers with median value\n",
    "\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "sns.boxplot(df[\"longitude\"])\n",
    "sns.stripplot(df[\"longitude_outlier_replaced_median\"], color=\"red\", alpha=0.5)                  # Adds all points\n",
    "plt.title(\"Boxplot of Longitude After Outlier Removal With Median\")\n",
    "plt.show() # Show the plot\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df[\"longitude_outlier_replaced_median_minmax_normalized\"]=scaler_minmax.fit_transform(df[\"longitude_outlier_replaced_median\"].values.reshape(-1,1)) # Fit and transform the data using MinMaxScaler\n",
    "df[\"longitude_outlier_replaced_median_minmax_normalized\"] = np.clip(df[\"longitude_outlier_replaced_median_minmax_normalized\"], 0, 1) # To ensure that the values are between 0 and 1, in case the testdata has values outside the range of training data\n",
    "\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "sns.boxplot(df[\"longitude_outlier_replaced_median_minmax_normalized\"])\n",
    "sns.stripplot(df[\"longitude_outlier_replaced_median_minmax_normalized\"], color=\"red\", alpha=0.5)  # Adds all points\n",
    "plt.ylabel(\"longitude (Outlier Replaced by Median and MinMax Normalized)\")\n",
    "plt.title(\"Boxplot of GPS longitude After Outlier Removal and Normalization\")\n",
    "plt.show() # Show the plot\n",
    "\n",
    "\n",
    "df[\"longitude\"] = df[\"longitude_outlier_replaced_median_minmax_normalized\"] # Replace original longitude with processed values\n",
    "\n",
    "df.drop(\"longitude_outlier_replaced_median\", axis=1, inplace=True) # Drop the intermediate column\n",
    "df.drop(\"longitude_outlier_replaced_median_minmax_normalized\", axis=1, inplace=True) # Drop the intermediate column\n",
    "\n",
    "\n",
    "df.columns\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Columns 07 latitude (GPS Coordinates) # The column needs to be considered as data is ok\n",
    "print(\"--------------------------------\")\n",
    "print(\"latitude\")\n",
    "print(\"NaN value count:\",df[\"latitude\"].isnull().sum())                # Check if there are any null values in the column\n",
    "print(df[\"latitude\"].describe())                                                  # Print the description of the column\n",
    "print(\"Median:\", df[\"latitude\"].median())                              # Print the median of the column\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "sns.histplot(df[\"latitude\"], bins=200, kde=True)               # Plot the histogram of the column\n",
    "plt.show()\n",
    "plt.figure(figsize=(8,5))\n",
    "sns.boxplot(df[\"latitude\"])\n",
    "sns.stripplot(df[\"latitude\"], color=\"red\", alpha=0.5)                  # Adds all points\n",
    "plt.title(\"Boxplot of latitude\")\n",
    "plt.show() # Show the plot\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df[\"latitude_minmax_normalized\"]=scaler_minmax.fit_transform(df[\"latitude\"].values.reshape(-1,1)) # Fit and transform the data using MinMaxScaler\n",
    "df[\"latitude_minmax_normalized\"] = np.clip(df[\"latitude_minmax_normalized\"], 0, 1) # To ensure that the values are between 0 and 1, in case the testdata has values outside the range of training data\n",
    "\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "sns.boxplot(df[\"latitude_minmax_normalized\"])\n",
    "sns.stripplot(df[\"latitude_minmax_normalized\"], color=\"red\", alpha=0.5)  # Adds all points\n",
    "plt.ylabel(\"Latitude MinMax Normalized)\")\n",
    "plt.title(\"Boxplot of Latitude After Normalization\")\n",
    "plt.show() # Show the plot\n",
    "\n",
    "\n",
    "df[\"latitude\"] = df[\"latitude_minmax_normalized\"] # Replace original longitude with processed values\n",
    "df.drop(\"latitude_minmax_normalized\", axis=1, inplace=True) # Drop the intermediate column\n",
    "df.columns\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_greater_than_minus_one = df[df['latitude'] > -1].shape[0]\n",
    "print(count_greater_than_minus_one)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "import plotly.express as px\n",
    "\n",
    "# Create a scatter mapbox plot with latitude and longitude\n",
    "fig = px.scatter_mapbox(df, lat=\"latitude\", lon=\"longitude\", \n",
    "                        color=\"target\",  # Optional: Use if you want to color by a target column\n",
    "                        title=\"Pump Locations\",\n",
    "                        hover_data=[\"latitude\", \"longitude\"])  # Optional: Display more info\n",
    "\n",
    "fig.update_layout(mapbox_style=\"open-street-map\")  # Use OpenStreetMap tiles\n",
    "\n",
    "# Use an appropriate renderer for your environment (e.g., 'notebook', 'browser', etc.)\n",
    "fig.show(renderer='browser')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column 08 wpt_name (Waterpoint Name)  # Not Clear how to handle this column\n",
    "print(\"NaN value count in wpt_name:\",df[\"wpt_name\"].isnull().sum()) # Check if there are any null values in the column\n",
    "print('Number of unique values',df['wpt_name'].nunique()) # Print the number of unique values the column\n",
    "\n",
    "df['wpt_name'].value_counts() # Print the value counts of the column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column 09 num_private # Since 75% values are zero better to drop this column\n",
    "print(\"--------------------------------\")\n",
    "print(df[\"num_private\"].describe()) # Print the description of the column\n",
    "df[\"num_private\"].isnull().sum() # Check if there are any null values in the column\n",
    "plt.figure(figsize=(8,5))\n",
    "sns.histplot(df[\"num_private\"], bins=200,kde=True) # Plot the histogram of the column\n",
    "plt.title(\"Histogram of num_private\")\n",
    "plt.xlabel(\"num_private\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.scatter(df.index, df['num_private'], color='blue', label='Values')\n",
    "plt.xlabel('Index')\n",
    "plt.ylabel('Values')\n",
    "plt.title(\"num_private vs Index\")\n",
    "plt.show()\n",
    "plt.figure(figsize=(8,5))\n",
    "sns.boxplot(df[\"num_private\"])\n",
    "#sns.stripplot(df[\"num_private\"], color=\"red\", alpha=0.5)                  # Adds all points\n",
    "plt.title(\"Boxplot of num_private\")\n",
    "# Labels and title\n",
    "plt.xlabel('Index')\n",
    "plt.ylabel('Values')\n",
    "plt.title('Values vs Index')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"NaN value count:\",df[\"basin\"].isnull().sum())                # Check if there are any null values in the column\n",
    "print(df['basin'].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Column 10 basin # The column needs to be considered as data is ok..I used One Hot Encoding for this\n",
    "print(\"NaN value count:\",df[\"basin\"].isnull().sum())                # Check if there are any null values in the column\n",
    "df[\"basin\"].describe() # Print the value counts of the column\n",
    "print(df.basin.value_counts()) # Print the value counts of the column\n",
    "print(df[\"basin\"].head())\n",
    "encoded_basin = ohe.fit_transform(df[['basin']])\n",
    "encoded_basin_df = pd.DataFrame(encoded_basin, columns=ohe.get_feature_names_out(['basin']), index=df.index)  \n",
    "\n",
    "print(encoded_basin_df.head(5)) # Print the first 5 rows of the encoded dataframe\n",
    "df = pd.concat([df, encoded_basin_df], axis=1) # Concatenate the original dataframe with the encoded dataframe\n",
    "df.drop(columns=['basin'], inplace=True) # Drop the original column\n",
    "df.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column 11 subvillage # Since region code and district code gives same info \n",
    "                    # so ignore this because it has NaN and string to number \n",
    "                    # conversion is needed \n",
    "print(\"NaN count:\",df[\"subvillage\"].isnull().sum()) # Check if there are any null values in the column\n",
    "print(df[\"subvillage\"].describe()) # Print the value counts of the column\n",
    "df[\"subvillage\"].value_counts() # Print the value counts of the column\n",
    "df[\"subvillage\"].head()\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "df['subvillage'] = df['subvillage'].fillna('Unknown')\n",
    "encoded_subvillage = ohe.fit_transform(df[['subvillage']])\n",
    "encoded_subvillage_df = pd.DataFrame(encoded_subvillage, columns=ohe.get_feature_names_out(['subvillage']), index=df.index)  \n",
    "\n",
    "print(encoded_subvillage_df.head(5)) # Print the first 5 rows of the encoded dataframe\n",
    "df = pd.concat([df, encoded_subvillage_df], axis=1) # Concatenate the original dataframe with the encoded dataframe\n",
    "#df.drop(columns=['region'], inplace=True) # Drop the original column\n",
    "#df.head()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pandas as pd\n",
    "#df=pd.read_csv(\"Training_Set_Values.csv\")    # Read the CSV file\n",
    "print(df[pd.isna(df[\"region\"])][['subvillage','region','region_code','district_code','lga','ward']])# Print the first 10 rows of the dataframe where region is Kigoma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column 12 region # Since region code and district code gives same info \n",
    "                    # so ignore this because string to number \n",
    "                    # conversion is needed\n",
    "print(\"NaN count:\", df[\"region\"].isnull().sum())           # Check if there are any null values in the column\n",
    "print(df[\"region\"].describe())                             # Print the description of the column\n",
    "df[\"region\"].value_counts()                                # Print the value counts of the column\n",
    "\n",
    "print(df['region'].nunique())\n",
    "\n",
    "\n",
    "\n",
    "encoded_region = ohe.fit_transform(df[['region']])\n",
    "encoded_region_df = pd.DataFrame(encoded_region, columns=ohe.get_feature_names_out(['region']), index=df.index)  \n",
    "\n",
    "print(encoded_region_df.head(5)) # Print the first 5 rows of the encoded dataframe\n",
    "df = pd.concat([df, encoded_region_df], axis=1) # Concatenate the original dataframe with the encoded dataframe\n",
    "df.drop(columns=['region'], inplace=True) # Drop the original column\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Column 13 region_code # Since it has 27 unique codes i chose to use one hot encoding.\n",
    "                    \n",
    "print(\"NaN count\",df[\"region_code\"].isnull().sum()) # Check if there are any null values in the column\n",
    "print(\"Unique values:\",df[\"region_code\"].nunique()) # Print the distinct values of the column\n",
    "print(df[\"region_code\"].describe()) # Print the value counts of the column\n",
    "df[\"region_code\"].value_counts() # Print the first 5 rows of the column\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "encoded_region_code = ohe.fit_transform(df[['region_code']])\n",
    "encoded_region_code_df = pd.DataFrame(encoded_region_code, columns=ohe.get_feature_names_out(['region_code']), index=df.index)  \n",
    "\n",
    "print(encoded_region_code_df.head(5)) # Print the first 5 rows of the encoded dataframe\n",
    "df = pd.concat([df, encoded_region_code_df], axis=1) # Concatenate the original dataframe with the encoded dataframe\n",
    "df.drop(columns=['region_code'], inplace=True) # Drop the original column\n",
    "df.head()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column 14 district_code # Since has 20 unique codes i chose to use one hot encoding.\n",
    "                    \n",
    "print(\"NaN count\",df[\"district_code\"].isnull().sum()) # Check if there are any null values in the column\n",
    "print(\"Unique values:\",df[\"district_code\"].nunique()) # Print the distinct values of the column\n",
    "\n",
    "print(df[\"district_code\"].describe()) # Print the value counts of the column\n",
    "print(\"code vs freq\",df[\"district_code\"].value_counts()) # Print the value counts of the column\n",
    "df[\"district_code\"].head() # Print the first 5 rows of the column\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "encoded_district_code = ohe.fit_transform(df[['district_code']])\n",
    "encoded_district_code_df = pd.DataFrame(encoded_district_code, columns=ohe.get_feature_names_out(['district_code']), index=df.index)  \n",
    "\n",
    "print(encoded_district_code_df.head(5)) # Print the first 5 rows of the encoded dataframe\n",
    "df = pd.concat([df, encoded_district_code_df], axis=1) # Concatenate the original dataframe with the encoded dataframe\n",
    "df.drop(columns=['district_code'], inplace=True) # Drop the original column\n",
    "df.head()\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column 15 lga # No NaN, 125 unique values\n",
    "print(\"NaN count:\",df[\"lga\"].isnull().sum())# Print the value counts of the column\n",
    "print(\"Unique values:\",df[\"lga\"].nunique())# Print the value counts of the column\n",
    "print(df[\"lga\"].describe()) # Print the description of the column\n",
    "print(df[\"lga\"].head()) # Print the description of the column\n",
    "#freq_encoding=df[\"lga\"].value_counts(normalize=True) # Frequency Encoding for lga column\n",
    "#df[\"lga\"]=df[\"lga\"].map(freq_encoding) # Map the frequencies to the original column\n",
    "df[\"lga\"].head()\n",
    "\n",
    "\n",
    "encoded_lga = ohe.fit_transform(df[['lga']])\n",
    "encoded_lga_df = pd.DataFrame(encoded_lga, columns=ohe.get_feature_names_out(['lga']), index=df.index)  \n",
    "\n",
    "print(encoded_lga_df.head(5)) # Print the first 5 rows of the encoded dataframe\n",
    "df = pd.concat([df, encoded_lga_df], axis=1) # Concatenate the original dataframe with the encoded dataframe\n",
    "df.drop(columns=['lga'], inplace=True) # Drop the original column\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column 16 ward # No NaN, 2092 unique values, used normalized frequency encoding\n",
    "print(\"NaN count:\",df[\"ward\"].isnull().sum())# Print the value counts of the column\n",
    "print(\"Unique values:\",df[\"ward\"].nunique())# Print the value counts of the column\n",
    "print(df[\"ward\"].describe()) # Print the description of the column\n",
    "#print(df[\"ward\"].head()) # Print the description of the column\n",
    "#freq_encoding=df[\"ward\"].value_counts(normalize=True) # Frequency Encoding for ward column\n",
    "#df[\"ward_freq\"]=df[\"ward\"].map(freq_encoding) # Map the frequencies to the original column\n",
    "df[\"ward\"].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column 17 population\n",
    "print('NaN count:',df['population'].isnull().sum()) # Check if there are any null values in the column\n",
    "#print('Unique values',df['population'].value_counts()) # Print the value counts of the column\n",
    "print('Median',df['population'].median()) # Print the median of the column\n",
    "print(df['population'].describe())\n",
    "plt.figure(figsize=(8,5))\n",
    "sns.histplot(df[\"population\"], bins=200, kde=True) # Plot the histogram of the column\n",
    "plt.figure(figsize=(8,5))\n",
    "sns.boxplot(df[\"population\"])\n",
    "sns.stripplot(df[\"population\"], color=\"red\", alpha=0.5)                  # Adds all points\n",
    "plt.ylim(-100,3000)\n",
    "plt.title(\"Boxplot of Population Before Outlier Removal\")\n",
    "plt.show() # Show the plot\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.scatter(df.index, df['population'], color='blue', label='Values')\n",
    "plt.show() # Show the plot\n",
    "\n",
    "# Remove the outliers from the population column\n",
    "Q1=df[\"population\"].quantile(0.25) # 25th percentile\n",
    "Q3=df[\"population\"].quantile(0.75) # 75th percentile\n",
    "IQR=Q3-Q1 # Interquartile range\n",
    "Lower_bound_population=Q1-1.5*IQR # Lower bound\n",
    "Upper_bound_population=Q3+1.5*IQR # Upper bound\n",
    "mean_population=df[\"population\"].mean() # Mean value\n",
    "\n",
    "df[\"population_outlier_replaced_mean\"]=df[\"population\"].apply(lambda x:x if ((x>=Lower_bound_population) & (x<=Upper_bound_population)) else mean_population) # Replace outliers with median value\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "sns.boxplot(y=df[\"population\"])\n",
    "sns.stripplot(y=df[\"population_outlier_replaced_mean\"], color=\"red\", alpha=0.5)\n",
    "plt.ylim(-100,3000)\n",
    "plt.title(\"Boxplot of Population After Outlier Removal With mean\")\n",
    "plt.show() # Show the plot\n",
    "\n",
    "df[\"population\"] = df[\"population_outlier_replaced_mean\"] # Replace original population with processed values\n",
    "df.drop(\"population_outlier_replaced_mean\", axis=1, inplace=True) # Drop the intermediate column\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "print('Nan count in population_outlier_replaced_NaN:',df['population_outlier_replaced_NaN'].isnull().sum()) # Check if there are any null values in the column\n",
    "df['population_outlier_replaced_mean'] = df.groupby('ward')['population_outlier_replaced_NaN'].transform(\n",
    "    lambda x: x.fillna(x.mean())\n",
    ")\n",
    "print(\"NaN count after imputaion: \",df['population_outlier_replaced_mean'].isnull().sum()) # Check if there are any null values in the column\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Column 18 public_meeting # 3328 NaN values replaced with string Unknown, One Hot Encoding used\n",
    "print(df['public_meeting'].value_counts())                   # Print the value counts of the column\n",
    "print('Nan count',df['public_meeting'].isnull().sum())                         # Check if there are any null values in the column)\n",
    "df['public_meeting']=df['public_meeting'].fillna('Unknown')  # Fill NaN values with 'Unknown'\n",
    "print(df['public_meeting'].value_counts())                   # Print the value counts of the column\n",
    "\n",
    "\n",
    "df['public_meeting']=df['public_meeting'].astype(str)\n",
    "encoded_public_meeting = ohe.fit_transform(df[['public_meeting']])\n",
    "encoded_public_meeting_df = pd.DataFrame(encoded_public_meeting, columns=ohe.get_feature_names_out(['public_meeting']), index=df.index)  \n",
    "\n",
    "print(encoded_public_meeting_df.head(5)) # Print the first 5 rows of the encoded dataframe\n",
    "df = pd.concat([df, encoded_public_meeting_df], axis=1) # Concatenate the original dataframe with the encoded dataframe\n",
    "df.drop(columns=['public_meeting'], inplace=True) # Drop the original column\n",
    "df.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column 19 recorded_by # All values are \"GeoData Consultants Ltd\" ignore this column\n",
    "print('Nan count:',df['recorded_by'].isnull().sum()) # Check if there are any null values in the column\n",
    "df['recorded_by'].describe() # Print the value counts of the column\n",
    "print('Unique values:',df['recorded_by'].nunique()) # Print the value counts of the column\n",
    "print(df['recorded_by'].describe())\n",
    "df.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column 20 scheme_management # could not decide about this column..drop/keep\n",
    "print('Nan count',df['scheme_management'].isnull().sum())\n",
    "print('Unique values',df['scheme_name'].nunique())\n",
    "print(df['scheme_management'].value_counts())\n",
    "print('Unique values',df['scheme_management'].nunique())\n",
    "\n",
    "#pd.crosstab(df['scheme_name'],df['scheme_management'])\n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "print(df[df['scheme_management'].isna()][['scheme_management','scheme_name']]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Column 21 scheme_name # Nan count is 28790..almost 50 percemt so ignore the column\n",
    "print('Nan count:',df['scheme_name'].isnull().sum()) # Check if there are any null values in the column\n",
    "df['scheme_name'].describe() # Print the value counts of the column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column 22 permit # NaN replaced with unknown and 1 hot encdoing is used\n",
    "print('Nan count:',df['permit'].isnull().sum()) # Check if there are any null values in the column\n",
    "print(df['permit'].describe())\n",
    "print('value count:',df['permit'].value_counts())\n",
    "df['permit']=df['permit'].fillna('Unknown')\n",
    "print('Nan count:',df['permit'].isnull().sum()) # Check if there are any null values in the column\n",
    "print('value count:',df['permit'].value_counts())\n",
    "\n",
    "\n",
    "df['permit']=df['permit'].astype(str)\n",
    "encoded_permit = ohe.fit_transform(df[['permit']])\n",
    "encoded_permit_df = pd.DataFrame(encoded_permit, columns=ohe.get_feature_names_out(['permit']), index=df.index)  \n",
    "\n",
    "print(encoded_permit_df.head(5)) # Print the first 5 rows of the encoded dataframe\n",
    "df = pd.concat([df, encoded_permit_df], axis=1) # Concatenate the original dataframe with the encoded dataframe\n",
    "df.drop(columns=['permit'], inplace=True) # Drop the original column\n",
    "df.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Column 23 construction_year # 1/3 values are zero.replaced with median of nonzero values\n",
    "print('Nan count:',df['construction_year'].isnull().sum()) # Check if there are any null values in the column\n",
    "print(df['construction_year'].value_counts().head(6)) # Print the value counts of the column\n",
    "print('unique values:',df['construction_year'].unique()) # Print the value counts of the column\n",
    "#pd.crosstab(df['construction_year'],df['target'])\n",
    "\n",
    "nonzero_median = df.loc[df['construction_year'] != 0, 'construction_year'].median()\n",
    "print('Nonzero median',nonzero_median)\n",
    "\n",
    "# Step 2: Replace 0s with that median\n",
    "df['construction_year'] = df['construction_year'].replace(0, nonzero_median)\n",
    "print(df['construction_year'].value_counts().head(6)) # Print the value counts of the column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns [extraction_type, extraction_type_group, extraction_type_class]\n",
    "# Based on anlysis, Extraction type is the deatiled version of other two columns \n",
    "# extraction type with 18 caregories\n",
    "# extraction_type_group with 13 caregories\n",
    "# extraction_type_classwith 7 caregories\n",
    "\n",
    "# I decided to drop two columns [extraction_type_group, extraction_type_class]\n",
    "# use column ['extraction_type']\n",
    "# Since 'extraction_type' has 0 NaN values and 18 unique values, use one hot encoding\n",
    "\n",
    "print('--------------------------------------------------------')\n",
    "print('Nan count',df['extraction_type'].isnull().sum()) # Check if there are any null values in the column\n",
    "print('Unique values',df['extraction_type'].nunique()) # Check if there are any null values in the column\n",
    "print('--------------------------------------------------------')\n",
    "\n",
    "\n",
    "encoded_extraction_type = ohe.fit_transform(df[['extraction_type']])\n",
    "encoded_extraction_type_df = pd.DataFrame(encoded_extraction_type, columns=ohe.get_feature_names_out(['extraction_type']), index=df.index)  \n",
    "\n",
    "print(encoded_extraction_type_df.head(5)) # Print the first 5 rows of the encoded dataframe\n",
    "df = pd.concat([df, encoded_extraction_type_df], axis=1) # Concatenate the original dataframe with the encoded dataframe\n",
    "df.drop(columns=['extraction_type'], inplace=True) # Drop the original column\n",
    "df.head()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column 27, 28 ['management', 'management_group'] # No NaN Values.\n",
    "# 12 unique values in management\n",
    "# 5 unique values in management_group\n",
    "# managment is the detailed version of management_group\n",
    "# I decided to drop the column management_group and use management column only\n",
    "# Used one hot encoding for both columns\n",
    "                                           \n",
    "print('--------------------------------------------------------')\n",
    "print('Nan count in management column',df['management'].isnull().sum())                         # Check if there are any null values in the column\n",
    "print('value_counts in management column',df['management'].value_counts())                      # Print the value counts of the column\n",
    "print('Unique values in management column',df['management'].nunique())                          # Print the number of unique values in the column\n",
    "print('--------------------------------------------------------')\n",
    "\n",
    "encoded_management = ohe.fit_transform(df[['management']])\n",
    "encoded_management_df = pd.DataFrame(encoded_management, columns=ohe.get_feature_names_out(['management']), index=df.index)  \n",
    "\n",
    "print(encoded_management_df.head(5)) # Print the first 5 rows of the encoded dataframe\n",
    "df = pd.concat([df, encoded_management_df], axis=1) # Concatenate the original dataframe with the encoded dataframe\n",
    "df.drop(columns=['management'], inplace=True) # Drop the original column\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Columns 29, 30 ['payment', 'payment_type']\n",
    "# No Nan values and 07 Unique categories in both columns\n",
    "# Semantically same values in both columns\n",
    "# After mapping(staderdization) choose one column i.e payment,\n",
    "# One hot encoding is used for payment column\n",
    "print(df['payment'].value_counts()) #\n",
    "print('--------------------------------------------------------')\n",
    "print(df['payment_type'].value_counts()) # Check if there are any null values in the column\n",
    "\n",
    "# The values in the two columns semantically are almost same just differet wording\n",
    "\n",
    "encoded_payment = ohe.fit_transform(df[['payment']])\n",
    "encoded_payment_df = pd.DataFrame(encoded_payment, columns=ohe.get_feature_names_out(['payment']), index=df.index)  \n",
    "\n",
    "print(encoded_payment_df.head(5)) # Print the first 5 rows of the encoded dataframe\n",
    "df = pd.concat([df, encoded_payment_df], axis=1) # Concatenate the original dataframe with the encoded dataframe\n",
    "df.drop(columns=['payment'], inplace=True) # Drop the original column\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns 31, 32 ['water_quality', 'quality_group']\n",
    "# Based on the value_counts water_quality is the detailed version of quality_group\n",
    "# No NaN values in both columns\n",
    "# 8 categories in water_quality and 6 categories in quality_group\n",
    "# I chose one hot encoding for water quality and decided to drop quality_group column\n",
    "#print(df['water_quality'].head())\n",
    "print('--------------------------------------------------------')\n",
    "print(df['water_quality'].value_counts()) # Check if there are any null values in the column\n",
    "print('Nan count in water quality:',df['water_quality'].isnull().sum()) # Check if there are any null values in the column\n",
    "print('--------------------------------------------------------')\n",
    "\n",
    "encoded_water_quality = ohe.fit_transform(df[['water_quality']])\n",
    "encoded_water_quality_df = pd.DataFrame(encoded_water_quality, columns=ohe.get_feature_names_out(['water_quality']), index=df.index)  \n",
    "\n",
    "print(encoded_water_quality_df.head(5)) # Print the first 5 rows of the encoded dataframe\n",
    "df = pd.concat([df, encoded_water_quality_df], axis=1) # Concatenate the original dataframe with the encoded dataframe\n",
    "df.drop(columns=['water_quality'], inplace=True) # Drop the original column\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns 33,34 ['quantity', 'quantity_group'] are same\n",
    "# No Nan Values in both columns\n",
    "# 5 categories in both columns\n",
    "# I chose to keep quantity column and drop quantity_group column\n",
    "# 1 hot encoding is used for quantity column\n",
    "\n",
    "print(df['quantity'].head()) \n",
    "print(df['quantity'].value_counts()) # Check if there are any null values in the column\n",
    "print('Nan count in quantity',df['quantity'].isnull().sum()) # Check if there are any null values in the column\n",
    "\n",
    "print('---------------------------------------------------------')\n",
    "\n",
    "\n",
    "encoded_quantity = ohe.fit_transform(df[['quantity']])\n",
    "encoded_quantity_df = pd.DataFrame(encoded_quantity, columns=ohe.get_feature_names_out(['quantity']), index=df.index)  \n",
    "\n",
    "print(encoded_quantity_df.head(5)) # Print the first 5 rows of the encoded dataframe\n",
    "df = pd.concat([df, encoded_quantity_df], axis=1) # Concatenate the original dataframe with the encoded dataframe\n",
    "df.drop(columns=['quantity'], inplace=True) # Drop the original column\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns 35, 36 ['source', 'source_type']\n",
    "# No NaN values in both columns\n",
    "# 'source' is the detailed version of 'source_type'\n",
    "# 10 categories in source and 7 categories in source_type\n",
    "# I chose to keep source column and drop source_type column\n",
    "# 1 hot encoding is used for source column\n",
    "print('---------------------------------------------------------')\n",
    "print(df['source'].value_counts()) # Check if there are any null values in the column\n",
    "print('Nan count in source',df['source'].isnull().sum()) # Check if there are any null values in the column\n",
    "print('---------------------------------------------------------')\n",
    "\n",
    "\n",
    "encoded_source = ohe.fit_transform(df[['source']])\n",
    "encoded_source_df = pd.DataFrame(encoded_source, columns=ohe.get_feature_names_out(['source']), index=df.index)  \n",
    "\n",
    "print(encoded_source_df.head(5)) # Print the first 5 rows of the encoded dataframe\n",
    "df = pd.concat([df, encoded_source_df], axis=1) # Concatenate the original dataframe with the encoded dataframe\n",
    "df.drop(columns=['source'], inplace=True) # Drop the original column\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column 37 ['source_class']\n",
    "# No Nan values in the column\n",
    "# 3 categories in the column\n",
    "# 1 hot encoding is used for source_class column\n",
    "print(df['source_class'].head()) # Print the first 5 rows of the column\n",
    "print('---------------------------------------------------------')\n",
    "#print(pd.crosstab(df['source_class'],df['source_type'])) # Print the cross tabulation of source and source_type columns\n",
    "print(df['source_class'].value_counts()) # Check if there are any null values in the column\n",
    "print('Nan count in source class',df['source_class'].isnull().sum()) # Check if there are any null values in the column\n",
    "print('---------------------------------------------------------')\n",
    "\n",
    "\n",
    "encoded_source_class = ohe.fit_transform(df[['source_class']])\n",
    "encoded_source_class_df = pd.DataFrame(encoded_source_class, columns=ohe.get_feature_names_out(['source_class']), index=df.index)  \n",
    "\n",
    "print(encoded_source_class_df.head(5)) # Print the first 5 rows of the encoded dataframe\n",
    "df = pd.concat([df, encoded_source_class_df], axis=1) # Concatenate the original dataframe with the encoded dataframe\n",
    "df.drop(columns=['source_class'], inplace=True) # Drop the original column\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns 38, 39 ['waterpoint_type', 'waterpoint_type_group'] have same info\n",
    "# 'waterpoint_type' is the detailed version of 'waterpoint_type_group'\n",
    "# No NaN values in both columns\n",
    "# 7 categories in waterpoint_type and 6 categories in waterpoint_type_group\n",
    "# I chose to keep waterpoint_type column and drop waterpoint_type_group column\n",
    "# 1 hot encoding is used for waterpoint_type column\n",
    "print(df['waterpoint_type'].head()) \n",
    "print('---------------------------------------------------------')\n",
    "print(df['waterpoint_type'].value_counts()) # Check if there are any null values in the column\n",
    "print('Nan count in waterpoint type',df['waterpoint_type'].isnull().sum()) # Check if there are any null values in the column\n",
    "\n",
    "print('---------------------------------------------------------')\n",
    "\n",
    "\n",
    "encoded_waterpoint_type = ohe.fit_transform(df[['waterpoint_type']])\n",
    "encoded_waterpoint_type_df = pd.DataFrame(encoded_waterpoint_type, columns=ohe.get_feature_names_out(['waterpoint_type']), index=df.index)  \n",
    "\n",
    "print(encoded_waterpoint_type_df.head(5)) # Print the first 5 rows of the encoded dataframe\n",
    "df = pd.concat([df, encoded_waterpoint_type_df], axis=1) # Concatenate the original dataframe with the encoded dataframe\n",
    "df.drop(columns=['waterpoint_type'], inplace=True) # Drop the original column\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Shape of original dataframe:', df.shape)\n",
    "\"\"\"\n",
    "columns_drop=['amount_tsh','num_private','subvillage','region','recorded_by','scheme_name',\n",
    "              'extraction_type_group','extraction_type_class',\n",
    "              'management_group','payment_type','quality_group','quantity_group','source_type','waterpoint_type_group',\n",
    "              'funder','installer','wpt_name','lga','ward','population','scheme_management']\n",
    "              \n",
    "'waterpoint_type_hand pump','management_vwc','date_recorded','waterpoint_type_communal standpipe multiple',\n",
    "              'extraction_type_gravity','payment_pay per bucket','water_quality_soft','district_code_1'\n",
    "\"\"\"\n",
    "\n",
    "columns_drop=['id','amount_tsh','num_private','subvillage','recorded_by','scheme_name',\n",
    "              'extraction_type_group','extraction_type_class',\n",
    "              'management_group','payment_type','quality_group','quantity_group','source_type','waterpoint_type_group',\n",
    "              'funder','installer','wpt_name','ward','scheme_management']\n",
    "df = df.drop(columns=columns_drop)\n",
    "print('Shape after dropping columns:', df.shape)\n",
    "#df = df.drop(columns=['region_code_13', 'region_code_14', 'management_water authority', 'region_code_21', 'district_code_8', 'region_code_4', 'region_code_2', 'region_code_18', 'quantity_unknown', 'region_code_20', 'region_code_1', 'region_code_19', 'waterpoint_type_improved spring', 'management_wua', 'region_code_12', 'region_code_5', 'source_lake', 'management_parastatal', 'region_code_3', 'management_company', 'management_water board', 'permit_Unknown', 'region_code_16', 'extraction_type_afridev', 'basin_Ruvuma / Southern Coast', 'extraction_type_india mark ii', 'management_private operator', 'basin_Wami / Ruvu', 'basin_Pangani', 'extraction_type_ksb', 'region_code_17', 'district_code_7', 'source_rainwater harvesting', 'basin_Lake Nyasa', 'basin_Rufiji', 'district_code_6', 'public_meeting_Unknown', 'basin_Lake Tanganyika', 'payment_pay when scheme fails', 'basin_Lake Rukwa', 'extraction_type_mono', 'basin_Lake Victoria', 'district_code_5', 'extraction_type_swn 80', 'extraction_type_submersible', 'source_river', 'management_wug', 'payment_pay annually', 'water_quality_salty', 'basin_Internal', 'source_class_surface', 'region_code_11', 'source_class_groundwater', 'public_meeting_False', 'district_code_4', 'source_machine dbh', 'payment_pay monthly', 'public_meeting_True', 'source_shallow well', 'payment_unknown', 'district_code_2', 'quantity_seasonal', 'district_code_3', 'water_quality_unknown', 'source_spring', 'permit_False', 'extraction_type_nira/tanira', 'permit_True', 'district_code_1', 'payment_pay per bucket', 'water_quality_soft', 'extraction_type_gravity', 'date_recorded', 'waterpoint_type_communal standpipe multiple', 'management_vwc', 'waterpoint_type_hand pump', 'waterpoint_type_communal standpipe', 'quantity_insufficient', 'payment_never pay', 'extraction_type_other', 'quantity_enough', 'waterpoint_type_other', 'population', 'construction_year', 'gps_height', 'quantity_dry', 'id', 'latitude', 'longitude']\n",
    "#)\n",
    "\n",
    "print('Shape after additional dropping columns:', df.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Drop target and non-numeric columns if any\n",
    "X = df.drop(columns=['target'])  # replace 'target' with actual target column name\n",
    "y = df['target']\n",
    "\n",
    "# Scale\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Keep enough components to retain 95% of variance\n",
    "pca = PCA(n_components=0.95)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "print(f\"Original shape: {X.shape}, After PCA: {X_pca.shape}\")\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_pca, y, test_size=0.1, random_state=42)\n",
    "\n",
    "# Train XGBoost\n",
    "model = XGBClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=11,\n",
    "    learning_rate=0.1,\n",
    "    subsample=0.9,\n",
    "    colsample_bytree=0.8,\n",
    "    use_label_encoder=False,\n",
    "    eval_metric='mlogloss',   # good for multi-class\n",
    "    objective='multi:softmax', # directly outputs class labels\n",
    "    num_class=3,               # number of target classes\n",
    "    random_state=42)\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "train_acc = accuracy_score(y_train, model.predict(X_train))\n",
    "test_acc = accuracy_score(y_test, model.predict(X_test))\n",
    "\n",
    "print(f\"Train Accuracy: {train_acc * 100:.2f}%\")\n",
    "print(f\"Test Accuracy: {test_acc * 100:.2f}%\")\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from xgboost import XGBClassifier\n",
    "# 1. Automatically detect target column (assumed name: 'traget_encoded')\n",
    "X = df.drop(columns=['target'])  # Features\n",
    "y = df['target']                 # Target (already label encoded)\n",
    "\n",
    "# 2. Train/test split (test size = 5000 rows)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.08, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# 3. Train Random Forest Classifier\n",
    "model = RandomForestClassifier(random_state=42)\n",
    "\n",
    "\n",
    "model = XGBClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=11,\n",
    "    learning_rate=0.1,\n",
    "    subsample=0.9,\n",
    "    colsample_bytree=0.8,\n",
    "    use_label_encoder=False,\n",
    "    eval_metric='mlogloss',   # good for multi-class\n",
    "    objective='multi:softmax', # directly outputs class labels\n",
    "    num_class=3,               # number of target classes\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# 4. Predictions\n",
    "y_train_pred = model.predict(X_train)\n",
    "y_test_pred = model.predict(X_test)\n",
    "\n",
    "# 5. Accuracy scores\n",
    "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "# 6. Print results\n",
    "print(f\" Training Accuracy: {train_accuracy:.4f}\")\n",
    "print(f\" Test Accuracy:     {test_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Training Accuracy: 0.8737\n",
    " Test Accuracy:     0.8149"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importances\n",
    "importances = model.feature_importances_\n",
    "feature_names = X.columns\n",
    "\n",
    "# Create a DataFrame\n",
    "importance_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Importance': importances\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Display  important features\n",
    "print(\" Important Features:\")\n",
    "print(importance_df.head())\n",
    "\n",
    "# Optional: Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(importance_df['Feature'][:10][::-1], importance_df['Importance'][:10][::-1])\n",
    "plt.xlabel(\"Feature Importance\")\n",
    "plt.title(\"Feature importance in Random Forest\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importance_df.sort_values(by='Importance').head(No_of_col_to_drop)\n",
    "\n",
    "importances = model.feature_importances_\n",
    "feature_names = X.columns\n",
    "len(importances),len(importance_df['Importance']),len(importance_df['Feature'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importance_df.sort_values(by='Importance', ascending=False)[['Feature', 'Importance']]\n",
    "importance_df['Feature'].head(20).tolist()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
