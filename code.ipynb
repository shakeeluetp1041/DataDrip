{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import sklearn\n",
    "from sklearn.preprocessing import  MinMaxScaler,OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "scaler_minmax= MinMaxScaler()                                                             # Create a MinMaxScaler object\n",
    "ohe = OneHotEncoder(sparse_output=False, handle_unknown='ignore',drop='first')            # Create a OneHotEncoder object\n",
    "\n",
    "\n",
    "# Read CSV files\n",
    "\n",
    "df=pd.read_csv(\"Training_Set_Values.csv\")       # Read the Training data CSV file\n",
    "name_featrures=df.columns                       # Get the features name\n",
    "len_features=len(name_featrures)                # Get the length of features\n",
    "labels=pd.read_csv(\"Training_Set_Labels.csv\")   # Read the labels (target) CSV file\n",
    "labels.head()\n",
    "df['target'] = labels['status_group']           # Add the target column to the dataframe\n",
    "#print(df.shape)                                 # Print the shape of the dataframe\n",
    "#df.head()\n",
    "#df.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape after dropping columns: (59400, 22)\n"
     ]
    }
   ],
   "source": [
    "# Columns to be dropped for the baseline models\n",
    "columns_drop=['id','amount_tsh','num_private','subvillage','recorded_by','scheme_name',\n",
    "              'extraction_type_group','extraction_type_class',\n",
    "              'management','payment_type','quality_group','quantity_group','source','waterpoint_type_group',\n",
    "              'funder','installer','wpt_name','ward','scheme_management']\n",
    "\n",
    "df = df.drop(columns=columns_drop)\n",
    "print('Shape after dropping columns:', df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the target column to labels \n",
    "#print(df['target'].unique())\n",
    "target_map_dict={'functional': 2, 'functional needs repair': 1, 'non functional': 0} # Defined the mapping of labels to numbers (integers)\n",
    "#print(df['target'].head())\n",
    "df['target'] =df['target'].map(target_map_dict) # transform the target column (labels) to  numbers (integers)\n",
    "#df['target'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and target and perform train test split\n",
    "X = df.drop(columns=['target'])  # Features only\n",
    "y = df['target']                 # Target column\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.04, random_state=42, stratify=y)  # 2376 records for the test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the custom transformers form helper_function.py\n",
    "# The helper_function.py file contains the definitions for StringConverter, YearExtractor, IQRCapper, and ConstructionYearTransformer\n",
    "from helper_function import (\n",
    "    StringConverter,\n",
    "    YearExtractor,\n",
    "    IQRCapper,\n",
    "    ConstructionYearTransformer,\n",
    "    ObjectToNumericConverter\n",
    ")\n",
    "    \n",
    "#pipeline transformers\n",
    "date_recorded_transformer_pipeline=Pipeline([\n",
    "    \n",
    "    ('year_extractor',YearExtractor()),\n",
    "    ('onehot', OneHotEncoder(sparse_output=False, handle_unknown='ignore',drop='first'))\n",
    "])\n",
    "\n",
    "\n",
    "oulier_minmax_pipeline_clip = Pipeline(steps=[\n",
    "    ('iqr_cap', IQRCapper(strategy='clip')),\n",
    "    ('scaler', MinMaxScaler())\n",
    "])\n",
    "\n",
    "oulier_minmax_pipeline_mean = Pipeline(steps=[\n",
    "    ('iqr_cap', IQRCapper(strategy='mean')),\n",
    "    ('scaler', MinMaxScaler())\n",
    "])\n",
    "\n",
    "oulier_minmax_pipeline_median = Pipeline(steps=[\n",
    "    ('iqr_cap', IQRCapper(strategy='median')),\n",
    "    ('scaler', MinMaxScaler())\n",
    "])\n",
    "\n",
    "\n",
    "cat_pipeline = Pipeline([\n",
    "     ('string_converter', StringConverter()),\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='Unknown')),\n",
    "    ('ohe', OneHotEncoder(sparse_output=False, handle_unknown='ignore',drop='first'))\n",
    "])\n",
    "\n",
    "constructionyear_pipeline = Pipeline(steps=[\n",
    "    ('replace_zeros_with_median', ConstructionYearTransformer()),\n",
    "    ('minmax_scaling', MinMaxScaler())\n",
    "])\n",
    "# ColumnTransformer and full pipeline setup for feature preprocessing\n",
    "# The ColumnTransformer allows us to apply different preprocessing steps to different columns of the DataFrame\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('date', date_recorded_transformer_pipeline, ['date_recorded']),\n",
    "        #('gps_height', scaler_minmax, ['gps_height']),\n",
    "        ('outlier_minmax_gps_height', oulier_minmax_pipeline_mean, ['gps_height']),\n",
    "        ('outlier_minmax_longitude', oulier_minmax_pipeline_mean, ['longitude']),\n",
    "        ('outlier_minmax_latitude', oulier_minmax_pipeline_mean, ['latitude']),\n",
    "         ('cat_ohe', cat_pipeline, ['basin','region','region_code','lga','public_meeting','permit','extraction_type','management_group','payment','water_quality','quantity','source_type','source_class','waterpoint_type']),\n",
    "        ('outlier_minmax_population', oulier_minmax_pipeline_clip, ['population']),\n",
    "        ('constructionyear', constructionyear_pipeline, ['construction_year'])\n",
    "\n",
    "\n",
    "    ],\n",
    "    remainder='passthrough',\n",
    "    verbose_feature_names_out=False\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "preprocess_pipeline = Pipeline([\n",
    "    ('preprocessing', preprocessor),\n",
    "    ('object_to_numeric', ObjectToNumericConverter())  # your custom step\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Comparison:\n",
      "Model           Train Acc       Test Acc       \n",
      "---------------------------------------------\n",
      "Decision Tree   0.7668          0.7483         \n",
      "Random Forest   0.8137          0.7778         \n",
      "XGBoost         0.8598          0.8085         \n",
      "\n",
      "Top 10 Important Features:\n",
      "\n",
      "Decision Tree:\n",
      "waterpoint_type_other          0.2024\n",
      "quantity_seasonal              0.1304\n",
      "quantity_enough                0.1123\n",
      "quantity_insufficient          0.0860\n",
      "longitude                      0.0614\n",
      "construction_year              0.0598\n",
      "latitude                       0.0412\n",
      "waterpoint_type_communal standpipe multiple 0.0378\n",
      "population                     0.0157\n",
      "gps_height                     0.0151\n",
      "\n",
      "Random Forest:\n",
      "quantity_enough                0.0774\n",
      "waterpoint_type_other          0.0748\n",
      "longitude                      0.0678\n",
      "latitude                       0.0634\n",
      "construction_year              0.0610\n",
      "extraction_type_other          0.0586\n",
      "gps_height                     0.0421\n",
      "population                     0.0306\n",
      "quantity_insufficient          0.0248\n",
      "waterpoint_type_communal standpipe 0.0199\n",
      "\n",
      "XGBoost:\n",
      "waterpoint_type_other          0.0528\n",
      "quantity_seasonal              0.0259\n",
      "lga_Bariadi                    0.0251\n",
      "region_Iringa                  0.0188\n",
      "region_code_18                 0.0172\n",
      "extraction_type_other          0.0168\n",
      "region_code_11                 0.0154\n",
      "region_code_15                 0.0148\n",
      "lga_Rombo                      0.0136\n",
      "lga_Chunya                     0.0134\n"
     ]
    }
   ],
   "source": [
    "# models (Decision Tree, Random Forest, XGBoost) to be used\n",
    "models = {    \n",
    "    \"Decision Tree\": DecisionTreeClassifier(\n",
    "        max_depth=10,  # You can tune this\n",
    "        random_state=42\n",
    "    ),\n",
    "    \"Random Forest\": RandomForestClassifier(\n",
    "        n_estimators=100,\n",
    "        max_depth=15,  # You can tune this too\n",
    "        random_state=42\n",
    "    ),\"XGBoost\": XGBClassifier(\n",
    "        n_estimators=100,\n",
    "        max_depth=11,\n",
    "        learning_rate=0.1,\n",
    "        subsample=0.9,\n",
    "        colsample_bytree=0.8,\n",
    "        #use_label_encoder=False,\n",
    "        eval_metric='mlogloss',    # good for multi-class\n",
    "        objective='multi:softmax', # directly outputs class labels\n",
    "        num_class=3,               # number of target classes\n",
    "        random_state=42\n",
    "    )\n",
    "}\n",
    "\n",
    "# Results and feature importances storage\n",
    "results = {}\n",
    "feature_importances = {}\n",
    "\n",
    "# Loop through each model\n",
    "for name, model in models.items():\n",
    "    full_pipeline = Pipeline([\n",
    "        ('preprocessing', preprocessor),\n",
    "        ('model', model)\n",
    "                            ])\n",
    "\n",
    "\n",
    "    full_pipeline.fit(X_train, y_train) \n",
    "    \"\"\"\n",
    "    # Step 1: Each transformer in the preprocessing pipeline Computes and stores necessary statistics \n",
    "    # (e.g., quartiles, medians, scalers) from X_train only.\n",
    "    #Step 2: The transformers are applied (transformed) to X_train to produce the final preprocessed training features.\n",
    "    # Step 3: The model is trained using these transformed features and y_train.\n",
    "    \"\"\" \n",
    "\n",
    "\n",
    "    # Predictions\n",
    "    y_train_pred = full_pipeline.predict(X_train)\n",
    "    y_test_pred = full_pipeline.predict(X_test)\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    The stored training statistics are used to transform X_train again and X_test (no re-fitting!).\n",
    "\n",
    "    Feeds this transformed data to the already-trained model.\n",
    "\n",
    "    Outputs predictions (y_train_pred, y_test_pred).\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Accuracy scores\n",
    "    train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "    test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "    results[name] = {\n",
    "        \"Train Accuracy\": train_accuracy,\n",
    "        \"Test Accuracy\": test_accuracy\n",
    "    }\n",
    "    # Extract feature importances\n",
    "    fitted_model = full_pipeline.named_steps['model']\n",
    "    \n",
    "    if hasattr(fitted_model, 'feature_importances_'):\n",
    "        # Get transformed feature names from preprocessor\n",
    "        feature_names = full_pipeline.named_steps['preprocessing'].get_feature_names_out()\n",
    "        importances = fitted_model.feature_importances_\n",
    "        feature_importances[name] = sorted(\n",
    "            zip(feature_names, importances),\n",
    "            key=lambda x: x[1],\n",
    "            reverse=True\n",
    "        )\n",
    "\n",
    "# Print results\n",
    "print(\"\\nModel Comparison:\")\n",
    "print(\"{:<15} {:<15} {:<15}\".format(\"Model\", \"Train Acc\", \"Test Acc\"))\n",
    "print(\"-\" * 45)\n",
    "for model_name, scores in results.items():\n",
    "    print(\"{:<15} {:<15.4f} {:<15.4f}\".format(model_name, scores[\"Train Accuracy\"], scores[\"Test Accuracy\"]))\n",
    "\n",
    "# Print top features\n",
    "print(\"\\nTop 10 Important Features:\")\n",
    "for model_name, importance_list in feature_importances.items():\n",
    "    print(f\"\\n{model_name}:\")\n",
    "    for feature, importance in importance_list[:10]:\n",
    "        print(f\"{feature:<30} {importance:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
